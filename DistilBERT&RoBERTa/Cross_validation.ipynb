{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aQEzwl_JTUep"
   },
   "source": [
    "# Cross-validation RoBERTa-based model using Tree-parzen Estimator algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MjYxPm_ITY3y"
   },
   "source": [
    "Install required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "id": "MCMpvcZZc3Oe",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import RobertaTokenizer, TFRobertaModel\n",
    "import matplotlib.pyplot as plt\n",
    "from helpers import *\n",
    "from implementations import *\n",
    "import pickle\n",
    "import os \n",
    "import json\n",
    "from bson import json_util\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "veS_okaSTlLp"
   },
   "source": [
    "To speed up the training and inferencing procedure we made use of the TPUs available on Google Colab. If you want to run this code on Google Colab, uncomment the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x8JUBHSqSNsI",
    "outputId": "299f6e9c-d86d-4178-a209-510fb9d94ae5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Entering into master device scope: /job:worker/replica:0/task:0/device:CPU:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 2.7.0\n",
      "Running on TPU  ['10.4.54.218:8470']\n",
      "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: grpc://10.4.54.218:8470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: grpc://10.4.54.218:8470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n",
      "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    }
   ],
   "source": [
    "# %tensorflow_version 2.x\n",
    "# import tensorflow as tf\n",
    "# print(\"Tensorflow version \" + tf.__version__)\n",
    "\n",
    "# try:\n",
    "#   tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
    "#   print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
    "# except ValueError:\n",
    "#   raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
    "\n",
    "# tf.config.experimental_connect_to_cluster(tpu)\n",
    "# tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "# tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6yEjji3kToOl"
   },
   "source": [
    "Load the data (we use the smaller dataset due to running time) and set required constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "oNOUXEucc3Oj",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Small dataset (200'000 observations)\n",
    "POS_TRAINING_PATH = \"train_pos.txt\"\n",
    "NEG_TRAINING_PATH = \"train_neg.txt\"\n",
    "\n",
    "# Full dataset (2.2 million obsevations)\n",
    "POS_TRAINING_PATH_FULL = \"train_pos_full.txt\"\n",
    "NEG_TRAINING_PATH_FULL = \"train_neg_full.txt\"\n",
    "\n",
    "# Test data\n",
    "TEST_PATH = \"test_data.txt\"\n",
    "\n",
    "# set the required constants\n",
    "RANDOM_STATE = 123\n",
    "MAX_LENGTH= 55\n",
    "USE_LSTM = False # set to false to cross validate the model without Bi-LSTM layers \n",
    "\n",
    "# Load both small and large training sets\n",
    "df_full = loadData(POS_TRAINING_PATH, NEG_TRAINING_PATH)\n",
    "# df_full = loadData(POS_TRAINING_PATH_FULL, NEG_TRAINING_PATH_FULL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wuyeBNC7TzeD"
   },
   "source": [
    "Load the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "b_zs2xDGc3Ok",
    "outputId": "057eade8-ea73-4dd7-ceee-a09acd34825b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-c5567d63-5229-4694-95ea-3bfae39fa885\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>sea doo pro sea scooter ( sports with the port...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>&lt;user&gt; shucks well i work all week so now i ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>i cant stay away from bug thats my baby\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>&lt;user&gt; no ma'am ! ! ! lol im perfectly fine an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>whenever i fall asleep watching the tv , i alw...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c5567d63-5229-4694-95ea-3bfae39fa885')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-c5567d63-5229-4694-95ea-3bfae39fa885 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-c5567d63-5229-4694-95ea-3bfae39fa885');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "   index                                              tweet\n",
       "0      1  sea doo pro sea scooter ( sports with the port...\n",
       "1      2  <user> shucks well i work all week so now i ca...\n",
       "2      3          i cant stay away from bug thats my baby\\n\n",
       "3      4  <user> no ma'am ! ! ! lol im perfectly fine an...\n",
       "4      5  whenever i fall asleep watching the tv , i alw..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the test set.\n",
    "test_sent = []\n",
    "idx = []\n",
    "with open(TEST_PATH) as test:\n",
    "    for line in test:\n",
    "        split = line.split(\",\", 1)\n",
    "        idx.append(int(split[0]))\n",
    "        test_sent.append(split[1])\n",
    "\n",
    "data = {'index':idx,'tweet':test_sent}\n",
    "df_test = pd.DataFrame(data)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WqAOIJYuT1Sr"
   },
   "source": [
    "Load and initialize the base RoBERTa tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "C3D2OE6e0sAH"
   },
   "outputs": [],
   "source": [
    "# RoBERTa\n",
    "tokenizer_roberta = RobertaTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "uzTrhJbxc3Om",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# we used the entire small dataset\n",
    "training = df_full.sample(frac=1,random_state=123) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "JLGDdJYYfDK5",
    "outputId": "e947ec13-b012-40e1-d972-cb86e9c77e7a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-34bc3f7d-ad3c-4423-966b-841739bff24e\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28285</th>\n",
       "      <td>&lt;user&gt; awww , whizzy is happy\\n</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5080</th>\n",
       "      <td>#waystomakemehappy back rubs please\\n</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93291</th>\n",
       "      <td>this is torture ; i'm sat upstairs revising an...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3185</th>\n",
       "      <td>8am she's lucky she is cute for waking me so e...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61505</th>\n",
       "      <td>anyone who takes the mick out of anyone on #un...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-34bc3f7d-ad3c-4423-966b-841739bff24e')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-34bc3f7d-ad3c-4423-966b-841739bff24e button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-34bc3f7d-ad3c-4423-966b-841739bff24e');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                                                   tweet  sentiment\n",
       "28285                    <user> awww , whizzy is happy\\n          1\n",
       "5080               #waystomakemehappy back rubs please\\n          1\n",
       "93291  this is torture ; i'm sat upstairs revising an...          0\n",
       "3185   8am she's lucky she is cute for waking me so e...          1\n",
       "61505  anyone who takes the mick out of anyone on #un...          1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kZiK_iDCUWBe"
   },
   "source": [
    "Obtain the DataFrame of indeces and of attentions for each tweet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ec_pzhwCc3Op",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_idx, train_att = batch_encode_cross_validation(tokenizer_roberta, training.tweet.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SeGl63cU3mk"
   },
   "source": [
    "Define the function that builds the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "d1PPDZDWTKXS"
   },
   "outputs": [],
   "source": [
    "def build_model(transformer, dropout_rate, layer1_nodes, layer2_nodes, act_func, l1, l2, max_length=MAX_LENGTH, random_state=RANDOM_STATE):\n",
    "    \"\"\"\"\"\"\"\"\"\n",
    "    Build the model given the set of hyperparameters\n",
    "\n",
    "    Input:\n",
    "        - transformer:   the Transformer model to be used (either DistilBERT or RoBERTa)\n",
    "        - dropout_rate:  dropout rate\n",
    "        - layer1_nodes:  number of neurons in the frist layer of the MLP classifier\n",
    "        - layer2_nodes:  number of neurons in the second layer of the MLP classifier\n",
    "        - act_func:      activation function\n",
    "        - l1:            l1 regularization weight\n",
    "        - l2:            l2 regularization weight\n",
    "        - max_length:    max sentence lenght (default=55) \n",
    "        - random_state:  seed for reproduceabilty\n",
    "    Output:\n",
    "        - model:         the built model ready to be compiled and trained\n",
    "    \"\"\"\"\"\"\"\"\"\n",
    "\n",
    "    # Define weight initializer with a random seed to ensure reproducibility\n",
    "    weight_initializer = tf.keras.initializers.GlorotNormal(seed=random_state)\n",
    "\n",
    "    # Define input layers\n",
    "    input_ids_layer = tf.keras.layers.Input(shape=(max_length,),\n",
    "                                            name='input_ids',\n",
    "                                            dtype='int32')\n",
    "    input_attention_layer = tf.keras.layers.Input(shape=(max_length,),\n",
    "                                                  name='input_attention',\n",
    "                                                  dtype='int32')\n",
    "\n",
    "    # Access the last layer of the Transformer model.\n",
    "    # It is a tf.Tensor of shape (batch_size, max_length, hidden_size=768).\n",
    "    last_hidden_state = transformer([input_ids_layer, input_attention_layer])[0]\n",
    "\n",
    "    # From the last layer, we select the embedding of the [CLS] token\n",
    "    cls_token = last_hidden_state[:, 0, :]\n",
    "    \n",
    "        # include 2 bi-direction LSTM layers\n",
    "    if USE_LSTM == True:\n",
    "        # obtain the vector representation of each word (do not select the [CLS] and [SEP] special tokens)\n",
    "        embeddings = last_hidden_state[:, 1:-1, :]\n",
    "        X = tf.keras.layers.Dropout(dropout_rateseed=random_state)(embeddings)\n",
    "        # Bi-LSTM layers \n",
    "        X = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=1024, kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1, l2=l2), kernel_initializer=weight_initializer, bias_initializer='zeros', return_sequences=True), merge_mode='ave')(X)        X = tf.keras.layers.Dropout(LAYER_DROPOUT)(X)\n",
    "        X = tf.keras.layers.Dropout(dropout_rateseed=random_state)(X)\n",
    "        X = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=1024, kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1, l2=l2), kernel_initializer=weight_initializer, bias_initializer='zeros', return_sequences=False), merge_mode='ave')(X)\n",
    "        X = tf.concat([cls_token, X], axis=1)\n",
    "    else:\n",
    "        X = cls_token\n",
    "        \n",
    "    # Build the 2-layers MLP classifier\n",
    "    X = tf.keras.layers.Dropout(dropout_rateseed=random_state)(X)\n",
    "    X = tf.keras.layers.Dense(layer1_nodes, activation=act_func, kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1, l2=l2), kernel_initializer=weight_initializer, bias_initializer='zeros')(X)\n",
    "    X = tf.keras.layers.Dropout(dropout_rateseed=random_state)(X)\n",
    "    if layer2_nodes != 0:\n",
    "        X = tf.keras.layers.Dense(layer2_nodes, activation=act_func, kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1, l2=l2), kernel_initializer=weight_initializer, bias_initializer='zeros')(X)\n",
    "        X = tf.keras.layers.Dropout(dropout_rateseed=random_state)(X)\n",
    "\n",
    "    # Define a single node that makes up the output layer (for binary classification)\n",
    "    output = tf.keras.layers.Dense(1,\n",
    "                                   activation='sigmoid',\n",
    "                                   kernel_initializer=weight_initializer,\n",
    "                                   kernel_constraint=None,\n",
    "                                   bias_initializer='zeros'\n",
    "                                   )(X)\n",
    "\n",
    "    # Define the model\n",
    "    model = tf.keras.Model([input_ids_layer, input_attention_layer], output)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HrNPKYsWVJF0"
   },
   "source": [
    "Define the function that compiles and trains the model given the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "eyTs_LzcTOAI"
   },
   "outputs": [],
   "source": [
    "def train_model(model, training_ids, training_attention, val_ids, val_attention, training_sentiment, val_sentiment, batch_size, lr, beta1, beta2, train_epochs=2, fine_tune_epochs=3):\n",
    "    \"\"\"\"\"\"\"\"\"\n",
    "    Compile and train the given model\n",
    "\n",
    "    Input:\n",
    "        - model:              the model built to be trained\n",
    "        - training_ids:       training set of indeces\n",
    "        - training_attention: training set of attention masks\n",
    "        - val_ids:            validation set of indeces \n",
    "        - val_attention:      validation set of attention masks\n",
    "        - training_sentiment: training labels\n",
    "        - val_sentiment:      validation labels\n",
    "        - batchsize:          batchsize to be used for training\n",
    "        - lr:                 learning rate to be used for training\n",
    "        - beta1:              beta_1 parameter of Adam optimizer\n",
    "        - beta2:              beta_2 parameter of Adam optimizer\n",
    "        - train_epochs:       number of epochs to train the full model (default=1)\n",
    "        - fine_tune_epochs    number of epochs to fine tune the MLP classifier (default=3)\n",
    "    Output:\n",
    "        - val_acc:            best validation accuracy \n",
    "        - best_epoch:         fine_tune epochs that gave the best validation accuracy\n",
    "    \"\"\"\"\"\"\"\"\"\n",
    "    \n",
    "    num_steps = tf.shape(training_ids)[0].numpy() // batch_size\n",
    "\n",
    "    # compile the model using Adam optimizer\n",
    "    model.compile(tf.keras.optimizers.Adam(lr=lr, beta_1=beta1, beta_2=beta2),\n",
    "                  loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    # Train the full model\n",
    "    train_history1 = model.fit(\n",
    "      x = [training_ids, training_attention],\n",
    "      y = training_sentiment,\n",
    "      epochs = train_epochs,\n",
    "      batch_size = batch_size,\n",
    "      steps_per_epoch = num_steps,\n",
    "      validation_data = ([val_ids, val_attention], val_sentiment),\n",
    "      verbose=2\n",
    "    )\n",
    "\n",
    "    # set the RoBERTa model to untrainable\n",
    "    model.get_layer(index=2).trainable = False\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    # Fine tune the Neural Network built on top of the RoBERTa model\n",
    "    train_history2 = model.fit(\n",
    "      x = [training_ids, training_attention],\n",
    "      y = training_sentiment,\n",
    "      epochs = fine_tune_epochs,\n",
    "      batch_size = batch_size,\n",
    "      steps_per_epoch = num_steps,\n",
    "      validation_data = ([val_ids, val_attention], val_sentiment),\n",
    "      verbose=2\n",
    "    )\n",
    "    \n",
    "    # Find the best validation accuracy\n",
    "    val_acc = max(train_history2.history['val_accuracy'])\n",
    "    print(val_acc)\n",
    "    \n",
    "    # Find the fine-tuning epoch that achieved the highest accuracy\n",
    "    best_epoch = np.argmax(train_history2.history['val_accuracy']) + 1\n",
    "    print(best_epoch)\n",
    "\n",
    "    return val_acc, best_epoch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yUVuzvJ7Ucgo"
   },
   "source": [
    "Define the function that will split the data into K-folds (we use 3 due to the runnning time) and trains and validates the current hyperparameters. Return the average accuracy over the folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "bg6SqiPhq-oR"
   },
   "outputs": [],
   "source": [
    "def hyperopt_cross_val(hyperparams):\n",
    "    \"\"\"\"\"\"\"\"\"\n",
    "    split the data into folds and run the training and validation on all the folds\n",
    "\n",
    "    Input:\n",
    "        - hyperparams:       set of hyperparameters to be used\n",
    "    Output:\n",
    "        - average accuracy:  average best valdiation accuracy \n",
    "        - best_epochs:       list of all the best epochs for each fold\n",
    "    \"\"\"\"\"\"\"\"\"\n",
    "    \n",
    "    # uncomment the line below to use the TPU from Google Colab\n",
    "    #with tpu_strategy.scope(): \n",
    "\n",
    "    kf = KFold(n_splits=3)\n",
    "    accuracy = []\n",
    "    best_epochs = []\n",
    "    for train, test in kf.split(train_idx):\n",
    "        # Initialize RoBERTa layer \n",
    "        RoBERTa = TFRobertaModel.from_pretrained('roberta-base')\n",
    "        for layer in RoBERTa.layers:\n",
    "            layer.trainable = True\n",
    "\n",
    "        # Create datasets\n",
    "        training_ids, training_attention = tf.convert_to_tensor(train_idx.iloc[train]), tf.convert_to_tensor(train_att.iloc[train])\n",
    "        val_ids, val_attention = tf.convert_to_tensor(train_idx.iloc[test]), tf.convert_to_tensor(train_att.iloc[test])\n",
    "        training_sentiment = training.sentiment.iloc[train]\n",
    "        val_sentiment = training.sentiment.iloc[test]\n",
    "\n",
    "        # Get hyperparameters\n",
    "        (learning_rate, drop_rate, beta1, beta2, l1, l2, batchsize, activation, layer1_nodes, layer2_nodes) = hyperparams\n",
    "        print(\"Training with the following hyperparameters:\" )\n",
    "        print(hyperparams)\n",
    "\n",
    "        # Build model\n",
    "        model = build_model(RoBERTa, drop_rate, layer1_nodes, layer2_nodes, activation, l1, l2, max_length=MAX_LENGTH, random_state=RANDOM_STATE)\n",
    "\n",
    "        # compile model\n",
    "        acc, epoch = train_model(model, training_ids, training_attention, val_ids, val_attention, training_sentiment, val_sentiment, batchsize, learning_rate, beta1, beta2)\n",
    "        accuracy.append(acc)\n",
    "        best_epochs.append(epoch)\n",
    "\n",
    "    return np.mean(accuracy), best_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rXya2jeTVpTL"
   },
   "source": [
    "Define the space of hyperparameters to be searched and run the Tree-Parzen Estimation algorithm for hyperparameter optimization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fj-7TiUM56GU",
    "outputId": "2c86939f-df67-41f4-d629-6eab3499eb76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing New Model\n",
      "Attempt to resume a past training if it exists:\n",
      "Found saved Trials! Loading...\n",
      "Rerunning from 0 trials to add another one.\n",
      "  0%|          | 0/1 [00:00<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with the following hyperparameters:\n",
      "(0.0001, 0.4, 0.9, 0.9, 0.01, 0.0005, 128, 'sigmoid', 512, 128)\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " el)                            thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_1 (Sl  (None, 53, 768)     0           ['tf_roberta_model[0][0]']       \n",
      " icingOpLambda)                                                                                   \n",
      " dropout_37 (Dropout)           (None, 53, 768)      0           ['tf.__operators__.getitem_1[0][0\n",
      "                                                                 ]']                              \n",
      "  0%|          | 0/1 [00:58<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " bidirectional (Bidirectional)  (None, 53, 1024)     14688256    ['dropout_37[0][0]']             \n",
      " dropout_38 (Dropout)           (None, 53, 1024)     0           ['bidirectional[0][0]']          \n",
      " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
      " ingOpLambda)                                                                                     \n",
      " bidirectional_1 (Bidirectional  (None, 1024)        16785408    ['dropout_38[0][0]']             \n",
      " )                                                                                                \n",
      " tf.concat (TFOpLambda)         (None, 1792)         0           ['tf.__operators__.getitem[0][0]'\n",
      "                                                                 , 'bidirectional_1[0][0]']       \n",
      " dropout_39 (Dropout)           (None, 1792)         0           ['tf.concat[0][0]']              \n",
      " dense (Dense)                  (None, 512)          918016      ['dropout_39[0][0]']             \n",
      " dropout_40 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
      " dense_1 (Dense)                (None, 128)          65664       ['dropout_40[0][0]']             \n",
      " dropout_41 (Dropout)           (None, 128)          0           ['dense_1[0][0]']                \n",
      " dense_2 (Dense)                (None, 1)            129         ['dropout_41[0][0]']             \n",
      "==================================================================================================\n",
      "Total params: 157,103,105\n",
      "Trainable params: 157,103,105\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "  0%|          | 0/1 [00:58<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:2970: StrategyBase.unwrap (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "use `experimental_local_results` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:2970: StrategyBase.unwrap (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "use `experimental_local_results` instead.\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "944/944 - 359s - loss: 368.1930 - accuracy: 0.5783 - val_loss: 3.5019 - val_accuracy: 0.5007 - 359s/epoch - 380ms/step\n",
      "\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " el)                            thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_1 (Sl  (None, 53, 768)     0           ['tf_roberta_model[0][0]']       \n",
      " icingOpLambda)                                                                                   \n",
      " dropout_37 (Dropout)           (None, 53, 768)      0           ['tf.__operators__.getitem_1[0][0\n",
      "                                                                 ]']                              \n",
      " bidirectional (Bidirectional)  (None, 53, 1024)     14688256    ['dropout_37[0][0]']             \n",
      " dropout_38 (Dropout)           (None, 53, 1024)     0           ['bidirectional[0][0]']          \n",
      " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
      " ingOpLambda)                                                                                     \n",
      " bidirectional_1 (Bidirectional  (None, 1024)        16785408    ['dropout_38[0][0]']             \n",
      " )                                                                                                \n",
      " tf.concat (TFOpLambda)         (None, 1792)         0           ['tf.__operators__.getitem[0][0]'\n",
      "                                                                 , 'bidirectional_1[0][0]']       \n",
      " dropout_39 (Dropout)           (None, 1792)         0           ['tf.concat[0][0]']              \n",
      " dense (Dense)                  (None, 512)          918016      ['dropout_39[0][0]']             \n",
      " dropout_40 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
      " dense_1 (Dense)                (None, 128)          65664       ['dropout_40[0][0]']             \n",
      " dropout_41 (Dropout)           (None, 128)          0           ['dense_1[0][0]']                \n",
      " dense_2 (Dense)                (None, 1)            129         ['dropout_41[0][0]']             \n",
      "==================================================================================================\n",
      "Total params: 157,103,105\n",
      "Trainable params: 32,457,473\n",
      "Non-trainable params: 124,645,632\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "944/944 - 241s - loss: 2.7197 - accuracy: 0.5014 - val_loss: 2.5830 - val_accuracy: 0.5007 - 241s/epoch - 256ms/step\n",
      "\n",
      "Epoch 2/2\n",
      "944/944 - 293s - loss: 2.6120 - accuracy: 0.4991 - val_loss: 2.5859 - val_accuracy: 0.5007 - 293s/epoch - 311ms/step\n",
      "\n",
      "0.5006535053253174\n",
      "1\n",
      "  0%|          | 0/1 [15:55<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with the following hyperparameters:\n",
      "(0.0001, 0.4, 0.9, 0.9, 0.01, 0.0005, 128, 'sigmoid', 512, 128)\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_1 (TFRobertaM  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " odel)                          thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_3 (Sl  (None, 53, 768)     0           ['tf_roberta_model_1[0][0]']     \n",
      " icingOpLambda)                                                                                   \n",
      " dropout_79 (Dropout)           (None, 53, 768)      0           ['tf.__operators__.getitem_3[0][0\n",
      "                                                                 ]']                              \n",
      " bidirectional_2 (Bidirectional  (None, 53, 1024)    14688256    ['dropout_79[0][0]']             \n",
      " )                                                                                                \n",
      " dropout_80 (Dropout)           (None, 53, 1024)     0           ['bidirectional_2[0][0]']        \n",
      " tf.__operators__.getitem_2 (Sl  (None, 768)         0           ['tf_roberta_model_1[0][0]']     \n",
      " icingOpLambda)                                                                                   \n",
      " bidirectional_3 (Bidirectional  (None, 1024)        16785408    ['dropout_80[0][0]']             \n",
      " )                                                                                                \n",
      " tf.concat_1 (TFOpLambda)       (None, 1792)         0           ['tf.__operators__.getitem_2[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'bidirectional_3[0][0]']        \n",
      " dropout_81 (Dropout)           (None, 1792)         0           ['tf.concat_1[0][0]']            \n",
      " dense_3 (Dense)                (None, 512)          918016      ['dropout_81[0][0]']             \n",
      " dropout_82 (Dropout)           (None, 512)          0           ['dense_3[0][0]']                \n",
      " dense_4 (Dense)                (None, 128)          65664       ['dropout_82[0][0]']             \n",
      " dropout_83 (Dropout)           (None, 128)          0           ['dense_4[0][0]']                \n",
      " dense_5 (Dense)                (None, 1)            129         ['dropout_83[0][0]']             \n",
      "==================================================================================================\n",
      "Total params: 157,103,105\n",
      "Trainable params: 157,103,105\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "  0%|          | 0/1 [16:42<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_1/roberta/pooler/dense/kernel:0', 'tf_roberta_model_1/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_1/roberta/pooler/dense/kernel:0', 'tf_roberta_model_1/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_1/roberta/pooler/dense/kernel:0', 'tf_roberta_model_1/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_1/roberta/pooler/dense/kernel:0', 'tf_roberta_model_1/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "944/944 - 364s - loss: 368.1852 - accuracy: 0.5735 - val_loss: 3.5405 - val_accuracy: 0.5038 - 364s/epoch - 386ms/step\n",
      "\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_1 (TFRobertaM  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " odel)                          thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_3 (Sl  (None, 53, 768)     0           ['tf_roberta_model_1[0][0]']     \n",
      " icingOpLambda)                                                                                   \n",
      " dropout_79 (Dropout)           (None, 53, 768)      0           ['tf.__operators__.getitem_3[0][0\n",
      "                                                                 ]']                              \n",
      " bidirectional_2 (Bidirectional  (None, 53, 1024)    14688256    ['dropout_79[0][0]']             \n",
      " )                                                                                                \n",
      " dropout_80 (Dropout)           (None, 53, 1024)     0           ['bidirectional_2[0][0]']        \n",
      " tf.__operators__.getitem_2 (Sl  (None, 768)         0           ['tf_roberta_model_1[0][0]']     \n",
      " icingOpLambda)                                                                                   \n",
      " bidirectional_3 (Bidirectional  (None, 1024)        16785408    ['dropout_80[0][0]']             \n",
      " )                                                                                                \n",
      " tf.concat_1 (TFOpLambda)       (None, 1792)         0           ['tf.__operators__.getitem_2[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'bidirectional_3[0][0]']        \n",
      " dropout_81 (Dropout)           (None, 1792)         0           ['tf.concat_1[0][0]']            \n",
      " dense_3 (Dense)                (None, 512)          918016      ['dropout_81[0][0]']             \n",
      " dropout_82 (Dropout)           (None, 512)          0           ['dense_3[0][0]']                \n",
      " dense_4 (Dense)                (None, 128)          65664       ['dropout_82[0][0]']             \n",
      " dropout_83 (Dropout)           (None, 128)          0           ['dense_4[0][0]']                \n",
      " dense_5 (Dense)                (None, 1)            129         ['dropout_83[0][0]']             \n",
      "==================================================================================================\n",
      "Total params: 157,103,105\n",
      "Trainable params: 32,457,473\n",
      "Non-trainable params: 124,645,632\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "944/944 - 242s - loss: 2.7450 - accuracy: 0.5004 - val_loss: 2.6108 - val_accuracy: 0.4962 - 242s/epoch - 256ms/step\n",
      "\n",
      "Epoch 2/2\n",
      "944/944 - 294s - loss: 2.6340 - accuracy: 0.5011 - val_loss: 2.6118 - val_accuracy: 0.4962 - 294s/epoch - 312ms/step\n",
      "\n",
      "0.49616149067878723\n",
      "1\n",
      "  0%|          | 0/1 [31:45<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with the following hyperparameters:\n",
      "(0.0001, 0.4, 0.9, 0.9, 0.01, 0.0005, 128, 'sigmoid', 512, 128)\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_2 (TFRobertaM  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " odel)                          thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_5 (Sl  (None, 53, 768)     0           ['tf_roberta_model_2[0][0]']     \n",
      " icingOpLambda)                                                                                   \n",
      " dropout_121 (Dropout)          (None, 53, 768)      0           ['tf.__operators__.getitem_5[0][0\n",
      "                                                                 ]']                              \n",
      " bidirectional_4 (Bidirectional  (None, 53, 1024)    14688256    ['dropout_121[0][0]']            \n",
      " )                                                                                                \n",
      " dropout_122 (Dropout)          (None, 53, 1024)     0           ['bidirectional_4[0][0]']        \n",
      " tf.__operators__.getitem_4 (Sl  (None, 768)         0           ['tf_roberta_model_2[0][0]']     \n",
      " icingOpLambda)                                                                                   \n",
      " bidirectional_5 (Bidirectional  (None, 1024)        16785408    ['dropout_122[0][0]']            \n",
      " )                                                                                                \n",
      " tf.concat_2 (TFOpLambda)       (None, 1792)         0           ['tf.__operators__.getitem_4[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'bidirectional_5[0][0]']        \n",
      " dropout_123 (Dropout)          (None, 1792)         0           ['tf.concat_2[0][0]']            \n",
      " dense_6 (Dense)                (None, 512)          918016      ['dropout_123[0][0]']            \n",
      " dropout_124 (Dropout)          (None, 512)          0           ['dense_6[0][0]']                \n",
      " dense_7 (Dense)                (None, 128)          65664       ['dropout_124[0][0]']            \n",
      " dropout_125 (Dropout)          (None, 128)          0           ['dense_7[0][0]']                \n",
      " dense_8 (Dense)                (None, 1)            129         ['dropout_125[0][0]']            \n",
      "==================================================================================================\n",
      "Total params: 157,103,105\n",
      "Trainable params: 157,103,105\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "  0%|          | 0/1 [32:31<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_2/roberta/pooler/dense/kernel:0', 'tf_roberta_model_2/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_2/roberta/pooler/dense/kernel:0', 'tf_roberta_model_2/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_2/roberta/pooler/dense/kernel:0', 'tf_roberta_model_2/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_2/roberta/pooler/dense/kernel:0', 'tf_roberta_model_2/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "944/944 - 373s - loss: 368.4350 - accuracy: 0.5865 - val_loss: 3.5080 - val_accuracy: 0.4974 - 373s/epoch - 395ms/step\n",
      "\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_2 (TFRobertaM  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " odel)                          thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_5 (Sl  (None, 53, 768)     0           ['tf_roberta_model_2[0][0]']     \n",
      " icingOpLambda)                                                                                   \n",
      " dropout_121 (Dropout)          (None, 53, 768)      0           ['tf.__operators__.getitem_5[0][0\n",
      "                                                                 ]']                              \n",
      " bidirectional_4 (Bidirectional  (None, 53, 1024)    14688256    ['dropout_121[0][0]']            \n",
      " )                                                                                                \n",
      " dropout_122 (Dropout)          (None, 53, 1024)     0           ['bidirectional_4[0][0]']        \n",
      " tf.__operators__.getitem_4 (Sl  (None, 768)         0           ['tf_roberta_model_2[0][0]']     \n",
      " icingOpLambda)                                                                                   \n",
      " bidirectional_5 (Bidirectional  (None, 1024)        16785408    ['dropout_122[0][0]']            \n",
      " )                                                                                                \n",
      " tf.concat_2 (TFOpLambda)       (None, 1792)         0           ['tf.__operators__.getitem_4[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'bidirectional_5[0][0]']        \n",
      " dropout_123 (Dropout)          (None, 1792)         0           ['tf.concat_2[0][0]']            \n",
      " dense_6 (Dense)                (None, 512)          918016      ['dropout_123[0][0]']            \n",
      " dropout_124 (Dropout)          (None, 512)          0           ['dense_6[0][0]']                \n",
      " dense_7 (Dense)                (None, 128)          65664       ['dropout_124[0][0]']            \n",
      " dropout_125 (Dropout)          (None, 128)          0           ['dense_7[0][0]']                \n",
      " dense_8 (Dense)                (None, 1)            129         ['dropout_125[0][0]']            \n",
      "==================================================================================================\n",
      "Total params: 157,103,105\n",
      "Trainable params: 32,457,473\n",
      "Non-trainable params: 124,645,632\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "944/944 - 242s - loss: 2.7110 - accuracy: 0.5000 - val_loss: 2.5793 - val_accuracy: 0.4974 - 242s/epoch - 256ms/step\n",
      "\n",
      "Epoch 2/2\n",
      "944/944 - 296s - loss: 2.6022 - accuracy: 0.5003 - val_loss: 2.5750 - val_accuracy: 0.5026 - 296s/epoch - 313ms/step\n",
      "\n",
      "0.5025811195373535\n",
      "2\n",
      "FINISHED THE HYPERPARAMTER TUNING\n",
      "0.499798705180486\n",
      "THE RESULTS ARE:\n",
      "{'acc': 0.499798705180486, 'epochs': '[1, 1, 2]', 'status': 'ok', 'space': (0.0001, 0.4, 0.9, 0.9, 0.01, 0.0005, 128, 'sigmoid', 512, 128)}\n",
      "  0%|          | 0/1 [47:45<?, ?it/s, best loss: ?]\n",
      "Starting from scratch: nesw trials.\n",
      "FINISHED FMIN\n",
      "best:\n",
      "local variable 'best' referenced before assignment\n",
      "Optimizing New Model\n",
      "Attempt to resume a past training if it exists:\n",
      "Found saved Trials! Loading...\n",
      "Rerunning from 0 trials to add another one.\n",
      "  0%|          | 0/1 [00:00<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with the following hyperparameters:\n",
      "(0.0001, 0.3, 0.95, 0.97, 0.01, 0.0005, 128, 'tanh', 512, 256)\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_3 (TFRobertaM  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " odel)                          thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_7 (Sl  (None, 53, 768)     0           ['tf_roberta_model_3[0][0]']     \n",
      " icingOpLambda)                                                                                   \n",
      " dropout_163 (Dropout)          (None, 53, 768)      0           ['tf.__operators__.getitem_7[0][0\n",
      "                                                                 ]']                              \n",
      " bidirectional_6 (Bidirectional  (None, 53, 1024)    14688256    ['dropout_163[0][0]']            \n",
      " )                                                                                                \n",
      " dropout_164 (Dropout)          (None, 53, 1024)     0           ['bidirectional_6[0][0]']        \n",
      " tf.__operators__.getitem_6 (Sl  (None, 768)         0           ['tf_roberta_model_3[0][0]']     \n",
      " icingOpLambda)                                                                                   \n",
      " bidirectional_7 (Bidirectional  (None, 1024)        16785408    ['dropout_164[0][0]']            \n",
      " )                                                                                                \n",
      " tf.concat_3 (TFOpLambda)       (None, 1792)         0           ['tf.__operators__.getitem_6[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'bidirectional_7[0][0]']        \n",
      " dropout_165 (Dropout)          (None, 1792)         0           ['tf.concat_3[0][0]']            \n",
      " dense_9 (Dense)                (None, 512)          918016      ['dropout_165[0][0]']            \n",
      " dropout_166 (Dropout)          (None, 512)          0           ['dense_9[0][0]']                \n",
      " dense_10 (Dense)               (None, 256)          131328      ['dropout_166[0][0]']            \n",
      " dropout_167 (Dropout)          (None, 256)          0           ['dense_10[0][0]']               \n",
      " dense_11 (Dense)               (None, 1)            257         ['dropout_167[0][0]']            \n",
      "==================================================================================================\n",
      "Total params: 157,168,897\n",
      "Trainable params: 157,168,897\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "  0%|          | 0/1 [00:46<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_3/roberta/pooler/dense/kernel:0', 'tf_roberta_model_3/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_3/roberta/pooler/dense/kernel:0', 'tf_roberta_model_3/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_3/roberta/pooler/dense/kernel:0', 'tf_roberta_model_3/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_3/roberta/pooler/dense/kernel:0', 'tf_roberta_model_3/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "944/944 - 368s - loss: 377.3647 - accuracy: 0.8171 - val_loss: 2.7533 - val_accuracy: 0.8073 - 368s/epoch - 390ms/step\n",
      "\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_3 (TFRobertaM  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " odel)                          thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_7 (Sl  (None, 53, 768)     0           ['tf_roberta_model_3[0][0]']     \n",
      " icingOpLambda)                                                                                   \n",
      " dropout_163 (Dropout)          (None, 53, 768)      0           ['tf.__operators__.getitem_7[0][0\n",
      "                                                                 ]']                              \n",
      " bidirectional_6 (Bidirectional  (None, 53, 1024)    14688256    ['dropout_163[0][0]']            \n",
      " )                                                                                                \n",
      " dropout_164 (Dropout)          (None, 53, 1024)     0           ['bidirectional_6[0][0]']        \n",
      " tf.__operators__.getitem_6 (Sl  (None, 768)         0           ['tf_roberta_model_3[0][0]']     \n",
      " icingOpLambda)                                                                                   \n",
      " bidirectional_7 (Bidirectional  (None, 1024)        16785408    ['dropout_164[0][0]']            \n",
      " )                                                                                                \n",
      " tf.concat_3 (TFOpLambda)       (None, 1792)         0           ['tf.__operators__.getitem_6[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'bidirectional_7[0][0]']        \n",
      " dropout_165 (Dropout)          (None, 1792)         0           ['tf.concat_3[0][0]']            \n",
      " dense_9 (Dense)                (None, 512)          918016      ['dropout_165[0][0]']            \n",
      " dropout_166 (Dropout)          (None, 512)          0           ['dense_9[0][0]']                \n",
      " dense_10 (Dense)               (None, 256)          131328      ['dropout_166[0][0]']            \n",
      " dropout_167 (Dropout)          (None, 256)          0           ['dense_10[0][0]']               \n",
      " dense_11 (Dense)               (None, 1)            257         ['dropout_167[0][0]']            \n",
      "==================================================================================================\n",
      "Total params: 157,168,897\n",
      "Trainable params: 32,523,265\n",
      "Non-trainable params: 124,645,632\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "944/944 - 243s - loss: 2.1163 - accuracy: 0.6203 - val_loss: 2.0667 - val_accuracy: 0.4993 - 243s/epoch - 257ms/step\n",
      "\n",
      "Epoch 2/2\n",
      "944/944 - 297s - loss: 2.0639 - accuracy: 0.5025 - val_loss: 2.0654 - val_accuracy: 0.4993 - 297s/epoch - 315ms/step\n",
      "\n",
      "0.49934646487236023\n",
      "1\n",
      "  0%|          | 0/1 [15:58<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with the following hyperparameters:\n",
      "(0.0001, 0.3, 0.95, 0.97, 0.01, 0.0005, 128, 'tanh', 512, 256)\n",
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_4 (TFRobertaM  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " odel)                          thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_9 (Sl  (None, 53, 768)     0           ['tf_roberta_model_4[0][0]']     \n",
      " icingOpLambda)                                                                                   \n",
      " dropout_205 (Dropout)          (None, 53, 768)      0           ['tf.__operators__.getitem_9[0][0\n",
      "                                                                 ]']                              \n",
      " bidirectional_8 (Bidirectional  (None, 53, 1024)    14688256    ['dropout_205[0][0]']            \n",
      " )                                                                                                \n",
      " dropout_206 (Dropout)          (None, 53, 1024)     0           ['bidirectional_8[0][0]']        \n",
      " tf.__operators__.getitem_8 (Sl  (None, 768)         0           ['tf_roberta_model_4[0][0]']     \n",
      " icingOpLambda)                                                                                   \n",
      " bidirectional_9 (Bidirectional  (None, 1024)        16785408    ['dropout_206[0][0]']            \n",
      " )                                                                                                \n",
      " tf.concat_4 (TFOpLambda)       (None, 1792)         0           ['tf.__operators__.getitem_8[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'bidirectional_9[0][0]']        \n",
      " dropout_207 (Dropout)          (None, 1792)         0           ['tf.concat_4[0][0]']            \n",
      " dense_12 (Dense)               (None, 512)          918016      ['dropout_207[0][0]']            \n",
      " dropout_208 (Dropout)          (None, 512)          0           ['dense_12[0][0]']               \n",
      " dense_13 (Dense)               (None, 256)          131328      ['dropout_208[0][0]']            \n",
      " dropout_209 (Dropout)          (None, 256)          0           ['dense_13[0][0]']               \n",
      " dense_14 (Dense)               (None, 1)            257         ['dropout_209[0][0]']            \n",
      "==================================================================================================\n",
      "Total params: 157,168,897\n",
      "Trainable params: 157,168,897\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "  0%|          | 0/1 [16:44<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_4/roberta/pooler/dense/kernel:0', 'tf_roberta_model_4/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_4/roberta/pooler/dense/kernel:0', 'tf_roberta_model_4/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_4/roberta/pooler/dense/kernel:0', 'tf_roberta_model_4/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_4/roberta/pooler/dense/kernel:0', 'tf_roberta_model_4/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "944/944 - 370s - loss: 376.8894 - accuracy: 0.8241 - val_loss: 2.7083 - val_accuracy: 0.8348 - 370s/epoch - 392ms/step\n",
      "\n",
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_4 (TFRobertaM  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " odel)                          thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_9 (Sl  (None, 53, 768)     0           ['tf_roberta_model_4[0][0]']     \n",
      " icingOpLambda)                                                                                   \n",
      " dropout_205 (Dropout)          (None, 53, 768)      0           ['tf.__operators__.getitem_9[0][0\n",
      "                                                                 ]']                              \n",
      " bidirectional_8 (Bidirectional  (None, 53, 1024)    14688256    ['dropout_205[0][0]']            \n",
      " )                                                                                                \n",
      " dropout_206 (Dropout)          (None, 53, 1024)     0           ['bidirectional_8[0][0]']        \n",
      " tf.__operators__.getitem_8 (Sl  (None, 768)         0           ['tf_roberta_model_4[0][0]']     \n",
      " icingOpLambda)                                                                                   \n",
      " bidirectional_9 (Bidirectional  (None, 1024)        16785408    ['dropout_206[0][0]']            \n",
      " )                                                                                                \n",
      " tf.concat_4 (TFOpLambda)       (None, 1792)         0           ['tf.__operators__.getitem_8[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'bidirectional_9[0][0]']        \n",
      " dropout_207 (Dropout)          (None, 1792)         0           ['tf.concat_4[0][0]']            \n",
      " dense_12 (Dense)               (None, 512)          918016      ['dropout_207[0][0]']            \n",
      " dropout_208 (Dropout)          (None, 512)          0           ['dense_12[0][0]']               \n",
      " dense_13 (Dense)               (None, 256)          131328      ['dropout_208[0][0]']            \n",
      " dropout_209 (Dropout)          (None, 256)          0           ['dense_13[0][0]']               \n",
      " dense_14 (Dense)               (None, 1)            257         ['dropout_209[0][0]']            \n",
      "==================================================================================================\n",
      "Total params: 157,168,897\n",
      "Trainable params: 32,523,265\n",
      "Non-trainable params: 124,645,632\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "944/944 - 243s - loss: 2.0121 - accuracy: 0.7638 - val_loss: 1.9717 - val_accuracy: 0.7132 - 243s/epoch - 257ms/step\n",
      "\n",
      "Epoch 2/2\n",
      "944/944 - 296s - loss: 1.9955 - accuracy: 0.6634 - val_loss: 2.0669 - val_accuracy: 0.5038 - 296s/epoch - 314ms/step\n",
      "\n",
      "0.7132362723350525\n",
      "1\n",
      "  0%|          | 0/1 [31:56<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with the following hyperparameters:\n",
      "(0.0001, 0.3, 0.95, 0.97, 0.01, 0.0005, 128, 'tanh', 512, 256)\n",
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_5 (TFRobertaM  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " odel)                          thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_11 (S  (None, 53, 768)     0           ['tf_roberta_model_5[0][0]']     \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_247 (Dropout)          (None, 53, 768)      0           ['tf.__operators__.getitem_11[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_10 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_247[0][0]']            \n",
      " l)                                                                                               \n",
      " dropout_248 (Dropout)          (None, 53, 1024)     0           ['bidirectional_10[0][0]']       \n",
      " tf.__operators__.getitem_10 (S  (None, 768)         0           ['tf_roberta_model_5[0][0]']     \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_11 (Bidirectiona  (None, 1024)        16785408    ['dropout_248[0][0]']            \n",
      " l)                                                                                               \n",
      " tf.concat_5 (TFOpLambda)       (None, 1792)         0           ['tf.__operators__.getitem_10[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_11[0][0]']       \n",
      " dropout_249 (Dropout)          (None, 1792)         0           ['tf.concat_5[0][0]']            \n",
      " dense_15 (Dense)               (None, 512)          918016      ['dropout_249[0][0]']            \n",
      " dropout_250 (Dropout)          (None, 512)          0           ['dense_15[0][0]']               \n",
      " dense_16 (Dense)               (None, 256)          131328      ['dropout_250[0][0]']            \n",
      " dropout_251 (Dropout)          (None, 256)          0           ['dense_16[0][0]']               \n",
      " dense_17 (Dense)               (None, 1)            257         ['dropout_251[0][0]']            \n",
      "==================================================================================================\n",
      "Total params: 157,168,897\n",
      "Trainable params: 157,168,897\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "  0%|          | 0/1 [32:43<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_5/roberta/pooler/dense/kernel:0', 'tf_roberta_model_5/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_5/roberta/pooler/dense/kernel:0', 'tf_roberta_model_5/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_5/roberta/pooler/dense/kernel:0', 'tf_roberta_model_5/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_5/roberta/pooler/dense/kernel:0', 'tf_roberta_model_5/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "944/944 - 369s - loss: 377.2551 - accuracy: 0.8061 - val_loss: 2.7619 - val_accuracy: 0.7980 - 369s/epoch - 391ms/step\n",
      "\n",
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_5 (TFRobertaM  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " odel)                          thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_11 (S  (None, 53, 768)     0           ['tf_roberta_model_5[0][0]']     \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_247 (Dropout)          (None, 53, 768)      0           ['tf.__operators__.getitem_11[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_10 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_247[0][0]']            \n",
      " l)                                                                                               \n",
      " dropout_248 (Dropout)          (None, 53, 1024)     0           ['bidirectional_10[0][0]']       \n",
      " tf.__operators__.getitem_10 (S  (None, 768)         0           ['tf_roberta_model_5[0][0]']     \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_11 (Bidirectiona  (None, 1024)        16785408    ['dropout_248[0][0]']            \n",
      " l)                                                                                               \n",
      " tf.concat_5 (TFOpLambda)       (None, 1792)         0           ['tf.__operators__.getitem_10[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_11[0][0]']       \n",
      " dropout_249 (Dropout)          (None, 1792)         0           ['tf.concat_5[0][0]']            \n",
      " dense_15 (Dense)               (None, 512)          918016      ['dropout_249[0][0]']            \n",
      " dropout_250 (Dropout)          (None, 512)          0           ['dense_15[0][0]']               \n",
      " dense_16 (Dense)               (None, 256)          131328      ['dropout_250[0][0]']            \n",
      " dropout_251 (Dropout)          (None, 256)          0           ['dense_16[0][0]']               \n",
      " dense_17 (Dense)               (None, 1)            257         ['dropout_251[0][0]']            \n",
      "==================================================================================================\n",
      "Total params: 157,168,897\n",
      "Trainable params: 32,523,265\n",
      "Non-trainable params: 124,645,632\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "944/944 - 243s - loss: 2.0887 - accuracy: 0.6695 - val_loss: 2.0196 - val_accuracy: 0.6520 - 243s/epoch - 258ms/step\n",
      "\n",
      "Epoch 2/2\n",
      "944/944 - 295s - loss: 2.0377 - accuracy: 0.6069 - val_loss: 2.0637 - val_accuracy: 0.5026 - 295s/epoch - 312ms/step\n",
      "\n",
      "0.6520185470581055\n",
      "1\n",
      "FINISHED THE HYPERPARAMTER TUNING\n",
      "0.6215337614218394\n",
      "THE RESULTS ARE:\n",
      "{'acc': 0.6215337614218394, 'epochs': '[1, 1, 1]', 'status': 'ok', 'space': (0.0001, 0.3, 0.95, 0.97, 0.01, 0.0005, 128, 'tanh', 512, 256)}\n",
      "  0%|          | 0/1 [47:52<?, ?it/s, best loss: ?]\n",
      "Starting from scratch: nesw trials.\n",
      "FINISHED FMIN\n",
      "best:\n",
      "local variable 'best' referenced before assignment\n",
      "Optimizing New Model\n",
      "Attempt to resume a past training if it exists:\n",
      "Found saved Trials! Loading...\n",
      "Rerunning from 0 trials to add another one.\n",
      "  0%|          | 0/1 [00:00<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with the following hyperparameters:\n",
      "(5e-06, 0.6, 0.99, 0.97, 0.0001, 0.0005, 128, 'relu', 512, 256)\n",
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_6 (TFRobertaM  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " odel)                          thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_13 (S  (None, 53, 768)     0           ['tf_roberta_model_6[0][0]']     \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_289 (Dropout)          (None, 53, 768)      0           ['tf.__operators__.getitem_13[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_12 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_289[0][0]']            \n",
      " l)                                                                                               \n",
      " dropout_290 (Dropout)          (None, 53, 1024)     0           ['bidirectional_12[0][0]']       \n",
      " tf.__operators__.getitem_12 (S  (None, 768)         0           ['tf_roberta_model_6[0][0]']     \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_13 (Bidirectiona  (None, 1024)        16785408    ['dropout_290[0][0]']            \n",
      " l)                                                                                               \n",
      " tf.concat_6 (TFOpLambda)       (None, 1792)         0           ['tf.__operators__.getitem_12[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_13[0][0]']       \n",
      " dropout_291 (Dropout)          (None, 1792)         0           ['tf.concat_6[0][0]']            \n",
      " dense_18 (Dense)               (None, 512)          918016      ['dropout_291[0][0]']            \n",
      " dropout_292 (Dropout)          (None, 512)          0           ['dense_18[0][0]']               \n",
      " dense_19 (Dense)               (None, 256)          131328      ['dropout_292[0][0]']            \n",
      " dropout_293 (Dropout)          (None, 256)          0           ['dense_19[0][0]']               \n",
      " dense_20 (Dense)               (None, 1)            257         ['dropout_293[0][0]']            \n",
      "==================================================================================================\n",
      "Total params: 157,168,897\n",
      "Trainable params: 157,168,897\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "  0%|          | 0/1 [00:47<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_6/roberta/pooler/dense/kernel:0', 'tf_roberta_model_6/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_6/roberta/pooler/dense/kernel:0', 'tf_roberta_model_6/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_6/roberta/pooler/dense/kernel:0', 'tf_roberta_model_6/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_6/roberta/pooler/dense/kernel:0', 'tf_roberta_model_6/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "944/944 - 367s - loss: 27.4303 - accuracy: 0.6710 - val_loss: 23.8451 - val_accuracy: 0.8491 - 367s/epoch - 388ms/step\n",
      "\n",
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_6 (TFRobertaM  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " odel)                          thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_13 (S  (None, 53, 768)     0           ['tf_roberta_model_6[0][0]']     \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_289 (Dropout)          (None, 53, 768)      0           ['tf.__operators__.getitem_13[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_12 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_289[0][0]']            \n",
      " l)                                                                                               \n",
      " dropout_290 (Dropout)          (None, 53, 1024)     0           ['bidirectional_12[0][0]']       \n",
      " tf.__operators__.getitem_12 (S  (None, 768)         0           ['tf_roberta_model_6[0][0]']     \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_13 (Bidirectiona  (None, 1024)        16785408    ['dropout_290[0][0]']            \n",
      " l)                                                                                               \n",
      " tf.concat_6 (TFOpLambda)       (None, 1792)         0           ['tf.__operators__.getitem_12[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_13[0][0]']       \n",
      " dropout_291 (Dropout)          (None, 1792)         0           ['tf.concat_6[0][0]']            \n",
      " dense_18 (Dense)               (None, 512)          918016      ['dropout_291[0][0]']            \n",
      " dropout_292 (Dropout)          (None, 512)          0           ['dense_18[0][0]']               \n",
      " dense_19 (Dense)               (None, 256)          131328      ['dropout_292[0][0]']            \n",
      " dropout_293 (Dropout)          (None, 256)          0           ['dense_19[0][0]']               \n",
      " dense_20 (Dense)               (None, 1)            257         ['dropout_293[0][0]']            \n",
      "==================================================================================================\n",
      "Total params: 157,168,897\n",
      "Trainable params: 32,523,265\n",
      "Non-trainable params: 124,645,632\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "944/944 - 243s - loss: 20.8571 - accuracy: 0.8459 - val_loss: 17.9773 - val_accuracy: 0.8590 - 243s/epoch - 257ms/step\n",
      "\n",
      "Epoch 2/2\n",
      "944/944 - 297s - loss: 15.5303 - accuracy: 0.8620 - val_loss: 13.2271 - val_accuracy: 0.8689 - 297s/epoch - 314ms/step\n",
      "\n",
      "0.868913471698761\n",
      "2\n",
      "  0%|          | 0/1 [15:56<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with the following hyperparameters:\n",
      "(5e-06, 0.6, 0.99, 0.97, 0.0001, 0.0005, 128, 'relu', 512, 256)\n",
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_7 (TFRobertaM  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " odel)                          thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_15 (S  (None, 53, 768)     0           ['tf_roberta_model_7[0][0]']     \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_331 (Dropout)          (None, 53, 768)      0           ['tf.__operators__.getitem_15[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_14 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_331[0][0]']            \n",
      " l)                                                                                               \n",
      " dropout_332 (Dropout)          (None, 53, 1024)     0           ['bidirectional_14[0][0]']       \n",
      " tf.__operators__.getitem_14 (S  (None, 768)         0           ['tf_roberta_model_7[0][0]']     \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_15 (Bidirectiona  (None, 1024)        16785408    ['dropout_332[0][0]']            \n",
      " l)                                                                                               \n",
      " tf.concat_7 (TFOpLambda)       (None, 1792)         0           ['tf.__operators__.getitem_14[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_15[0][0]']       \n",
      " dropout_333 (Dropout)          (None, 1792)         0           ['tf.concat_7[0][0]']            \n",
      " dense_21 (Dense)               (None, 512)          918016      ['dropout_333[0][0]']            \n",
      " dropout_334 (Dropout)          (None, 512)          0           ['dense_21[0][0]']               \n",
      " dense_22 (Dense)               (None, 256)          131328      ['dropout_334[0][0]']            \n",
      " dropout_335 (Dropout)          (None, 256)          0           ['dense_22[0][0]']               \n",
      " dense_23 (Dense)               (None, 1)            257         ['dropout_335[0][0]']            \n",
      "==================================================================================================\n",
      "Total params: 157,168,897\n",
      "Trainable params: 157,168,897\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "  0%|          | 0/1 [16:44<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_7/roberta/pooler/dense/kernel:0', 'tf_roberta_model_7/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_7/roberta/pooler/dense/kernel:0', 'tf_roberta_model_7/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_7/roberta/pooler/dense/kernel:0', 'tf_roberta_model_7/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_7/roberta/pooler/dense/kernel:0', 'tf_roberta_model_7/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "944/944 - 368s - loss: 27.4778 - accuracy: 0.6697 - val_loss: 23.9110 - val_accuracy: 0.8463 - 368s/epoch - 390ms/step\n",
      "\n",
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_7 (TFRobertaM  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " odel)                          thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_15 (S  (None, 53, 768)     0           ['tf_roberta_model_7[0][0]']     \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_331 (Dropout)          (None, 53, 768)      0           ['tf.__operators__.getitem_15[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_14 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_331[0][0]']            \n",
      " l)                                                                                               \n",
      " dropout_332 (Dropout)          (None, 53, 1024)     0           ['bidirectional_14[0][0]']       \n",
      " tf.__operators__.getitem_14 (S  (None, 768)         0           ['tf_roberta_model_7[0][0]']     \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_15 (Bidirectiona  (None, 1024)        16785408    ['dropout_332[0][0]']            \n",
      " l)                                                                                               \n",
      " tf.concat_7 (TFOpLambda)       (None, 1792)         0           ['tf.__operators__.getitem_14[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_15[0][0]']       \n",
      " dropout_333 (Dropout)          (None, 1792)         0           ['tf.concat_7[0][0]']            \n",
      " dense_21 (Dense)               (None, 512)          918016      ['dropout_333[0][0]']            \n",
      " dropout_334 (Dropout)          (None, 512)          0           ['dense_21[0][0]']               \n",
      " dense_22 (Dense)               (None, 256)          131328      ['dropout_334[0][0]']            \n",
      " dropout_335 (Dropout)          (None, 256)          0           ['dense_22[0][0]']               \n",
      " dense_23 (Dense)               (None, 1)            257         ['dropout_335[0][0]']            \n",
      "==================================================================================================\n",
      "Total params: 157,168,897\n",
      "Trainable params: 32,523,265\n",
      "Non-trainable params: 124,645,632\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "944/944 - 243s - loss: 20.9304 - accuracy: 0.8437 - val_loss: 18.0708 - val_accuracy: 0.8544 - 243s/epoch - 258ms/step\n",
      "\n",
      "Epoch 2/2\n",
      "944/944 - 297s - loss: 15.6236 - accuracy: 0.8615 - val_loss: 13.3367 - val_accuracy: 0.8629 - 297s/epoch - 315ms/step\n",
      "\n",
      "0.8628557324409485\n",
      "2\n",
      "  0%|          | 0/1 [31:55<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with the following hyperparameters:\n",
      "(5e-06, 0.6, 0.99, 0.97, 0.0001, 0.0005, 128, 'relu', 512, 256)\n",
      "Model: \"model_8\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_8 (TFRobertaM  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " odel)                          thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_17 (S  (None, 53, 768)     0           ['tf_roberta_model_8[0][0]']     \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_373 (Dropout)          (None, 53, 768)      0           ['tf.__operators__.getitem_17[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_16 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_373[0][0]']            \n",
      " l)                                                                                               \n",
      " dropout_374 (Dropout)          (None, 53, 1024)     0           ['bidirectional_16[0][0]']       \n",
      " tf.__operators__.getitem_16 (S  (None, 768)         0           ['tf_roberta_model_8[0][0]']     \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_17 (Bidirectiona  (None, 1024)        16785408    ['dropout_374[0][0]']            \n",
      " l)                                                                                               \n",
      " tf.concat_8 (TFOpLambda)       (None, 1792)         0           ['tf.__operators__.getitem_16[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_17[0][0]']       \n",
      " dropout_375 (Dropout)          (None, 1792)         0           ['tf.concat_8[0][0]']            \n",
      " dense_24 (Dense)               (None, 512)          918016      ['dropout_375[0][0]']            \n",
      " dropout_376 (Dropout)          (None, 512)          0           ['dense_24[0][0]']               \n",
      " dense_25 (Dense)               (None, 256)          131328      ['dropout_376[0][0]']            \n",
      " dropout_377 (Dropout)          (None, 256)          0           ['dense_25[0][0]']               \n",
      " dense_26 (Dense)               (None, 1)            257         ['dropout_377[0][0]']            \n",
      "==================================================================================================\n",
      "Total params: 157,168,897\n",
      "Trainable params: 157,168,897\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "  0%|          | 0/1 [32:44<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_8/roberta/pooler/dense/kernel:0', 'tf_roberta_model_8/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_8/roberta/pooler/dense/kernel:0', 'tf_roberta_model_8/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_8/roberta/pooler/dense/kernel:0', 'tf_roberta_model_8/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_8/roberta/pooler/dense/kernel:0', 'tf_roberta_model_8/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "944/944 - 370s - loss: 27.4326 - accuracy: 0.6871 - val_loss: 23.8720 - val_accuracy: 0.8489 - 370s/epoch - 392ms/step\n",
      "\n",
      "Model: \"model_8\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_8 (TFRobertaM  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " odel)                          thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_17 (S  (None, 53, 768)     0           ['tf_roberta_model_8[0][0]']     \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_373 (Dropout)          (None, 53, 768)      0           ['tf.__operators__.getitem_17[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_16 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_373[0][0]']            \n",
      " l)                                                                                               \n",
      " dropout_374 (Dropout)          (None, 53, 1024)     0           ['bidirectional_16[0][0]']       \n",
      " tf.__operators__.getitem_16 (S  (None, 768)         0           ['tf_roberta_model_8[0][0]']     \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_17 (Bidirectiona  (None, 1024)        16785408    ['dropout_374[0][0]']            \n",
      " l)                                                                                               \n",
      " tf.concat_8 (TFOpLambda)       (None, 1792)         0           ['tf.__operators__.getitem_16[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_17[0][0]']       \n",
      " dropout_375 (Dropout)          (None, 1792)         0           ['tf.concat_8[0][0]']            \n",
      " dense_24 (Dense)               (None, 512)          918016      ['dropout_375[0][0]']            \n",
      " dropout_376 (Dropout)          (None, 512)          0           ['dense_24[0][0]']               \n",
      " dense_25 (Dense)               (None, 256)          131328      ['dropout_376[0][0]']            \n",
      " dropout_377 (Dropout)          (None, 256)          0           ['dense_25[0][0]']               \n",
      " dense_26 (Dense)               (None, 1)            257         ['dropout_377[0][0]']            \n",
      "==================================================================================================\n",
      "Total params: 157,168,897\n",
      "Trainable params: 32,523,265\n",
      "Non-trainable params: 124,645,632\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "944/944 - 244s - loss: 20.8813 - accuracy: 0.8464 - val_loss: 18.0096 - val_accuracy: 0.8616 - 244s/epoch - 258ms/step\n",
      "\n",
      "Epoch 2/2\n",
      "944/944 - 299s - loss: 15.5623 - accuracy: 0.8629 - val_loss: 13.2696 - val_accuracy: 0.8674 - 299s/epoch - 316ms/step\n",
      "\n",
      "0.8673726320266724\n",
      "2\n",
      "FINISHED THE HYPERPARAMTER TUNING\n",
      "0.8663806120554606\n",
      "THE RESULTS ARE:\n",
      "{'acc': 0.8663806120554606, 'epochs': '[2, 2, 2]', 'status': 'ok', 'space': (5e-06, 0.6, 0.99, 0.97, 0.0001, 0.0005, 128, 'relu', 512, 256)}\n",
      "  0%|          | 0/1 [47:59<?, ?it/s, best loss: ?]\n",
      "Starting from scratch: nesw trials.\n",
      "FINISHED FMIN\n",
      "best:\n",
      "local variable 'best' referenced before assignment\n",
      "Optimizing New Model\n",
      "Attempt to resume a past training if it exists:\n",
      "Found saved Trials! Loading...\n",
      "Rerunning from 0 trials to add another one.\n",
      "  0%|          | 0/1 [00:00<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with the following hyperparameters:\n",
      "(1e-05, 0.5, 0.97, 0.9, 0.001, 0.001, 256, 'tanh', 1024, 256)\n",
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_9 (TFRobertaM  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " odel)                          thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_19 (S  (None, 53, 768)     0           ['tf_roberta_model_9[0][0]']     \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_415 (Dropout)          (None, 53, 768)      0           ['tf.__operators__.getitem_19[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_18 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_415[0][0]']            \n",
      " l)                                                                                               \n",
      " dropout_416 (Dropout)          (None, 53, 1024)     0           ['bidirectional_18[0][0]']       \n",
      " tf.__operators__.getitem_18 (S  (None, 768)         0           ['tf_roberta_model_9[0][0]']     \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_19 (Bidirectiona  (None, 1024)        16785408    ['dropout_416[0][0]']            \n",
      " l)                                                                                               \n",
      " tf.concat_9 (TFOpLambda)       (None, 1792)         0           ['tf.__operators__.getitem_18[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_19[0][0]']       \n",
      " dropout_417 (Dropout)          (None, 1792)         0           ['tf.concat_9[0][0]']            \n",
      " dense_27 (Dense)               (None, 1024)         1836032     ['dropout_417[0][0]']            \n",
      " dropout_418 (Dropout)          (None, 1024)         0           ['dense_27[0][0]']               \n",
      " dense_28 (Dense)               (None, 256)          262400      ['dropout_418[0][0]']            \n",
      " dropout_419 (Dropout)          (None, 256)          0           ['dense_28[0][0]']               \n",
      " dense_29 (Dense)               (None, 1)            257         ['dropout_419[0][0]']            \n",
      "==================================================================================================\n",
      "Total params: 158,217,985\n",
      "Trainable params: 158,217,985\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "  0%|          | 0/1 [00:49<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_9/roberta/pooler/dense/kernel:0', 'tf_roberta_model_9/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_9/roberta/pooler/dense/kernel:0', 'tf_roberta_model_9/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_9/roberta/pooler/dense/kernel:0', 'tf_roberta_model_9/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_9/roberta/pooler/dense/kernel:0', 'tf_roberta_model_9/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472/472 - 282s - loss: 259.7688 - accuracy: 0.7803 - val_loss: 223.5496 - val_accuracy: 0.8616 - 282s/epoch - 598ms/step\n",
      "\n",
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_9 (TFRobertaM  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " odel)                          thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_19 (S  (None, 53, 768)     0           ['tf_roberta_model_9[0][0]']     \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_415 (Dropout)          (None, 53, 768)      0           ['tf.__operators__.getitem_19[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_18 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_415[0][0]']            \n",
      " l)                                                                                               \n",
      " dropout_416 (Dropout)          (None, 53, 1024)     0           ['bidirectional_18[0][0]']       \n",
      " tf.__operators__.getitem_18 (S  (None, 768)         0           ['tf_roberta_model_9[0][0]']     \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_19 (Bidirectiona  (None, 1024)        16785408    ['dropout_416[0][0]']            \n",
      " l)                                                                                               \n",
      " tf.concat_9 (TFOpLambda)       (None, 1792)         0           ['tf.__operators__.getitem_18[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_19[0][0]']       \n",
      " dropout_417 (Dropout)          (None, 1792)         0           ['tf.concat_9[0][0]']            \n",
      " dense_27 (Dense)               (None, 1024)         1836032     ['dropout_417[0][0]']            \n",
      " dropout_418 (Dropout)          (None, 1024)         0           ['dense_27[0][0]']               \n",
      " dense_28 (Dense)               (None, 256)          262400      ['dropout_418[0][0]']            \n",
      " dropout_419 (Dropout)          (None, 256)          0           ['dense_28[0][0]']               \n",
      " dense_29 (Dense)               (None, 1)            257         ['dropout_419[0][0]']            \n",
      "==================================================================================================\n",
      "Total params: 158,217,985\n",
      "Trainable params: 33,572,353\n",
      "Non-trainable params: 124,645,632\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "472/472 - 154s - loss: 192.0038 - accuracy: 0.8600 - val_loss: 162.4756 - val_accuracy: 0.8702 - 154s/epoch - 327ms/step\n",
      "\n",
      "Epoch 2/2\n",
      "472/472 - 211s - loss: 137.2050 - accuracy: 0.8758 - val_loss: 113.9272 - val_accuracy: 0.8734 - 211s/epoch - 448ms/step\n",
      "\n",
      "0.8733640909194946\n",
      "2\n",
      "  0%|          | 0/1 [11:40<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with the following hyperparameters:\n",
      "(1e-05, 0.5, 0.97, 0.9, 0.001, 0.001, 256, 'tanh', 1024, 256)\n",
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_10 (TFRoberta  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " Model)                         thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_21 (S  (None, 53, 768)     0           ['tf_roberta_model_10[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_457 (Dropout)          (None, 53, 768)      0           ['tf.__operators__.getitem_21[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_20 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_457[0][0]']            \n",
      " l)                                                                                               \n",
      " dropout_458 (Dropout)          (None, 53, 1024)     0           ['bidirectional_20[0][0]']       \n",
      " tf.__operators__.getitem_20 (S  (None, 768)         0           ['tf_roberta_model_10[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_21 (Bidirectiona  (None, 1024)        16785408    ['dropout_458[0][0]']            \n",
      " l)                                                                                               \n",
      " tf.concat_10 (TFOpLambda)      (None, 1792)         0           ['tf.__operators__.getitem_20[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_21[0][0]']       \n",
      " dropout_459 (Dropout)          (None, 1792)         0           ['tf.concat_10[0][0]']           \n",
      " dense_30 (Dense)               (None, 1024)         1836032     ['dropout_459[0][0]']            \n",
      " dropout_460 (Dropout)          (None, 1024)         0           ['dense_30[0][0]']               \n",
      " dense_31 (Dense)               (None, 256)          262400      ['dropout_460[0][0]']            \n",
      " dropout_461 (Dropout)          (None, 256)          0           ['dense_31[0][0]']               \n",
      " dense_32 (Dense)               (None, 1)            257         ['dropout_461[0][0]']            \n",
      "==================================================================================================\n",
      "Total params: 158,217,985\n",
      "Trainable params: 158,217,985\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "  0%|          | 0/1 [12:26<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_10/roberta/pooler/dense/kernel:0', 'tf_roberta_model_10/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_10/roberta/pooler/dense/kernel:0', 'tf_roberta_model_10/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_10/roberta/pooler/dense/kernel:0', 'tf_roberta_model_10/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_10/roberta/pooler/dense/kernel:0', 'tf_roberta_model_10/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472/472 - 282s - loss: 259.6718 - accuracy: 0.7860 - val_loss: 223.4798 - val_accuracy: 0.8587 - 282s/epoch - 598ms/step\n",
      "\n",
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_10 (TFRoberta  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " Model)                         thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_21 (S  (None, 53, 768)     0           ['tf_roberta_model_10[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_457 (Dropout)          (None, 53, 768)      0           ['tf.__operators__.getitem_21[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_20 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_457[0][0]']            \n",
      " l)                                                                                               \n",
      " dropout_458 (Dropout)          (None, 53, 1024)     0           ['bidirectional_20[0][0]']       \n",
      " tf.__operators__.getitem_20 (S  (None, 768)         0           ['tf_roberta_model_10[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_21 (Bidirectiona  (None, 1024)        16785408    ['dropout_458[0][0]']            \n",
      " l)                                                                                               \n",
      " tf.concat_10 (TFOpLambda)      (None, 1792)         0           ['tf.__operators__.getitem_20[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_21[0][0]']       \n",
      " dropout_459 (Dropout)          (None, 1792)         0           ['tf.concat_10[0][0]']           \n",
      " dense_30 (Dense)               (None, 1024)         1836032     ['dropout_459[0][0]']            \n",
      " dropout_460 (Dropout)          (None, 1024)         0           ['dense_30[0][0]']               \n",
      " dense_31 (Dense)               (None, 256)          262400      ['dropout_460[0][0]']            \n",
      " dropout_461 (Dropout)          (None, 256)          0           ['dense_31[0][0]']               \n",
      " dense_32 (Dense)               (None, 1)            257         ['dropout_461[0][0]']            \n",
      "==================================================================================================\n",
      "Total params: 158,217,985\n",
      "Trainable params: 33,572,353\n",
      "Non-trainable params: 124,645,632\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "472/472 - 154s - loss: 191.9348 - accuracy: 0.8610 - val_loss: 162.4082 - val_accuracy: 0.8675 - 154s/epoch - 327ms/step\n",
      "\n",
      "Epoch 2/2\n",
      "472/472 - 210s - loss: 137.1401 - accuracy: 0.8764 - val_loss: 113.8862 - val_accuracy: 0.8680 - 210s/epoch - 444ms/step\n",
      "\n",
      "0.8679848313331604\n",
      "2\n",
      "  0%|          | 0/1 [23:15<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with the following hyperparameters:\n",
      "(1e-05, 0.5, 0.97, 0.9, 0.001, 0.001, 256, 'tanh', 1024, 256)\n",
      "Model: \"model_11\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_11 (TFRoberta  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " Model)                         thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_23 (S  (None, 53, 768)     0           ['tf_roberta_model_11[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_499 (Dropout)          (None, 53, 768)      0           ['tf.__operators__.getitem_23[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_22 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_499[0][0]']            \n",
      " l)                                                                                               \n",
      " dropout_500 (Dropout)          (None, 53, 1024)     0           ['bidirectional_22[0][0]']       \n",
      " tf.__operators__.getitem_22 (S  (None, 768)         0           ['tf_roberta_model_11[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_23 (Bidirectiona  (None, 1024)        16785408    ['dropout_500[0][0]']            \n",
      " l)                                                                                               \n",
      " tf.concat_11 (TFOpLambda)      (None, 1792)         0           ['tf.__operators__.getitem_22[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_23[0][0]']       \n",
      " dropout_501 (Dropout)          (None, 1792)         0           ['tf.concat_11[0][0]']           \n",
      " dense_33 (Dense)               (None, 1024)         1836032     ['dropout_501[0][0]']            \n",
      " dropout_502 (Dropout)          (None, 1024)         0           ['dense_33[0][0]']               \n",
      " dense_34 (Dense)               (None, 256)          262400      ['dropout_502[0][0]']            \n",
      " dropout_503 (Dropout)          (None, 256)          0           ['dense_34[0][0]']               \n",
      " dense_35 (Dense)               (None, 1)            257         ['dropout_503[0][0]']            \n",
      "==================================================================================================\n",
      "Total params: 158,217,985\n",
      "Trainable params: 158,217,985\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "  0%|          | 0/1 [24:00<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_11/roberta/pooler/dense/kernel:0', 'tf_roberta_model_11/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_11/roberta/pooler/dense/kernel:0', 'tf_roberta_model_11/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_11/roberta/pooler/dense/kernel:0', 'tf_roberta_model_11/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_11/roberta/pooler/dense/kernel:0', 'tf_roberta_model_11/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472/472 - 281s - loss: 259.5987 - accuracy: 0.7861 - val_loss: 223.4059 - val_accuracy: 0.8486 - 281s/epoch - 596ms/step\n",
      "\n",
      "Model: \"model_11\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_11 (TFRoberta  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " Model)                         thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_23 (S  (None, 53, 768)     0           ['tf_roberta_model_11[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_499 (Dropout)          (None, 53, 768)      0           ['tf.__operators__.getitem_23[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_22 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_499[0][0]']            \n",
      " l)                                                                                               \n",
      " dropout_500 (Dropout)          (None, 53, 1024)     0           ['bidirectional_22[0][0]']       \n",
      " tf.__operators__.getitem_22 (S  (None, 768)         0           ['tf_roberta_model_11[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_23 (Bidirectiona  (None, 1024)        16785408    ['dropout_500[0][0]']            \n",
      " l)                                                                                               \n",
      " tf.concat_11 (TFOpLambda)      (None, 1792)         0           ['tf.__operators__.getitem_22[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_23[0][0]']       \n",
      " dropout_501 (Dropout)          (None, 1792)         0           ['tf.concat_11[0][0]']           \n",
      " dense_33 (Dense)               (None, 1024)         1836032     ['dropout_501[0][0]']            \n",
      " dropout_502 (Dropout)          (None, 1024)         0           ['dense_33[0][0]']               \n",
      " dense_34 (Dense)               (None, 256)          262400      ['dropout_502[0][0]']            \n",
      " dropout_503 (Dropout)          (None, 256)          0           ['dense_34[0][0]']               \n",
      " dense_35 (Dense)               (None, 1)            257         ['dropout_503[0][0]']            \n",
      "==================================================================================================\n",
      "Total params: 158,217,985\n",
      "Trainable params: 33,572,353\n",
      "Non-trainable params: 124,645,632\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "472/472 - 155s - loss: 191.8286 - accuracy: 0.8616 - val_loss: 162.2907 - val_accuracy: 0.8676 - 155s/epoch - 328ms/step\n",
      "\n",
      "Epoch 2/2\n",
      "472/472 - 210s - loss: 137.0235 - accuracy: 0.8771 - val_loss: 113.7643 - val_accuracy: 0.8713 - 210s/epoch - 446ms/step\n",
      "\n",
      "0.8713104128837585\n",
      "2\n",
      "FINISHED THE HYPERPARAMTER TUNING\n",
      "0.8708864450454712\n",
      "THE RESULTS ARE:\n",
      "{'acc': 0.8708864450454712, 'epochs': '[2, 2, 2]', 'status': 'ok', 'space': (1e-05, 0.5, 0.97, 0.9, 0.001, 0.001, 256, 'tanh', 1024, 256)}\n",
      "  0%|          | 0/1 [34:49<?, ?it/s, best loss: ?]\n",
      "Starting from scratch: nesw trials.\n",
      "FINISHED FMIN\n",
      "best:\n",
      "local variable 'best' referenced before assignment\n",
      "Optimizing New Model\n",
      "Attempt to resume a past training if it exists:\n",
      "Found saved Trials! Loading...\n",
      "Rerunning from 0 trials to add another one.\n",
      "  0%|          | 0/1 [00:00<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with the following hyperparameters:\n",
      "(5e-05, 0.4, 0.95, 0.9, 0.005, 0.0001, 256, 'sigmoid', 1024, 512)\n",
      "Model: \"model_12\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_12 (TFRoberta  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " Model)                         thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_25 (S  (None, 53, 768)     0           ['tf_roberta_model_12[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_541 (Dropout)          (None, 53, 768)      0           ['tf.__operators__.getitem_25[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_24 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_541[0][0]']            \n",
      " l)                                                                                               \n",
      " dropout_542 (Dropout)          (None, 53, 1024)     0           ['bidirectional_24[0][0]']       \n",
      " tf.__operators__.getitem_24 (S  (None, 768)         0           ['tf_roberta_model_12[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_25 (Bidirectiona  (None, 1024)        16785408    ['dropout_542[0][0]']            \n",
      " l)                                                                                               \n",
      " tf.concat_12 (TFOpLambda)      (None, 1792)         0           ['tf.__operators__.getitem_24[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_25[0][0]']       \n",
      " dropout_543 (Dropout)          (None, 1792)         0           ['tf.concat_12[0][0]']           \n",
      " dense_36 (Dense)               (None, 1024)         1836032     ['dropout_543[0][0]']            \n",
      " dropout_544 (Dropout)          (None, 1024)         0           ['dense_36[0][0]']               \n",
      " dense_37 (Dense)               (None, 512)          524800      ['dropout_544[0][0]']            \n",
      " dropout_545 (Dropout)          (None, 512)          0           ['dense_37[0][0]']               \n",
      " dense_38 (Dense)               (None, 1)            513         ['dropout_545[0][0]']            \n",
      "==================================================================================================\n",
      "Total params: 158,480,641\n",
      "Trainable params: 158,480,641\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "  0%|          | 0/1 [00:46<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_12/roberta/pooler/dense/kernel:0', 'tf_roberta_model_12/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_12/roberta/pooler/dense/kernel:0', 'tf_roberta_model_12/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_12/roberta/pooler/dense/kernel:0', 'tf_roberta_model_12/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_12/roberta/pooler/dense/kernel:0', 'tf_roberta_model_12/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472/472 - 281s - loss: 748.4526 - accuracy: 0.7594 - val_loss: 248.3278 - val_accuracy: 0.8363 - 281s/epoch - 596ms/step\n",
      "\n",
      "Model: \"model_12\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_12 (TFRoberta  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " Model)                         thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_25 (S  (None, 53, 768)     0           ['tf_roberta_model_12[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_541 (Dropout)          (None, 53, 768)      0           ['tf.__operators__.getitem_25[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_24 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_541[0][0]']            \n",
      " l)                                                                                               \n",
      " dropout_542 (Dropout)          (None, 53, 1024)     0           ['bidirectional_24[0][0]']       \n",
      " tf.__operators__.getitem_24 (S  (None, 768)         0           ['tf_roberta_model_12[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_25 (Bidirectiona  (None, 1024)        16785408    ['dropout_542[0][0]']            \n",
      " l)                                                                                               \n",
      " tf.concat_12 (TFOpLambda)      (None, 1792)         0           ['tf.__operators__.getitem_24[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_25[0][0]']       \n",
      " dropout_543 (Dropout)          (None, 1792)         0           ['tf.concat_12[0][0]']           \n",
      " dense_36 (Dense)               (None, 1024)         1836032     ['dropout_543[0][0]']            \n",
      " dropout_544 (Dropout)          (None, 1024)         0           ['dense_36[0][0]']               \n",
      " dense_37 (Dense)               (None, 512)          524800      ['dropout_544[0][0]']            \n",
      " dropout_545 (Dropout)          (None, 512)          0           ['dense_37[0][0]']               \n",
      " dense_38 (Dense)               (None, 1)            513         ['dropout_545[0][0]']            \n",
      "==================================================================================================\n",
      "Total params: 158,480,641\n",
      "Trainable params: 33,835,009\n",
      "Non-trainable params: 124,645,632\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "472/472 - 155s - loss: 84.9948 - accuracy: 0.6061 - val_loss: 13.1069 - val_accuracy: 0.6544 - 155s/epoch - 328ms/step\n",
      "\n",
      "Epoch 2/2\n",
      "472/472 - 208s - loss: 5.1547 - accuracy: 0.5009 - val_loss: 1.6435 - val_accuracy: 0.5007 - 208s/epoch - 442ms/step\n",
      "\n",
      "0.6544398665428162\n",
      "1\n",
      "  0%|          | 0/1 [11:34<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with the following hyperparameters:\n",
      "(5e-05, 0.4, 0.95, 0.9, 0.005, 0.0001, 256, 'sigmoid', 1024, 512)\n",
      "Model: \"model_13\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_13 (TFRoberta  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " Model)                         thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_27 (S  (None, 53, 768)     0           ['tf_roberta_model_13[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_583 (Dropout)          (None, 53, 768)      0           ['tf.__operators__.getitem_27[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_26 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_583[0][0]']            \n",
      " l)                                                                                               \n",
      " dropout_584 (Dropout)          (None, 53, 1024)     0           ['bidirectional_26[0][0]']       \n",
      " tf.__operators__.getitem_26 (S  (None, 768)         0           ['tf_roberta_model_13[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_27 (Bidirectiona  (None, 1024)        16785408    ['dropout_584[0][0]']            \n",
      " l)                                                                                               \n",
      " tf.concat_13 (TFOpLambda)      (None, 1792)         0           ['tf.__operators__.getitem_26[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_27[0][0]']       \n",
      " dropout_585 (Dropout)          (None, 1792)         0           ['tf.concat_13[0][0]']           \n",
      " dense_39 (Dense)               (None, 1024)         1836032     ['dropout_585[0][0]']            \n",
      " dropout_586 (Dropout)          (None, 1024)         0           ['dense_39[0][0]']               \n",
      " dense_40 (Dense)               (None, 512)          524800      ['dropout_586[0][0]']            \n",
      " dropout_587 (Dropout)          (None, 512)          0           ['dense_40[0][0]']               \n",
      " dense_41 (Dense)               (None, 1)            513         ['dropout_587[0][0]']            \n",
      "==================================================================================================\n",
      "Total params: 158,480,641\n",
      "Trainable params: 158,480,641\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "  0%|          | 0/1 [12:22<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_13/roberta/pooler/dense/kernel:0', 'tf_roberta_model_13/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_13/roberta/pooler/dense/kernel:0', 'tf_roberta_model_13/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_13/roberta/pooler/dense/kernel:0', 'tf_roberta_model_13/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_13/roberta/pooler/dense/kernel:0', 'tf_roberta_model_13/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472/472 - 285s - loss: 747.7946 - accuracy: 0.7535 - val_loss: 247.8346 - val_accuracy: 0.8394 - 285s/epoch - 604ms/step\n",
      "\n",
      "Model: \"model_13\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_13 (TFRoberta  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " Model)                         thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_27 (S  (None, 53, 768)     0           ['tf_roberta_model_13[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_583 (Dropout)          (None, 53, 768)      0           ['tf.__operators__.getitem_27[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_26 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_583[0][0]']            \n",
      " l)                                                                                               \n",
      " dropout_584 (Dropout)          (None, 53, 1024)     0           ['bidirectional_26[0][0]']       \n",
      " tf.__operators__.getitem_26 (S  (None, 768)         0           ['tf_roberta_model_13[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_27 (Bidirectiona  (None, 1024)        16785408    ['dropout_584[0][0]']            \n",
      " l)                                                                                               \n",
      " tf.concat_13 (TFOpLambda)      (None, 1792)         0           ['tf.__operators__.getitem_26[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_27[0][0]']       \n",
      " dropout_585 (Dropout)          (None, 1792)         0           ['tf.concat_13[0][0]']           \n",
      " dense_39 (Dense)               (None, 1024)         1836032     ['dropout_585[0][0]']            \n",
      " dropout_586 (Dropout)          (None, 1024)         0           ['dense_39[0][0]']               \n",
      " dense_40 (Dense)               (None, 512)          524800      ['dropout_586[0][0]']            \n",
      " dropout_587 (Dropout)          (None, 512)          0           ['dense_40[0][0]']               \n",
      " dense_41 (Dense)               (None, 1)            513         ['dropout_587[0][0]']            \n",
      "==================================================================================================\n",
      "Total params: 158,480,641\n",
      "Trainable params: 33,835,009\n",
      "Non-trainable params: 124,645,632\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "472/472 - 154s - loss: 84.8244 - accuracy: 0.5983 - val_loss: 13.0710 - val_accuracy: 0.5038 - 154s/epoch - 327ms/step\n",
      "\n",
      "Epoch 2/2\n",
      "472/472 - 209s - loss: 5.1231 - accuracy: 0.5045 - val_loss: 1.6217 - val_accuracy: 0.5038 - 209s/epoch - 443ms/step\n",
      "\n",
      "0.5038385391235352\n",
      "1\n",
      "  0%|          | 0/1 [23:14<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with the following hyperparameters:\n",
      "(5e-05, 0.4, 0.95, 0.9, 0.005, 0.0001, 256, 'sigmoid', 1024, 512)\n",
      "Model: \"model_14\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_14 (TFRoberta  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " Model)                         thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_29 (S  (None, 53, 768)     0           ['tf_roberta_model_14[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_625 (Dropout)          (None, 53, 768)      0           ['tf.__operators__.getitem_29[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_28 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_625[0][0]']            \n",
      " l)                                                                                               \n",
      " dropout_626 (Dropout)          (None, 53, 1024)     0           ['bidirectional_28[0][0]']       \n",
      " tf.__operators__.getitem_28 (S  (None, 768)         0           ['tf_roberta_model_14[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_29 (Bidirectiona  (None, 1024)        16785408    ['dropout_626[0][0]']            \n",
      " l)                                                                                               \n",
      " tf.concat_14 (TFOpLambda)      (None, 1792)         0           ['tf.__operators__.getitem_28[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_29[0][0]']       \n",
      " dropout_627 (Dropout)          (None, 1792)         0           ['tf.concat_14[0][0]']           \n",
      " dense_42 (Dense)               (None, 1024)         1836032     ['dropout_627[0][0]']            \n",
      " dropout_628 (Dropout)          (None, 1024)         0           ['dense_42[0][0]']               \n",
      " dense_43 (Dense)               (None, 512)          524800      ['dropout_628[0][0]']            \n",
      " dropout_629 (Dropout)          (None, 512)          0           ['dense_43[0][0]']               \n",
      " dense_44 (Dense)               (None, 1)            513         ['dropout_629[0][0]']            \n",
      "==================================================================================================\n",
      "Total params: 158,480,641\n",
      "Trainable params: 158,480,641\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "  0%|          | 0/1 [24:00<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_14/roberta/pooler/dense/kernel:0', 'tf_roberta_model_14/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_14/roberta/pooler/dense/kernel:0', 'tf_roberta_model_14/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_14/roberta/pooler/dense/kernel:0', 'tf_roberta_model_14/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_14/roberta/pooler/dense/kernel:0', 'tf_roberta_model_14/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472/472 - 280s - loss: 748.1220 - accuracy: 0.7721 - val_loss: 248.8858 - val_accuracy: 0.8349 - 280s/epoch - 593ms/step\n",
      "\n",
      "Model: \"model_14\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_14 (TFRoberta  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " Model)                         thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_29 (S  (None, 53, 768)     0           ['tf_roberta_model_14[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_625 (Dropout)          (None, 53, 768)      0           ['tf.__operators__.getitem_29[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_28 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_625[0][0]']            \n",
      " l)                                                                                               \n",
      " dropout_626 (Dropout)          (None, 53, 1024)     0           ['bidirectional_28[0][0]']       \n",
      " tf.__operators__.getitem_28 (S  (None, 768)         0           ['tf_roberta_model_14[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_29 (Bidirectiona  (None, 1024)        16785408    ['dropout_626[0][0]']            \n",
      " l)                                                                                               \n",
      " tf.concat_14 (TFOpLambda)      (None, 1792)         0           ['tf.__operators__.getitem_28[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_29[0][0]']       \n",
      " dropout_627 (Dropout)          (None, 1792)         0           ['tf.concat_14[0][0]']           \n",
      " dense_42 (Dense)               (None, 1024)         1836032     ['dropout_627[0][0]']            \n",
      " dropout_628 (Dropout)          (None, 1024)         0           ['dense_42[0][0]']               \n",
      " dense_43 (Dense)               (None, 512)          524800      ['dropout_628[0][0]']            \n",
      " dropout_629 (Dropout)          (None, 512)          0           ['dense_43[0][0]']               \n",
      " dense_44 (Dense)               (None, 1)            513         ['dropout_629[0][0]']            \n",
      "==================================================================================================\n",
      "Total params: 158,480,641\n",
      "Trainable params: 33,835,009\n",
      "Non-trainable params: 124,645,632\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "472/472 - 155s - loss: 85.2065 - accuracy: 0.6039 - val_loss: 13.0894 - val_accuracy: 0.5026 - 155s/epoch - 329ms/step\n",
      "\n",
      "Epoch 2/2\n",
      "472/472 - 209s - loss: 5.0967 - accuracy: 0.5018 - val_loss: 1.6001 - val_accuracy: 0.5026 - 209s/epoch - 443ms/step\n",
      "\n",
      "0.5025811195373535\n",
      "1\n",
      "FINISHED THE HYPERPARAMTER TUNING\n",
      "0.5536198417345682\n",
      "THE RESULTS ARE:\n",
      "{'acc': 0.5536198417345682, 'epochs': '[1, 1, 1]', 'status': 'ok', 'space': (5e-05, 0.4, 0.95, 0.9, 0.005, 0.0001, 256, 'sigmoid', 1024, 512)}\n",
      "  0%|          | 0/1 [34:48<?, ?it/s, best loss: ?]\n",
      "Starting from scratch: nesw trials.\n",
      "FINISHED FMIN\n",
      "best:\n",
      "local variable 'best' referenced before assignment\n",
      "Optimizing New Model\n",
      "Attempt to resume a past training if it exists:\n",
      "Found saved Trials! Loading...\n",
      "Rerunning from 0 trials to add another one.\n",
      "  0%|          | 0/1 [00:00<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with the following hyperparameters:\n",
      "(1e-06, 0.6, 0.97, 0.99, 0.0005, 0, 128, 'sigmoid', 1024, 512)\n",
      "Model: \"model_15\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_15 (TFRoberta  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " Model)                         thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_31 (S  (None, 53, 768)     0           ['tf_roberta_model_15[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_667 (Dropout)          (None, 53, 768)      0           ['tf.__operators__.getitem_31[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_30 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_667[0][0]']            \n",
      " l)                                                                                               \n",
      " dropout_668 (Dropout)          (None, 53, 1024)     0           ['bidirectional_30[0][0]']       \n",
      " tf.__operators__.getitem_30 (S  (None, 768)         0           ['tf_roberta_model_15[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_31 (Bidirectiona  (None, 1024)        16785408    ['dropout_668[0][0]']            \n",
      " l)                                                                                               \n",
      " tf.concat_15 (TFOpLambda)      (None, 1792)         0           ['tf.__operators__.getitem_30[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_31[0][0]']       \n",
      " dropout_669 (Dropout)          (None, 1792)         0           ['tf.concat_15[0][0]']           \n",
      " dense_45 (Dense)               (None, 1024)         1836032     ['dropout_669[0][0]']            \n",
      " dropout_670 (Dropout)          (None, 1024)         0           ['dense_45[0][0]']               \n",
      " dense_46 (Dense)               (None, 512)          524800      ['dropout_670[0][0]']            \n",
      " dropout_671 (Dropout)          (None, 512)          0           ['dense_46[0][0]']               \n",
      " dense_47 (Dense)               (None, 1)            513         ['dropout_671[0][0]']            \n",
      "==================================================================================================\n",
      "Total params: 158,480,641\n",
      "Trainable params: 158,480,641\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "  0%|          | 0/1 [00:48<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_15/roberta/pooler/dense/kernel:0', 'tf_roberta_model_15/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_15/roberta/pooler/dense/kernel:0', 'tf_roberta_model_15/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_15/roberta/pooler/dense/kernel:0', 'tf_roberta_model_15/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_15/roberta/pooler/dense/kernel:0', 'tf_roberta_model_15/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "944/944 - 372s - loss: 145.2019 - accuracy: 0.5402 - val_loss: 141.1254 - val_accuracy: 0.8131 - 372s/epoch - 394ms/step\n",
      "\n",
      "Model: \"model_15\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_15 (TFRoberta  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " Model)                         thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_31 (S  (None, 53, 768)     0           ['tf_roberta_model_15[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_667 (Dropout)          (None, 53, 768)      0           ['tf.__operators__.getitem_31[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_30 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_667[0][0]']            \n",
      " l)                                                                                               \n",
      " dropout_668 (Dropout)          (None, 53, 1024)     0           ['bidirectional_30[0][0]']       \n",
      " tf.__operators__.getitem_30 (S  (None, 768)         0           ['tf_roberta_model_15[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_31 (Bidirectiona  (None, 1024)        16785408    ['dropout_668[0][0]']            \n",
      " l)                                                                                               \n",
      " tf.concat_15 (TFOpLambda)      (None, 1792)         0           ['tf.__operators__.getitem_30[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_31[0][0]']       \n",
      " dropout_669 (Dropout)          (None, 1792)         0           ['tf.concat_15[0][0]']           \n",
      " dense_45 (Dense)               (None, 1024)         1836032     ['dropout_669[0][0]']            \n",
      " dropout_670 (Dropout)          (None, 1024)         0           ['dense_45[0][0]']               \n",
      " dense_46 (Dense)               (None, 512)          524800      ['dropout_670[0][0]']            \n",
      " dropout_671 (Dropout)          (None, 512)          0           ['dense_46[0][0]']               \n",
      " dense_47 (Dense)               (None, 1)            513         ['dropout_671[0][0]']            \n",
      "==================================================================================================\n",
      "Total params: 158,480,641\n",
      "Trainable params: 33,835,009\n",
      "Non-trainable params: 124,645,632\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "944/944 - 244s - loss: 137.3993 - accuracy: 0.6770 - val_loss: 133.4600 - val_accuracy: 0.8398 - 244s/epoch - 259ms/step\n",
      "\n",
      "Epoch 2/2\n",
      "944/944 - 298s - loss: 129.8852 - accuracy: 0.7884 - val_loss: 126.1579 - val_accuracy: 0.8496 - 298s/epoch - 316ms/step\n",
      "\n",
      "0.8496053814888\n",
      "2\n",
      "  0%|          | 0/1 [16:06<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with the following hyperparameters:\n",
      "(1e-06, 0.6, 0.97, 0.99, 0.0005, 0, 128, 'sigmoid', 1024, 512)\n",
      "Model: \"model_16\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_16 (TFRoberta  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " Model)                         thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_33 (S  (None, 53, 768)     0           ['tf_roberta_model_16[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_709 (Dropout)          (None, 53, 768)      0           ['tf.__operators__.getitem_33[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_32 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_709[0][0]']            \n",
      " l)                                                                                               \n",
      " dropout_710 (Dropout)          (None, 53, 1024)     0           ['bidirectional_32[0][0]']       \n",
      " tf.__operators__.getitem_32 (S  (None, 768)         0           ['tf_roberta_model_16[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_33 (Bidirectiona  (None, 1024)        16785408    ['dropout_710[0][0]']            \n",
      " l)                                                                                               \n",
      " tf.concat_16 (TFOpLambda)      (None, 1792)         0           ['tf.__operators__.getitem_32[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_33[0][0]']       \n",
      " dropout_711 (Dropout)          (None, 1792)         0           ['tf.concat_16[0][0]']           \n",
      " dense_48 (Dense)               (None, 1024)         1836032     ['dropout_711[0][0]']            \n",
      " dropout_712 (Dropout)          (None, 1024)         0           ['dense_48[0][0]']               \n",
      " dense_49 (Dense)               (None, 512)          524800      ['dropout_712[0][0]']            \n",
      " dropout_713 (Dropout)          (None, 512)          0           ['dense_49[0][0]']               \n",
      " dense_50 (Dense)               (None, 1)            513         ['dropout_713[0][0]']            \n",
      "==================================================================================================\n",
      "Total params: 158,480,641\n",
      "Trainable params: 158,480,641\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "  0%|          | 0/1 [16:53<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_16/roberta/pooler/dense/kernel:0', 'tf_roberta_model_16/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_16/roberta/pooler/dense/kernel:0', 'tf_roberta_model_16/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_16/roberta/pooler/dense/kernel:0', 'tf_roberta_model_16/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_16/roberta/pooler/dense/kernel:0', 'tf_roberta_model_16/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "944/944 - 367s - loss: 145.2378 - accuracy: 0.5375 - val_loss: 141.1095 - val_accuracy: 0.8107 - 367s/epoch - 389ms/step\n",
      "\n",
      "Model: \"model_16\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_16 (TFRoberta  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " Model)                         thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_33 (S  (None, 53, 768)     0           ['tf_roberta_model_16[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_709 (Dropout)          (None, 53, 768)      0           ['tf.__operators__.getitem_33[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_32 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_709[0][0]']            \n",
      " l)                                                                                               \n",
      " dropout_710 (Dropout)          (None, 53, 1024)     0           ['bidirectional_32[0][0]']       \n",
      " tf.__operators__.getitem_32 (S  (None, 768)         0           ['tf_roberta_model_16[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_33 (Bidirectiona  (None, 1024)        16785408    ['dropout_710[0][0]']            \n",
      " l)                                                                                               \n",
      " tf.concat_16 (TFOpLambda)      (None, 1792)         0           ['tf.__operators__.getitem_32[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_33[0][0]']       \n",
      " dropout_711 (Dropout)          (None, 1792)         0           ['tf.concat_16[0][0]']           \n",
      " dense_48 (Dense)               (None, 1024)         1836032     ['dropout_711[0][0]']            \n",
      " dropout_712 (Dropout)          (None, 1024)         0           ['dense_48[0][0]']               \n",
      " dense_49 (Dense)               (None, 512)          524800      ['dropout_712[0][0]']            \n",
      " dropout_713 (Dropout)          (None, 512)          0           ['dense_49[0][0]']               \n",
      " dense_50 (Dense)               (None, 1)            513         ['dropout_713[0][0]']            \n",
      "==================================================================================================\n",
      "Total params: 158,480,641\n",
      "Trainable params: 33,835,009\n",
      "Non-trainable params: 124,645,632\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "944/944 - 244s - loss: 137.3946 - accuracy: 0.7323 - val_loss: 133.5046 - val_accuracy: 0.8398 - 244s/epoch - 259ms/step\n",
      "\n",
      "Epoch 2/2\n",
      "944/944 - 299s - loss: 129.9243 - accuracy: 0.7984 - val_loss: 126.2105 - val_accuracy: 0.8469 - 299s/epoch - 317ms/step\n",
      "\n",
      "0.8469060659408569\n",
      "2\n",
      "  0%|          | 0/1 [32:06<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with the following hyperparameters:\n",
      "(1e-06, 0.6, 0.97, 0.99, 0.0005, 0, 128, 'sigmoid', 1024, 512)\n",
      "Model: \"model_17\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_17 (TFRoberta  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " Model)                         thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_35 (S  (None, 53, 768)     0           ['tf_roberta_model_17[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_751 (Dropout)          (None, 53, 768)      0           ['tf.__operators__.getitem_35[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_34 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_751[0][0]']            \n",
      " l)                                                                                               \n",
      " dropout_752 (Dropout)          (None, 53, 1024)     0           ['bidirectional_34[0][0]']       \n",
      " tf.__operators__.getitem_34 (S  (None, 768)         0           ['tf_roberta_model_17[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_35 (Bidirectiona  (None, 1024)        16785408    ['dropout_752[0][0]']            \n",
      " l)                                                                                               \n",
      " tf.concat_17 (TFOpLambda)      (None, 1792)         0           ['tf.__operators__.getitem_34[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_35[0][0]']       \n",
      " dropout_753 (Dropout)          (None, 1792)         0           ['tf.concat_17[0][0]']           \n",
      " dense_51 (Dense)               (None, 1024)         1836032     ['dropout_753[0][0]']            \n",
      " dropout_754 (Dropout)          (None, 1024)         0           ['dense_51[0][0]']               \n",
      " dense_52 (Dense)               (None, 512)          524800      ['dropout_754[0][0]']            \n",
      " dropout_755 (Dropout)          (None, 512)          0           ['dense_52[0][0]']               \n",
      " dense_53 (Dense)               (None, 1)            513         ['dropout_755[0][0]']            \n",
      "==================================================================================================\n",
      "Total params: 158,480,641\n",
      "Trainable params: 158,480,641\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "  0%|          | 0/1 [32:54<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_17/roberta/pooler/dense/kernel:0', 'tf_roberta_model_17/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_17/roberta/pooler/dense/kernel:0', 'tf_roberta_model_17/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_17/roberta/pooler/dense/kernel:0', 'tf_roberta_model_17/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_17/roberta/pooler/dense/kernel:0', 'tf_roberta_model_17/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "944/944 - 369s - loss: 145.2586 - accuracy: 0.5274 - val_loss: 141.1787 - val_accuracy: 0.8125 - 369s/epoch - 391ms/step\n",
      "\n",
      "Model: \"model_17\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_17 (TFRoberta  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " Model)                         thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_35 (S  (None, 53, 768)     0           ['tf_roberta_model_17[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_751 (Dropout)          (None, 53, 768)      0           ['tf.__operators__.getitem_35[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_34 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_751[0][0]']            \n",
      " l)                                                                                               \n",
      " dropout_752 (Dropout)          (None, 53, 1024)     0           ['bidirectional_34[0][0]']       \n",
      " tf.__operators__.getitem_34 (S  (None, 768)         0           ['tf_roberta_model_17[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_35 (Bidirectiona  (None, 1024)        16785408    ['dropout_752[0][0]']            \n",
      " l)                                                                                               \n",
      " tf.concat_17 (TFOpLambda)      (None, 1792)         0           ['tf.__operators__.getitem_34[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_35[0][0]']       \n",
      " dropout_753 (Dropout)          (None, 1792)         0           ['tf.concat_17[0][0]']           \n",
      " dense_51 (Dense)               (None, 1024)         1836032     ['dropout_753[0][0]']            \n",
      " dropout_754 (Dropout)          (None, 1024)         0           ['dense_51[0][0]']               \n",
      " dense_52 (Dense)               (None, 512)          524800      ['dropout_754[0][0]']            \n",
      " dropout_755 (Dropout)          (None, 512)          0           ['dense_52[0][0]']               \n",
      " dense_53 (Dense)               (None, 1)            513         ['dropout_755[0][0]']            \n",
      "==================================================================================================\n",
      "Total params: 158,480,641\n",
      "Trainable params: 33,835,009\n",
      "Non-trainable params: 124,645,632\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "944/944 - 245s - loss: 137.4477 - accuracy: 0.6714 - val_loss: 133.5109 - val_accuracy: 0.8408 - 245s/epoch - 259ms/step\n",
      "\n",
      "Epoch 2/2\n",
      "944/944 - 300s - loss: 129.9343 - accuracy: 0.7862 - val_loss: 126.2038 - val_accuracy: 0.8501 - 300s/epoch - 317ms/step\n",
      "\n",
      "0.850132405757904\n",
      "2\n",
      "FINISHED THE HYPERPARAMTER TUNING\n",
      "0.8488812843958536\n",
      "THE RESULTS ARE:\n",
      "{'acc': 0.8488812843958536, 'epochs': '[2, 2, 2]', 'status': 'ok', 'space': (1e-06, 0.6, 0.97, 0.99, 0.0005, 0, 128, 'sigmoid', 1024, 512)}\n",
      "  0%|          | 0/1 [48:10<?, ?it/s, best loss: ?]\n",
      "Starting from scratch: nesw trials.\n",
      "FINISHED FMIN\n",
      "best:\n",
      "local variable 'best' referenced before assignment\n",
      "Optimizing New Model\n",
      "Attempt to resume a past training if it exists:\n",
      "Found saved Trials! Loading...\n",
      "Rerunning from 0 trials to add another one.\n",
      "  0%|          | 0/1 [00:00<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with the following hyperparameters:\n",
      "(1e-05, 0.5, 0.99, 0.9, 0.01, 0.005, 128, 'sigmoid', 512, 512)\n",
      "Model: \"model_18\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_18 (TFRoberta  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " Model)                         thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_37 (S  (None, 53, 768)     0           ['tf_roberta_model_18[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_793 (Dropout)          (None, 53, 768)      0           ['tf.__operators__.getitem_37[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_36 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_793[0][0]']            \n",
      " l)                                                                                               \n",
      " dropout_794 (Dropout)          (None, 53, 1024)     0           ['bidirectional_36[0][0]']       \n",
      " tf.__operators__.getitem_36 (S  (None, 768)         0           ['tf_roberta_model_18[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_37 (Bidirectiona  (None, 1024)        16785408    ['dropout_794[0][0]']            \n",
      " l)                                                                                               \n",
      " tf.concat_18 (TFOpLambda)      (None, 1792)         0           ['tf.__operators__.getitem_36[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_37[0][0]']       \n",
      " dropout_795 (Dropout)          (None, 1792)         0           ['tf.concat_18[0][0]']           \n",
      " dense_54 (Dense)               (None, 512)          918016      ['dropout_795[0][0]']            \n",
      " dropout_796 (Dropout)          (None, 512)          0           ['dense_54[0][0]']               \n",
      " dense_55 (Dense)               (None, 512)          262656      ['dropout_796[0][0]']            \n",
      " dropout_797 (Dropout)          (None, 512)          0           ['dense_55[0][0]']               \n",
      " dense_56 (Dense)               (None, 1)            513         ['dropout_797[0][0]']            \n",
      "==================================================================================================\n",
      "Total params: 157,300,481\n",
      "Trainable params: 157,300,481\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "  0%|          | 0/1 [00:47<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_18/roberta/pooler/dense/kernel:0', 'tf_roberta_model_18/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_18/roberta/pooler/dense/kernel:0', 'tf_roberta_model_18/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_18/roberta/pooler/dense/kernel:0', 'tf_roberta_model_18/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_18/roberta/pooler/dense/kernel:0', 'tf_roberta_model_18/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "944/944 - 371s - loss: 2091.5554 - accuracy: 0.6921 - val_loss: 1499.4747 - val_accuracy: 0.8057 - 371s/epoch - 393ms/step\n",
      "\n",
      "Model: \"model_18\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_18 (TFRoberta  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " Model)                         thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_37 (S  (None, 53, 768)     0           ['tf_roberta_model_18[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_793 (Dropout)          (None, 53, 768)      0           ['tf.__operators__.getitem_37[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_36 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_793[0][0]']            \n",
      " l)                                                                                               \n",
      " dropout_794 (Dropout)          (None, 53, 1024)     0           ['bidirectional_36[0][0]']       \n",
      " tf.__operators__.getitem_36 (S  (None, 768)         0           ['tf_roberta_model_18[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_37 (Bidirectiona  (None, 1024)        16785408    ['dropout_794[0][0]']            \n",
      " l)                                                                                               \n",
      " tf.concat_18 (TFOpLambda)      (None, 1792)         0           ['tf.__operators__.getitem_36[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_37[0][0]']       \n",
      " dropout_795 (Dropout)          (None, 1792)         0           ['tf.concat_18[0][0]']           \n",
      " dense_54 (Dense)               (None, 512)          918016      ['dropout_795[0][0]']            \n",
      " dropout_796 (Dropout)          (None, 512)          0           ['dense_54[0][0]']               \n",
      " dense_55 (Dense)               (None, 512)          262656      ['dropout_796[0][0]']            \n",
      " dropout_797 (Dropout)          (None, 512)          0           ['dense_55[0][0]']               \n",
      " dense_56 (Dense)               (None, 1)            513         ['dropout_797[0][0]']            \n",
      "==================================================================================================\n",
      "Total params: 157,300,481\n",
      "Trainable params: 32,654,849\n",
      "Non-trainable params: 124,645,632\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "944/944 - 244s - loss: 1065.0909 - accuracy: 0.7592 - val_loss: 700.6234 - val_accuracy: 0.8435 - 244s/epoch - 258ms/step\n",
      "\n",
      "Epoch 2/2\n",
      "944/944 - 300s - loss: 459.2437 - accuracy: 0.7215 - val_loss: 268.1217 - val_accuracy: 0.8464 - 300s/epoch - 318ms/step\n",
      "\n",
      "0.8463791012763977\n",
      "2\n",
      "  0%|          | 0/1 [16:05<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with the following hyperparameters:\n",
      "(1e-05, 0.5, 0.99, 0.9, 0.01, 0.005, 128, 'sigmoid', 512, 512)\n",
      "Model: \"model_19\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_19 (TFRoberta  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " Model)                         thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_39 (S  (None, 53, 768)     0           ['tf_roberta_model_19[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_835 (Dropout)          (None, 53, 768)      0           ['tf.__operators__.getitem_39[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_38 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_835[0][0]']            \n",
      " l)                                                                                               \n",
      " dropout_836 (Dropout)          (None, 53, 1024)     0           ['bidirectional_38[0][0]']       \n",
      " tf.__operators__.getitem_38 (S  (None, 768)         0           ['tf_roberta_model_19[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_39 (Bidirectiona  (None, 1024)        16785408    ['dropout_836[0][0]']            \n",
      " l)                                                                                               \n",
      " tf.concat_19 (TFOpLambda)      (None, 1792)         0           ['tf.__operators__.getitem_38[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_39[0][0]']       \n",
      " dropout_837 (Dropout)          (None, 1792)         0           ['tf.concat_19[0][0]']           \n",
      " dense_57 (Dense)               (None, 512)          918016      ['dropout_837[0][0]']            \n",
      " dropout_838 (Dropout)          (None, 512)          0           ['dense_57[0][0]']               \n",
      " dense_58 (Dense)               (None, 512)          262656      ['dropout_838[0][0]']            \n",
      " dropout_839 (Dropout)          (None, 512)          0           ['dense_58[0][0]']               \n",
      " dense_59 (Dense)               (None, 1)            513         ['dropout_839[0][0]']            \n",
      "==================================================================================================\n",
      "Total params: 157,300,481\n",
      "Trainable params: 157,300,481\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "  0%|          | 0/1 [16:52<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_19/roberta/pooler/dense/kernel:0', 'tf_roberta_model_19/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_19/roberta/pooler/dense/kernel:0', 'tf_roberta_model_19/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_19/roberta/pooler/dense/kernel:0', 'tf_roberta_model_19/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_19/roberta/pooler/dense/kernel:0', 'tf_roberta_model_19/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "944/944 - 372s - loss: 2091.1287 - accuracy: 0.7276 - val_loss: 1499.1377 - val_accuracy: 0.8459 - 372s/epoch - 394ms/step\n",
      "\n",
      "Model: \"model_19\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_19 (TFRoberta  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " Model)                         thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_39 (S  (None, 53, 768)     0           ['tf_roberta_model_19[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_835 (Dropout)          (None, 53, 768)      0           ['tf.__operators__.getitem_39[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_38 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_835[0][0]']            \n",
      " l)                                                                                               \n",
      " dropout_836 (Dropout)          (None, 53, 1024)     0           ['bidirectional_38[0][0]']       \n",
      " tf.__operators__.getitem_38 (S  (None, 768)         0           ['tf_roberta_model_19[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_39 (Bidirectiona  (None, 1024)        16785408    ['dropout_836[0][0]']            \n",
      " l)                                                                                               \n",
      " tf.concat_19 (TFOpLambda)      (None, 1792)         0           ['tf.__operators__.getitem_38[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_39[0][0]']       \n",
      " dropout_837 (Dropout)          (None, 1792)         0           ['tf.concat_19[0][0]']           \n",
      " dense_57 (Dense)               (None, 512)          918016      ['dropout_837[0][0]']            \n",
      " dropout_838 (Dropout)          (None, 512)          0           ['dense_57[0][0]']               \n",
      " dense_58 (Dense)               (None, 512)          262656      ['dropout_838[0][0]']            \n",
      " dropout_839 (Dropout)          (None, 512)          0           ['dense_58[0][0]']               \n",
      " dense_59 (Dense)               (None, 1)            513         ['dropout_839[0][0]']            \n",
      "==================================================================================================\n",
      "Total params: 157,300,481\n",
      "Trainable params: 32,654,849\n",
      "Non-trainable params: 124,645,632\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "944/944 - 244s - loss: 1064.8593 - accuracy: 0.7864 - val_loss: 700.4576 - val_accuracy: 0.8540 - 244s/epoch - 259ms/step\n",
      "\n",
      "Epoch 2/2\n",
      "944/944 - 300s - loss: 458.8895 - accuracy: 0.7285 - val_loss: 267.8137 - val_accuracy: 0.8583 - 300s/epoch - 318ms/step\n",
      "\n",
      "0.8583388924598694\n",
      "2\n",
      "  0%|          | 0/1 [32:12<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with the following hyperparameters:\n",
      "(1e-05, 0.5, 0.99, 0.9, 0.01, 0.005, 128, 'sigmoid', 512, 512)\n",
      "Model: \"model_20\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_20 (TFRoberta  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " Model)                         thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_41 (S  (None, 53, 768)     0           ['tf_roberta_model_20[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_877 (Dropout)          (None, 53, 768)      0           ['tf.__operators__.getitem_41[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_40 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_877[0][0]']            \n",
      " l)                                                                                               \n",
      " dropout_878 (Dropout)          (None, 53, 1024)     0           ['bidirectional_40[0][0]']       \n",
      " tf.__operators__.getitem_40 (S  (None, 768)         0           ['tf_roberta_model_20[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_41 (Bidirectiona  (None, 1024)        16785408    ['dropout_878[0][0]']            \n",
      " l)                                                                                               \n",
      " tf.concat_20 (TFOpLambda)      (None, 1792)         0           ['tf.__operators__.getitem_40[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_41[0][0]']       \n",
      " dropout_879 (Dropout)          (None, 1792)         0           ['tf.concat_20[0][0]']           \n",
      " dense_60 (Dense)               (None, 512)          918016      ['dropout_879[0][0]']            \n",
      " dropout_880 (Dropout)          (None, 512)          0           ['dense_60[0][0]']               \n",
      " dense_61 (Dense)               (None, 512)          262656      ['dropout_880[0][0]']            \n",
      " dropout_881 (Dropout)          (None, 512)          0           ['dense_61[0][0]']               \n",
      " dense_62 (Dense)               (None, 1)            513         ['dropout_881[0][0]']            \n",
      "==================================================================================================\n",
      "Total params: 157,300,481\n",
      "Trainable params: 157,300,481\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "  0%|          | 0/1 [33:00<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_20/roberta/pooler/dense/kernel:0', 'tf_roberta_model_20/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_20/roberta/pooler/dense/kernel:0', 'tf_roberta_model_20/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_20/roberta/pooler/dense/kernel:0', 'tf_roberta_model_20/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_20/roberta/pooler/dense/kernel:0', 'tf_roberta_model_20/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "944/944 - 370s - loss: 2091.5513 - accuracy: 0.7239 - val_loss: 1499.3756 - val_accuracy: 0.8556 - 370s/epoch - 391ms/step\n",
      "\n",
      "Model: \"model_20\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_20 (TFRoberta  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " Model)                         thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_41 (S  (None, 53, 768)     0           ['tf_roberta_model_20[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_877 (Dropout)          (None, 53, 768)      0           ['tf.__operators__.getitem_41[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_40 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_877[0][0]']            \n",
      " l)                                                                                               \n",
      " dropout_878 (Dropout)          (None, 53, 1024)     0           ['bidirectional_40[0][0]']       \n",
      " tf.__operators__.getitem_40 (S  (None, 768)         0           ['tf_roberta_model_20[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_41 (Bidirectiona  (None, 1024)        16785408    ['dropout_878[0][0]']            \n",
      " l)                                                                                               \n",
      " tf.concat_20 (TFOpLambda)      (None, 1792)         0           ['tf.__operators__.getitem_40[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_41[0][0]']       \n",
      " dropout_879 (Dropout)          (None, 1792)         0           ['tf.concat_20[0][0]']           \n",
      " dense_60 (Dense)               (None, 512)          918016      ['dropout_879[0][0]']            \n",
      " dropout_880 (Dropout)          (None, 512)          0           ['dense_60[0][0]']               \n",
      " dense_61 (Dense)               (None, 512)          262656      ['dropout_880[0][0]']            \n",
      " dropout_881 (Dropout)          (None, 512)          0           ['dense_61[0][0]']               \n",
      " dense_62 (Dense)               (None, 1)            513         ['dropout_881[0][0]']            \n",
      "==================================================================================================\n",
      "Total params: 157,300,481\n",
      "Trainable params: 32,654,849\n",
      "Non-trainable params: 124,645,632\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "944/944 - 243s - loss: 1065.0071 - accuracy: 0.7889 - val_loss: 700.5696 - val_accuracy: 0.8544 - 243s/epoch - 258ms/step\n",
      "\n",
      "Epoch 2/2\n",
      "944/944 - 300s - loss: 458.9671 - accuracy: 0.7457 - val_loss: 267.8259 - val_accuracy: 0.8631 - 300s/epoch - 317ms/step\n",
      "\n",
      "0.8631370067596436\n",
      "2\n",
      "FINISHED THE HYPERPARAMTER TUNING\n",
      "0.8559516668319702\n",
      "THE RESULTS ARE:\n",
      "{'acc': 0.8559516668319702, 'epochs': '[2, 2, 2]', 'status': 'ok', 'space': (1e-05, 0.5, 0.99, 0.9, 0.01, 0.005, 128, 'sigmoid', 512, 512)}\n",
      "  0%|          | 0/1 [48:16<?, ?it/s, best loss: ?]\n",
      "Starting from scratch: nesw trials.\n",
      "FINISHED FMIN\n",
      "best:\n",
      "local variable 'best' referenced before assignment\n",
      "Optimizing New Model\n",
      "Attempt to resume a past training if it exists:\n",
      "Found saved Trials! Loading...\n",
      "Rerunning from 0 trials to add another one.\n",
      "  0%|          | 0/1 [00:00<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with the following hyperparameters:\n",
      "(0.0005, 0.6, 0.97, 0.95, 0.0001, 0.001, 256, 'tanh', 1024, 128)\n",
      "Model: \"model_21\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_21 (TFRoberta  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " Model)                         thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_43 (S  (None, 53, 768)     0           ['tf_roberta_model_21[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_919 (Dropout)          (None, 53, 768)      0           ['tf.__operators__.getitem_43[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_42 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_919[0][0]']            \n",
      " l)                                                                                               \n",
      " dropout_920 (Dropout)          (None, 53, 1024)     0           ['bidirectional_42[0][0]']       \n",
      " tf.__operators__.getitem_42 (S  (None, 768)         0           ['tf_roberta_model_21[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_43 (Bidirectiona  (None, 1024)        16785408    ['dropout_920[0][0]']            \n",
      " l)                                                                                               \n",
      " tf.concat_21 (TFOpLambda)      (None, 1792)         0           ['tf.__operators__.getitem_42[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_43[0][0]']       \n",
      " dropout_921 (Dropout)          (None, 1792)         0           ['tf.concat_21[0][0]']           \n",
      " dense_63 (Dense)               (None, 1024)         1836032     ['dropout_921[0][0]']            \n",
      " dropout_922 (Dropout)          (None, 1024)         0           ['dense_63[0][0]']               \n",
      " dense_64 (Dense)               (None, 128)          131200      ['dropout_922[0][0]']            \n",
      " dropout_923 (Dropout)          (None, 128)          0           ['dense_64[0][0]']               \n",
      " dense_65 (Dense)               (None, 1)            129         ['dropout_923[0][0]']            \n",
      "==================================================================================================\n",
      "Total params: 158,086,657\n",
      "Trainable params: 158,086,657\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "  0%|          | 0/1 [00:46<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_21/roberta/pooler/dense/kernel:0', 'tf_roberta_model_21/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_21/roberta/pooler/dense/kernel:0', 'tf_roberta_model_21/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_21/roberta/pooler/dense/kernel:0', 'tf_roberta_model_21/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_21/roberta/pooler/dense/kernel:0', 'tf_roberta_model_21/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472/472 - 287s - loss: 4.7777 - accuracy: 0.5012 - val_loss: 0.9863 - val_accuracy: 0.5007 - 287s/epoch - 608ms/step\n",
      "\n",
      "Model: \"model_21\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_21 (TFRoberta  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " Model)                         thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_43 (S  (None, 53, 768)     0           ['tf_roberta_model_21[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_919 (Dropout)          (None, 53, 768)      0           ['tf.__operators__.getitem_43[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_42 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_919[0][0]']            \n",
      " l)                                                                                               \n",
      " dropout_920 (Dropout)          (None, 53, 1024)     0           ['bidirectional_42[0][0]']       \n",
      " tf.__operators__.getitem_42 (S  (None, 768)         0           ['tf_roberta_model_21[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_43 (Bidirectiona  (None, 1024)        16785408    ['dropout_920[0][0]']            \n",
      " l)                                                                                               \n",
      " tf.concat_21 (TFOpLambda)      (None, 1792)         0           ['tf.__operators__.getitem_42[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_43[0][0]']       \n",
      " dropout_921 (Dropout)          (None, 1792)         0           ['tf.concat_21[0][0]']           \n",
      " dense_63 (Dense)               (None, 1024)         1836032     ['dropout_921[0][0]']            \n",
      " dropout_922 (Dropout)          (None, 1024)         0           ['dense_63[0][0]']               \n",
      " dense_64 (Dense)               (None, 128)          131200      ['dropout_922[0][0]']            \n",
      " dropout_923 (Dropout)          (None, 128)          0           ['dense_64[0][0]']               \n",
      " dense_65 (Dense)               (None, 1)            129         ['dropout_923[0][0]']            \n",
      "==================================================================================================\n",
      "Total params: 158,086,657\n",
      "Trainable params: 33,441,025\n",
      "Non-trainable params: 124,645,632\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "472/472 - 154s - loss: 0.8055 - accuracy: 0.5009 - val_loss: 0.7408 - val_accuracy: 0.5007 - 154s/epoch - 327ms/step\n",
      "\n",
      "Epoch 2/2\n",
      "472/472 - 211s - loss: 0.7390 - accuracy: 0.5017 - val_loss: 0.7385 - val_accuracy: 0.5007 - 211s/epoch - 446ms/step\n",
      "\n",
      "0.5006535053253174\n",
      "1\n",
      "  0%|          | 0/1 [11:41<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with the following hyperparameters:\n",
      "(0.0005, 0.6, 0.97, 0.95, 0.0001, 0.001, 256, 'tanh', 1024, 128)\n",
      "Model: \"model_22\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_22 (TFRoberta  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " Model)                         thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_45 (S  (None, 53, 768)     0           ['tf_roberta_model_22[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_961 (Dropout)          (None, 53, 768)      0           ['tf.__operators__.getitem_45[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_44 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_961[0][0]']            \n",
      " l)                                                                                               \n",
      " dropout_962 (Dropout)          (None, 53, 1024)     0           ['bidirectional_44[0][0]']       \n",
      " tf.__operators__.getitem_44 (S  (None, 768)         0           ['tf_roberta_model_22[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_45 (Bidirectiona  (None, 1024)        16785408    ['dropout_962[0][0]']            \n",
      " l)                                                                                               \n",
      " tf.concat_22 (TFOpLambda)      (None, 1792)         0           ['tf.__operators__.getitem_44[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_45[0][0]']       \n",
      " dropout_963 (Dropout)          (None, 1792)         0           ['tf.concat_22[0][0]']           \n",
      " dense_66 (Dense)               (None, 1024)         1836032     ['dropout_963[0][0]']            \n",
      " dropout_964 (Dropout)          (None, 1024)         0           ['dense_66[0][0]']               \n",
      " dense_67 (Dense)               (None, 128)          131200      ['dropout_964[0][0]']            \n",
      " dropout_965 (Dropout)          (None, 128)          0           ['dense_67[0][0]']               \n",
      " dense_68 (Dense)               (None, 1)            129         ['dropout_965[0][0]']            \n",
      "==================================================================================================\n",
      "Total params: 158,086,657\n",
      "Trainable params: 158,086,657\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "  0%|          | 0/1 [12:29<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_22/roberta/pooler/dense/kernel:0', 'tf_roberta_model_22/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_22/roberta/pooler/dense/kernel:0', 'tf_roberta_model_22/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_22/roberta/pooler/dense/kernel:0', 'tf_roberta_model_22/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_22/roberta/pooler/dense/kernel:0', 'tf_roberta_model_22/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472/472 - 284s - loss: 4.7761 - accuracy: 0.5007 - val_loss: 0.9822 - val_accuracy: 0.5038 - 284s/epoch - 603ms/step\n",
      "\n",
      "Model: \"model_22\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_22 (TFRoberta  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " Model)                         thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_45 (S  (None, 53, 768)     0           ['tf_roberta_model_22[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_961 (Dropout)          (None, 53, 768)      0           ['tf.__operators__.getitem_45[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_44 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_961[0][0]']            \n",
      " l)                                                                                               \n",
      " dropout_962 (Dropout)          (None, 53, 1024)     0           ['bidirectional_44[0][0]']       \n",
      " tf.__operators__.getitem_44 (S  (None, 768)         0           ['tf_roberta_model_22[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_45 (Bidirectiona  (None, 1024)        16785408    ['dropout_962[0][0]']            \n",
      " l)                                                                                               \n",
      " tf.concat_22 (TFOpLambda)      (None, 1792)         0           ['tf.__operators__.getitem_44[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_45[0][0]']       \n",
      " dropout_963 (Dropout)          (None, 1792)         0           ['tf.concat_22[0][0]']           \n",
      " dense_66 (Dense)               (None, 1024)         1836032     ['dropout_963[0][0]']            \n",
      " dropout_964 (Dropout)          (None, 1024)         0           ['dense_66[0][0]']               \n",
      " dense_67 (Dense)               (None, 128)          131200      ['dropout_964[0][0]']            \n",
      " dropout_965 (Dropout)          (None, 128)          0           ['dense_67[0][0]']               \n",
      " dense_68 (Dense)               (None, 1)            129         ['dropout_965[0][0]']            \n",
      "==================================================================================================\n",
      "Total params: 158,086,657\n",
      "Trainable params: 33,441,025\n",
      "Non-trainable params: 124,645,632\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "472/472 - 155s - loss: 0.7999 - accuracy: 0.5040 - val_loss: 0.7394 - val_accuracy: 0.5038 - 155s/epoch - 327ms/step\n",
      "\n",
      "Epoch 2/2\n",
      "472/472 - 210s - loss: 0.7368 - accuracy: 0.5008 - val_loss: 0.7361 - val_accuracy: 0.5038 - 210s/epoch - 445ms/step\n",
      "\n",
      "0.5038385391235352\n",
      "1\n",
      "  0%|          | 0/1 [23:21<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with the following hyperparameters:\n",
      "(0.0005, 0.6, 0.97, 0.95, 0.0001, 0.001, 256, 'tanh', 1024, 128)\n",
      "Model: \"model_23\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_23 (TFRoberta  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " Model)                         thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_47 (S  (None, 53, 768)     0           ['tf_roberta_model_23[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_1003 (Dropout)         (None, 53, 768)      0           ['tf.__operators__.getitem_47[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_46 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_1003[0][0]']           \n",
      " l)                                                                                               \n",
      " dropout_1004 (Dropout)         (None, 53, 1024)     0           ['bidirectional_46[0][0]']       \n",
      " tf.__operators__.getitem_46 (S  (None, 768)         0           ['tf_roberta_model_23[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_47 (Bidirectiona  (None, 1024)        16785408    ['dropout_1004[0][0]']           \n",
      " l)                                                                                               \n",
      " tf.concat_23 (TFOpLambda)      (None, 1792)         0           ['tf.__operators__.getitem_46[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_47[0][0]']       \n",
      " dropout_1005 (Dropout)         (None, 1792)         0           ['tf.concat_23[0][0]']           \n",
      " dense_69 (Dense)               (None, 1024)         1836032     ['dropout_1005[0][0]']           \n",
      " dropout_1006 (Dropout)         (None, 1024)         0           ['dense_69[0][0]']               \n",
      " dense_70 (Dense)               (None, 128)          131200      ['dropout_1006[0][0]']           \n",
      " dropout_1007 (Dropout)         (None, 128)          0           ['dense_70[0][0]']               \n",
      " dense_71 (Dense)               (None, 1)            129         ['dropout_1007[0][0]']           \n",
      "==================================================================================================\n",
      "Total params: 158,086,657\n",
      "Trainable params: 158,086,657\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "  0%|          | 0/1 [24:08<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_23/roberta/pooler/dense/kernel:0', 'tf_roberta_model_23/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_23/roberta/pooler/dense/kernel:0', 'tf_roberta_model_23/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_23/roberta/pooler/dense/kernel:0', 'tf_roberta_model_23/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_23/roberta/pooler/dense/kernel:0', 'tf_roberta_model_23/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472/472 - 285s - loss: 4.7419 - accuracy: 0.4994 - val_loss: 0.9499 - val_accuracy: 0.4974 - 285s/epoch - 604ms/step\n",
      "\n",
      "Model: \"model_23\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_23 (TFRoberta  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " Model)                         thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_47 (S  (None, 53, 768)     0           ['tf_roberta_model_23[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_1003 (Dropout)         (None, 53, 768)      0           ['tf.__operators__.getitem_47[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_46 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_1003[0][0]']           \n",
      " l)                                                                                               \n",
      " dropout_1004 (Dropout)         (None, 53, 1024)     0           ['bidirectional_46[0][0]']       \n",
      " tf.__operators__.getitem_46 (S  (None, 768)         0           ['tf_roberta_model_23[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_47 (Bidirectiona  (None, 1024)        16785408    ['dropout_1004[0][0]']           \n",
      " l)                                                                                               \n",
      " tf.concat_23 (TFOpLambda)      (None, 1792)         0           ['tf.__operators__.getitem_46[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_47[0][0]']       \n",
      " dropout_1005 (Dropout)         (None, 1792)         0           ['tf.concat_23[0][0]']           \n",
      " dense_69 (Dense)               (None, 1024)         1836032     ['dropout_1005[0][0]']           \n",
      " dropout_1006 (Dropout)         (None, 1024)         0           ['dense_69[0][0]']               \n",
      " dense_70 (Dense)               (None, 128)          131200      ['dropout_1006[0][0]']           \n",
      " dropout_1007 (Dropout)         (None, 128)          0           ['dense_70[0][0]']               \n",
      " dense_71 (Dense)               (None, 1)            129         ['dropout_1007[0][0]']           \n",
      "==================================================================================================\n",
      "Total params: 158,086,657\n",
      "Trainable params: 33,441,025\n",
      "Non-trainable params: 124,645,632\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "472/472 - 155s - loss: 0.7895 - accuracy: 0.4986 - val_loss: 0.7400 - val_accuracy: 0.5026 - 155s/epoch - 328ms/step\n",
      "\n",
      "Epoch 2/2\n",
      "472/472 - 211s - loss: 0.7380 - accuracy: 0.4994 - val_loss: 0.7376 - val_accuracy: 0.5026 - 211s/epoch - 447ms/step\n",
      "\n",
      "0.5025811195373535\n",
      "1\n",
      "FINISHED THE HYPERPARAMTER TUNING\n",
      "0.5023577213287354\n",
      "THE RESULTS ARE:\n",
      "{'acc': 0.5023577213287354, 'epochs': '[1, 1, 1]', 'status': 'ok', 'space': (0.0005, 0.6, 0.97, 0.95, 0.0001, 0.001, 256, 'tanh', 1024, 128)}\n",
      "  0%|          | 0/1 [35:02<?, ?it/s, best loss: ?]\n",
      "Starting from scratch: nesw trials.\n",
      "FINISHED FMIN\n",
      "best:\n",
      "local variable 'best' referenced before assignment\n",
      "Optimizing New Model\n",
      "Attempt to resume a past training if it exists:\n",
      "Found saved Trials! Loading...\n",
      "Rerunning from 0 trials to add another one.\n",
      "  0%|          | 0/1 [00:00<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with the following hyperparameters:\n",
      "(5e-05, 0.5, 0.95, 0.9, 0.001, 0, 256, 'tanh', 256, 128)\n",
      "Model: \"model_24\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_24 (TFRoberta  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " Model)                         thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_49 (S  (None, 53, 768)     0           ['tf_roberta_model_24[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_1045 (Dropout)         (None, 53, 768)      0           ['tf.__operators__.getitem_49[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_48 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_1045[0][0]']           \n",
      " l)                                                                                               \n",
      " dropout_1046 (Dropout)         (None, 53, 1024)     0           ['bidirectional_48[0][0]']       \n",
      " tf.__operators__.getitem_48 (S  (None, 768)         0           ['tf_roberta_model_24[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_49 (Bidirectiona  (None, 1024)        16785408    ['dropout_1046[0][0]']           \n",
      " l)                                                                                               \n",
      " tf.concat_24 (TFOpLambda)      (None, 1792)         0           ['tf.__operators__.getitem_48[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_49[0][0]']       \n",
      " dropout_1047 (Dropout)         (None, 1792)         0           ['tf.concat_24[0][0]']           \n",
      " dense_72 (Dense)               (None, 256)          459008      ['dropout_1047[0][0]']           \n",
      " dropout_1048 (Dropout)         (None, 256)          0           ['dense_72[0][0]']               \n",
      " dense_73 (Dense)               (None, 128)          32896       ['dropout_1048[0][0]']           \n",
      " dropout_1049 (Dropout)         (None, 128)          0           ['dense_73[0][0]']               \n",
      " dense_74 (Dense)               (None, 1)            129         ['dropout_1049[0][0]']           \n",
      "==================================================================================================\n",
      "Total params: 156,611,329\n",
      "Trainable params: 156,611,329\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "  0%|          | 0/1 [00:47<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_24/roberta/pooler/dense/kernel:0', 'tf_roberta_model_24/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_24/roberta/pooler/dense/kernel:0', 'tf_roberta_model_24/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_24/roberta/pooler/dense/kernel:0', 'tf_roberta_model_24/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_24/roberta/pooler/dense/kernel:0', 'tf_roberta_model_24/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472/472 - 282s - loss: 126.0422 - accuracy: 0.8264 - val_loss: 39.5688 - val_accuracy: 0.8688 - 282s/epoch - 596ms/step\n",
      "\n",
      "Model: \"model_24\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_24 (TFRoberta  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " Model)                         thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_49 (S  (None, 53, 768)     0           ['tf_roberta_model_24[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_1045 (Dropout)         (None, 53, 768)      0           ['tf.__operators__.getitem_49[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_48 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_1045[0][0]']           \n",
      " l)                                                                                               \n",
      " dropout_1046 (Dropout)         (None, 53, 1024)     0           ['bidirectional_48[0][0]']       \n",
      " tf.__operators__.getitem_48 (S  (None, 768)         0           ['tf_roberta_model_24[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_49 (Bidirectiona  (None, 1024)        16785408    ['dropout_1046[0][0]']           \n",
      " l)                                                                                               \n",
      " tf.concat_24 (TFOpLambda)      (None, 1792)         0           ['tf.__operators__.getitem_48[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_49[0][0]']       \n",
      " dropout_1047 (Dropout)         (None, 1792)         0           ['tf.concat_24[0][0]']           \n",
      " dense_72 (Dense)               (None, 256)          459008      ['dropout_1047[0][0]']           \n",
      " dropout_1048 (Dropout)         (None, 256)          0           ['dense_72[0][0]']               \n",
      " dense_73 (Dense)               (None, 128)          32896       ['dropout_1048[0][0]']           \n",
      " dropout_1049 (Dropout)         (None, 128)          0           ['dense_73[0][0]']               \n",
      " dense_74 (Dense)               (None, 1)            129         ['dropout_1049[0][0]']           \n",
      "==================================================================================================\n",
      "Total params: 156,611,329\n",
      "Trainable params: 31,965,697\n",
      "Non-trainable params: 124,645,632\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "472/472 - 154s - loss: 13.2022 - accuracy: 0.8761 - val_loss: 2.6400 - val_accuracy: 0.8726 - 154s/epoch - 327ms/step\n",
      "\n",
      "Epoch 2/2\n",
      "472/472 - 211s - loss: 1.7163 - accuracy: 0.8982 - val_loss: 1.2319 - val_accuracy: 0.8699 - 211s/epoch - 446ms/step\n",
      "\n",
      "0.8726195693016052\n",
      "1\n",
      "  0%|          | 0/1 [11:36<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with the following hyperparameters:\n",
      "(5e-05, 0.5, 0.95, 0.9, 0.001, 0, 256, 'tanh', 256, 128)\n",
      "Model: \"model_25\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_25 (TFRoberta  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " Model)                         thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_51 (S  (None, 53, 768)     0           ['tf_roberta_model_25[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_1087 (Dropout)         (None, 53, 768)      0           ['tf.__operators__.getitem_51[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_50 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_1087[0][0]']           \n",
      " l)                                                                                               \n",
      " dropout_1088 (Dropout)         (None, 53, 1024)     0           ['bidirectional_50[0][0]']       \n",
      " tf.__operators__.getitem_50 (S  (None, 768)         0           ['tf_roberta_model_25[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_51 (Bidirectiona  (None, 1024)        16785408    ['dropout_1088[0][0]']           \n",
      " l)                                                                                               \n",
      " tf.concat_25 (TFOpLambda)      (None, 1792)         0           ['tf.__operators__.getitem_50[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_51[0][0]']       \n",
      " dropout_1089 (Dropout)         (None, 1792)         0           ['tf.concat_25[0][0]']           \n",
      " dense_75 (Dense)               (None, 256)          459008      ['dropout_1089[0][0]']           \n",
      " dropout_1090 (Dropout)         (None, 256)          0           ['dense_75[0][0]']               \n",
      " dense_76 (Dense)               (None, 128)          32896       ['dropout_1090[0][0]']           \n",
      " dropout_1091 (Dropout)         (None, 128)          0           ['dense_76[0][0]']               \n",
      " dense_77 (Dense)               (None, 1)            129         ['dropout_1091[0][0]']           \n",
      "==================================================================================================\n",
      "Total params: 156,611,329\n",
      "Trainable params: 156,611,329\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "  0%|          | 0/1 [12:22<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_25/roberta/pooler/dense/kernel:0', 'tf_roberta_model_25/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_25/roberta/pooler/dense/kernel:0', 'tf_roberta_model_25/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_25/roberta/pooler/dense/kernel:0', 'tf_roberta_model_25/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_25/roberta/pooler/dense/kernel:0', 'tf_roberta_model_25/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472/472 - 282s - loss: 125.7987 - accuracy: 0.8297 - val_loss: 39.3083 - val_accuracy: 0.8680 - 282s/epoch - 598ms/step\n",
      "\n",
      "Model: \"model_25\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_25 (TFRoberta  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " Model)                         thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_51 (S  (None, 53, 768)     0           ['tf_roberta_model_25[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_1087 (Dropout)         (None, 53, 768)      0           ['tf.__operators__.getitem_51[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_50 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_1087[0][0]']           \n",
      " l)                                                                                               \n",
      " dropout_1088 (Dropout)         (None, 53, 1024)     0           ['bidirectional_50[0][0]']       \n",
      " tf.__operators__.getitem_50 (S  (None, 768)         0           ['tf_roberta_model_25[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_51 (Bidirectiona  (None, 1024)        16785408    ['dropout_1088[0][0]']           \n",
      " l)                                                                                               \n",
      " tf.concat_25 (TFOpLambda)      (None, 1792)         0           ['tf.__operators__.getitem_50[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_51[0][0]']       \n",
      " dropout_1089 (Dropout)         (None, 1792)         0           ['tf.concat_25[0][0]']           \n",
      " dense_75 (Dense)               (None, 256)          459008      ['dropout_1089[0][0]']           \n",
      " dropout_1090 (Dropout)         (None, 256)          0           ['dense_75[0][0]']               \n",
      " dense_76 (Dense)               (None, 128)          32896       ['dropout_1090[0][0]']           \n",
      " dropout_1091 (Dropout)         (None, 128)          0           ['dense_76[0][0]']               \n",
      " dense_77 (Dense)               (None, 1)            129         ['dropout_1091[0][0]']           \n",
      "==================================================================================================\n",
      "Total params: 156,611,329\n",
      "Trainable params: 31,965,697\n",
      "Non-trainable params: 124,645,632\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "472/472 - 154s - loss: 13.0077 - accuracy: 0.8797 - val_loss: 2.5110 - val_accuracy: 0.8720 - 154s/epoch - 326ms/step\n",
      "\n",
      "Epoch 2/2\n",
      "472/472 - 210s - loss: 1.6213 - accuracy: 0.9020 - val_loss: 1.1747 - val_accuracy: 0.8676 - 210s/epoch - 445ms/step\n",
      "\n",
      "0.8720218539237976\n",
      "1\n",
      "  0%|          | 0/1 [23:11<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with the following hyperparameters:\n",
      "(5e-05, 0.5, 0.95, 0.9, 0.001, 0, 256, 'tanh', 256, 128)\n",
      "Model: \"model_26\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_26 (TFRoberta  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " Model)                         thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_53 (S  (None, 53, 768)     0           ['tf_roberta_model_26[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_1129 (Dropout)         (None, 53, 768)      0           ['tf.__operators__.getitem_53[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_52 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_1129[0][0]']           \n",
      " l)                                                                                               \n",
      " dropout_1130 (Dropout)         (None, 53, 1024)     0           ['bidirectional_52[0][0]']       \n",
      " tf.__operators__.getitem_52 (S  (None, 768)         0           ['tf_roberta_model_26[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_53 (Bidirectiona  (None, 1024)        16785408    ['dropout_1130[0][0]']           \n",
      " l)                                                                                               \n",
      " tf.concat_26 (TFOpLambda)      (None, 1792)         0           ['tf.__operators__.getitem_52[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_53[0][0]']       \n",
      " dropout_1131 (Dropout)         (None, 1792)         0           ['tf.concat_26[0][0]']           \n",
      " dense_78 (Dense)               (None, 256)          459008      ['dropout_1131[0][0]']           \n",
      " dropout_1132 (Dropout)         (None, 256)          0           ['dense_78[0][0]']               \n",
      " dense_79 (Dense)               (None, 128)          32896       ['dropout_1132[0][0]']           \n",
      " dropout_1133 (Dropout)         (None, 128)          0           ['dense_79[0][0]']               \n",
      " dense_80 (Dense)               (None, 1)            129         ['dropout_1133[0][0]']           \n",
      "==================================================================================================\n",
      "Total params: 156,611,329\n",
      "Trainable params: 156,611,329\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "  0%|          | 0/1 [23:58<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_26/roberta/pooler/dense/kernel:0', 'tf_roberta_model_26/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_26/roberta/pooler/dense/kernel:0', 'tf_roberta_model_26/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_26/roberta/pooler/dense/kernel:0', 'tf_roberta_model_26/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_26/roberta/pooler/dense/kernel:0', 'tf_roberta_model_26/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472/472 - 284s - loss: 125.8828 - accuracy: 0.8252 - val_loss: 39.4103 - val_accuracy: 0.8650 - 284s/epoch - 601ms/step\n",
      "\n",
      "Model: \"model_26\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_26 (TFRoberta  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " Model)                         thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_53 (S  (None, 53, 768)     0           ['tf_roberta_model_26[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_1129 (Dropout)         (None, 53, 768)      0           ['tf.__operators__.getitem_53[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_52 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_1129[0][0]']           \n",
      " l)                                                                                               \n",
      " dropout_1130 (Dropout)         (None, 53, 1024)     0           ['bidirectional_52[0][0]']       \n",
      " tf.__operators__.getitem_52 (S  (None, 768)         0           ['tf_roberta_model_26[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_53 (Bidirectiona  (None, 1024)        16785408    ['dropout_1130[0][0]']           \n",
      " l)                                                                                               \n",
      " tf.concat_26 (TFOpLambda)      (None, 1792)         0           ['tf.__operators__.getitem_52[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_53[0][0]']       \n",
      " dropout_1131 (Dropout)         (None, 1792)         0           ['tf.concat_26[0][0]']           \n",
      " dense_78 (Dense)               (None, 256)          459008      ['dropout_1131[0][0]']           \n",
      " dropout_1132 (Dropout)         (None, 256)          0           ['dense_78[0][0]']               \n",
      " dense_79 (Dense)               (None, 128)          32896       ['dropout_1132[0][0]']           \n",
      " dropout_1133 (Dropout)         (None, 128)          0           ['dense_79[0][0]']               \n",
      " dense_80 (Dense)               (None, 1)            129         ['dropout_1133[0][0]']           \n",
      "==================================================================================================\n",
      "Total params: 156,611,329\n",
      "Trainable params: 31,965,697\n",
      "Non-trainable params: 124,645,632\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "472/472 - 154s - loss: 13.0758 - accuracy: 0.8760 - val_loss: 2.5740 - val_accuracy: 0.8687 - 154s/epoch - 326ms/step\n",
      "\n",
      "Epoch 2/2\n",
      "472/472 - 211s - loss: 1.6638 - accuracy: 0.8988 - val_loss: 1.1940 - val_accuracy: 0.8707 - 211s/epoch - 447ms/step\n",
      "\n",
      "0.8707313537597656\n",
      "2\n",
      "FINISHED THE HYPERPARAMTER TUNING\n",
      "0.8717909256617228\n",
      "THE RESULTS ARE:\n",
      "{'acc': 0.8717909256617228, 'epochs': '[1, 1, 2]', 'status': 'ok', 'space': (5e-05, 0.5, 0.95, 0.9, 0.001, 0, 256, 'tanh', 256, 128)}\n",
      "  0%|          | 0/1 [34:50<?, ?it/s, best loss: ?]\n",
      "Starting from scratch: nesw trials.\n",
      "FINISHED FMIN\n",
      "best:\n",
      "local variable 'best' referenced before assignment\n",
      "Optimizing New Model\n",
      "Attempt to resume a past training if it exists:\n",
      "Found saved Trials! Loading...\n",
      "Rerunning from 0 trials to add another one.\n",
      "  0%|          | 0/1 [00:00<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with the following hyperparameters:\n",
      "(5e-05, 0.3, 0.97, 0.99, 0.001, 0.0001, 64, 'tanh', 1024, 64)\n",
      "Model: \"model_27\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_27 (TFRoberta  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " Model)                         thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_55 (S  (None, 53, 768)     0           ['tf_roberta_model_27[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_1171 (Dropout)         (None, 53, 768)      0           ['tf.__operators__.getitem_55[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_54 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_1171[0][0]']           \n",
      " l)                                                                                               \n",
      " dropout_1172 (Dropout)         (None, 53, 1024)     0           ['bidirectional_54[0][0]']       \n",
      " tf.__operators__.getitem_54 (S  (None, 768)         0           ['tf_roberta_model_27[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_55 (Bidirectiona  (None, 1024)        16785408    ['dropout_1172[0][0]']           \n",
      " l)                                                                                               \n",
      " tf.concat_27 (TFOpLambda)      (None, 1792)         0           ['tf.__operators__.getitem_54[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_55[0][0]']       \n",
      " dropout_1173 (Dropout)         (None, 1792)         0           ['tf.concat_27[0][0]']           \n",
      " dense_81 (Dense)               (None, 1024)         1836032     ['dropout_1173[0][0]']           \n",
      " dropout_1174 (Dropout)         (None, 1024)         0           ['dense_81[0][0]']               \n",
      " dense_82 (Dense)               (None, 64)           65600       ['dropout_1174[0][0]']           \n",
      " dropout_1175 (Dropout)         (None, 64)           0           ['dense_82[0][0]']               \n",
      " dense_83 (Dense)               (None, 1)            65          ['dropout_1175[0][0]']           \n",
      "==================================================================================================\n",
      "Total params: 158,020,993\n",
      "Trainable params: 158,020,993\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "  0%|          | 0/1 [00:49<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_27/roberta/pooler/dense/kernel:0', 'tf_roberta_model_27/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_27/roberta/pooler/dense/kernel:0', 'tf_roberta_model_27/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_27/roberta/pooler/dense/kernel:0', 'tf_roberta_model_27/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_27/roberta/pooler/dense/kernel:0', 'tf_roberta_model_27/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1888/1888 - 553s - loss: 40.7712 - accuracy: 0.8358 - val_loss: 0.7281 - val_accuracy: 0.8546 - 553s/epoch - 293ms/step\n",
      "\n",
      "Model: \"model_27\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_27 (TFRoberta  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " Model)                         thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_55 (S  (None, 53, 768)     0           ['tf_roberta_model_27[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_1171 (Dropout)         (None, 53, 768)      0           ['tf.__operators__.getitem_55[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_54 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_1171[0][0]']           \n",
      " l)                                                                                               \n",
      " dropout_1172 (Dropout)         (None, 53, 1024)     0           ['bidirectional_54[0][0]']       \n",
      " tf.__operators__.getitem_54 (S  (None, 768)         0           ['tf_roberta_model_27[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_55 (Bidirectiona  (None, 1024)        16785408    ['dropout_1172[0][0]']           \n",
      " l)                                                                                               \n",
      " tf.concat_27 (TFOpLambda)      (None, 1792)         0           ['tf.__operators__.getitem_54[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_55[0][0]']       \n",
      " dropout_1173 (Dropout)         (None, 1792)         0           ['tf.concat_27[0][0]']           \n",
      " dense_81 (Dense)               (None, 1024)         1836032     ['dropout_1173[0][0]']           \n",
      " dropout_1174 (Dropout)         (None, 1024)         0           ['dense_81[0][0]']               \n",
      " dense_82 (Dense)               (None, 64)           65600       ['dropout_1174[0][0]']           \n",
      " dropout_1175 (Dropout)         (None, 64)           0           ['dense_82[0][0]']               \n",
      " dense_83 (Dense)               (None, 1)            65          ['dropout_1175[0][0]']           \n",
      "==================================================================================================\n",
      "Total params: 158,020,993\n",
      "Trainable params: 33,375,361\n",
      "Non-trainable params: 124,645,632\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "1888/1888 - 431s - loss: 0.4723 - accuracy: 0.8728 - val_loss: 0.4072 - val_accuracy: 0.8648 - 431s/epoch - 228ms/step\n",
      "\n",
      "Epoch 2/2\n",
      "1888/1888 - 487s - loss: 0.3467 - accuracy: 0.8962 - val_loss: 0.4059 - val_accuracy: 0.8686 - 487s/epoch - 258ms/step\n",
      "\n",
      "0.8686321973800659\n",
      "2\n",
      "  0%|          | 0/1 [25:23<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with the following hyperparameters:\n",
      "(5e-05, 0.3, 0.97, 0.99, 0.001, 0.0001, 64, 'tanh', 1024, 64)\n",
      "Model: \"model_28\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_28 (TFRoberta  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " Model)                         thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_57 (S  (None, 53, 768)     0           ['tf_roberta_model_28[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_1213 (Dropout)         (None, 53, 768)      0           ['tf.__operators__.getitem_57[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_56 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_1213[0][0]']           \n",
      " l)                                                                                               \n",
      " dropout_1214 (Dropout)         (None, 53, 1024)     0           ['bidirectional_56[0][0]']       \n",
      " tf.__operators__.getitem_56 (S  (None, 768)         0           ['tf_roberta_model_28[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_57 (Bidirectiona  (None, 1024)        16785408    ['dropout_1214[0][0]']           \n",
      " l)                                                                                               \n",
      " tf.concat_28 (TFOpLambda)      (None, 1792)         0           ['tf.__operators__.getitem_56[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_57[0][0]']       \n",
      " dropout_1215 (Dropout)         (None, 1792)         0           ['tf.concat_28[0][0]']           \n",
      " dense_84 (Dense)               (None, 1024)         1836032     ['dropout_1215[0][0]']           \n",
      " dropout_1216 (Dropout)         (None, 1024)         0           ['dense_84[0][0]']               \n",
      " dense_85 (Dense)               (None, 64)           65600       ['dropout_1216[0][0]']           \n",
      " dropout_1217 (Dropout)         (None, 64)           0           ['dense_85[0][0]']               \n",
      " dense_86 (Dense)               (None, 1)            65          ['dropout_1217[0][0]']           \n",
      "==================================================================================================\n",
      "Total params: 158,020,993\n",
      "Trainable params: 158,020,993\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "  0%|          | 0/1 [26:10<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_28/roberta/pooler/dense/kernel:0', 'tf_roberta_model_28/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_28/roberta/pooler/dense/kernel:0', 'tf_roberta_model_28/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_28/roberta/pooler/dense/kernel:0', 'tf_roberta_model_28/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_28/roberta/pooler/dense/kernel:0', 'tf_roberta_model_28/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1888/1888 - 555s - loss: 40.7250 - accuracy: 0.8396 - val_loss: 0.6513 - val_accuracy: 0.8630 - 555s/epoch - 294ms/step\n",
      "\n",
      "Model: \"model_28\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_28 (TFRoberta  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " Model)                         thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_57 (S  (None, 53, 768)     0           ['tf_roberta_model_28[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_1213 (Dropout)         (None, 53, 768)      0           ['tf.__operators__.getitem_57[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_56 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_1213[0][0]']           \n",
      " l)                                                                                               \n",
      " dropout_1214 (Dropout)         (None, 53, 1024)     0           ['bidirectional_56[0][0]']       \n",
      " tf.__operators__.getitem_56 (S  (None, 768)         0           ['tf_roberta_model_28[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_57 (Bidirectiona  (None, 1024)        16785408    ['dropout_1214[0][0]']           \n",
      " l)                                                                                               \n",
      " tf.concat_28 (TFOpLambda)      (None, 1792)         0           ['tf.__operators__.getitem_56[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_57[0][0]']       \n",
      " dropout_1215 (Dropout)         (None, 1792)         0           ['tf.concat_28[0][0]']           \n",
      " dense_84 (Dense)               (None, 1024)         1836032     ['dropout_1215[0][0]']           \n",
      " dropout_1216 (Dropout)         (None, 1024)         0           ['dense_84[0][0]']               \n",
      " dense_85 (Dense)               (None, 64)           65600       ['dropout_1216[0][0]']           \n",
      " dropout_1217 (Dropout)         (None, 64)           0           ['dense_85[0][0]']               \n",
      " dense_86 (Dense)               (None, 1)            65          ['dropout_1217[0][0]']           \n",
      "==================================================================================================\n",
      "Total params: 158,020,993\n",
      "Trainable params: 33,375,361\n",
      "Non-trainable params: 124,645,632\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "1888/1888 - 432s - loss: 0.4427 - accuracy: 0.8756 - val_loss: 0.4149 - val_accuracy: 0.8567 - 432s/epoch - 229ms/step\n",
      "\n",
      "Epoch 2/2\n",
      "1888/1888 - 487s - loss: 0.3450 - accuracy: 0.8968 - val_loss: 0.4149 - val_accuracy: 0.8612 - 487s/epoch - 258ms/step\n",
      "\n",
      "0.8612012267112732\n",
      "2\n",
      "  0%|          | 0/1 [50:47<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with the following hyperparameters:\n",
      "(5e-05, 0.3, 0.97, 0.99, 0.001, 0.0001, 64, 'tanh', 1024, 64)\n",
      "Model: \"model_29\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_29 (TFRoberta  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " Model)                         thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_59 (S  (None, 53, 768)     0           ['tf_roberta_model_29[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_1255 (Dropout)         (None, 53, 768)      0           ['tf.__operators__.getitem_59[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_58 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_1255[0][0]']           \n",
      " l)                                                                                               \n",
      " dropout_1256 (Dropout)         (None, 53, 1024)     0           ['bidirectional_58[0][0]']       \n",
      " tf.__operators__.getitem_58 (S  (None, 768)         0           ['tf_roberta_model_29[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_59 (Bidirectiona  (None, 1024)        16785408    ['dropout_1256[0][0]']           \n",
      " l)                                                                                               \n",
      " tf.concat_29 (TFOpLambda)      (None, 1792)         0           ['tf.__operators__.getitem_58[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_59[0][0]']       \n",
      " dropout_1257 (Dropout)         (None, 1792)         0           ['tf.concat_29[0][0]']           \n",
      " dense_87 (Dense)               (None, 1024)         1836032     ['dropout_1257[0][0]']           \n",
      " dropout_1258 (Dropout)         (None, 1024)         0           ['dense_87[0][0]']               \n",
      " dense_88 (Dense)               (None, 64)           65600       ['dropout_1258[0][0]']           \n",
      " dropout_1259 (Dropout)         (None, 64)           0           ['dense_88[0][0]']               \n",
      " dense_89 (Dense)               (None, 1)            65          ['dropout_1259[0][0]']           \n",
      "==================================================================================================\n",
      "Total params: 158,020,993\n",
      "Trainable params: 158,020,993\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "  0%|          | 0/1 [51:38<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_29/roberta/pooler/dense/kernel:0', 'tf_roberta_model_29/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_29/roberta/pooler/dense/kernel:0', 'tf_roberta_model_29/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_29/roberta/pooler/dense/kernel:0', 'tf_roberta_model_29/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_29/roberta/pooler/dense/kernel:0', 'tf_roberta_model_29/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1888/1888 - 555s - loss: 41.0602 - accuracy: 0.8361 - val_loss: 0.7625 - val_accuracy: 0.8584 - 555s/epoch - 294ms/step\n",
      "\n",
      "Model: \"model_29\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_29 (TFRoberta  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " Model)                         thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_59 (S  (None, 53, 768)     0           ['tf_roberta_model_29[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_1255 (Dropout)         (None, 53, 768)      0           ['tf.__operators__.getitem_59[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_58 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_1255[0][0]']           \n",
      " l)                                                                                               \n",
      " dropout_1256 (Dropout)         (None, 53, 1024)     0           ['bidirectional_58[0][0]']       \n",
      " tf.__operators__.getitem_58 (S  (None, 768)         0           ['tf_roberta_model_29[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_59 (Bidirectiona  (None, 1024)        16785408    ['dropout_1256[0][0]']           \n",
      " l)                                                                                               \n",
      " tf.concat_29 (TFOpLambda)      (None, 1792)         0           ['tf.__operators__.getitem_58[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_59[0][0]']       \n",
      " dropout_1257 (Dropout)         (None, 1792)         0           ['tf.concat_29[0][0]']           \n",
      " dense_87 (Dense)               (None, 1024)         1836032     ['dropout_1257[0][0]']           \n",
      " dropout_1258 (Dropout)         (None, 1024)         0           ['dense_87[0][0]']               \n",
      " dense_88 (Dense)               (None, 64)           65600       ['dropout_1258[0][0]']           \n",
      " dropout_1259 (Dropout)         (None, 64)           0           ['dense_88[0][0]']               \n",
      " dense_89 (Dense)               (None, 1)            65          ['dropout_1259[0][0]']           \n",
      "==================================================================================================\n",
      "Total params: 158,020,993\n",
      "Trainable params: 33,375,361\n",
      "Non-trainable params: 124,645,632\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "1888/1888 - 431s - loss: 0.4727 - accuracy: 0.8749 - val_loss: 0.4112 - val_accuracy: 0.8645 - 431s/epoch - 228ms/step\n",
      "\n",
      "Epoch 2/2\n",
      "1888/1888 - 488s - loss: 0.3415 - accuracy: 0.8983 - val_loss: 0.4306 - val_accuracy: 0.8675 - 488s/epoch - 259ms/step\n",
      "\n",
      "0.867488443851471\n",
      "2\n",
      "FINISHED THE HYPERPARAMTER TUNING\n",
      "0.8657739559809366\n",
      "THE RESULTS ARE:\n",
      "{'acc': 0.8657739559809366, 'epochs': '[2, 2, 2]', 'status': 'ok', 'space': (5e-05, 0.3, 0.97, 0.99, 0.001, 0.0001, 64, 'tanh', 1024, 64)}\n",
      "  0%|          | 0/1 [1:16:15<?, ?it/s, best loss: ?]\n",
      "Starting from scratch: nesw trials.\n",
      "FINISHED FMIN\n",
      "best:\n",
      "local variable 'best' referenced before assignment\n",
      "Optimizing New Model\n",
      "Attempt to resume a past training if it exists:\n",
      "Found saved Trials! Loading...\n",
      "Rerunning from 0 trials to add another one.\n",
      "  0%|          | 0/1 [00:00<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with the following hyperparameters:\n",
      "(1e-05, 0.3, 0.95, 0.9, 0.005, 0.001, 128, 'sigmoid', 512, 512)\n",
      "Model: \"model_30\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_30 (TFRoberta  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " Model)                         thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_61 (S  (None, 53, 768)     0           ['tf_roberta_model_30[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_1297 (Dropout)         (None, 53, 768)      0           ['tf.__operators__.getitem_61[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_60 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_1297[0][0]']           \n",
      " l)                                                                                               \n",
      " dropout_1298 (Dropout)         (None, 53, 1024)     0           ['bidirectional_60[0][0]']       \n",
      " tf.__operators__.getitem_60 (S  (None, 768)         0           ['tf_roberta_model_30[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_61 (Bidirectiona  (None, 1024)        16785408    ['dropout_1298[0][0]']           \n",
      " l)                                                                                               \n",
      " tf.concat_30 (TFOpLambda)      (None, 1792)         0           ['tf.__operators__.getitem_60[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_61[0][0]']       \n",
      " dropout_1299 (Dropout)         (None, 1792)         0           ['tf.concat_30[0][0]']           \n",
      " dense_90 (Dense)               (None, 512)          918016      ['dropout_1299[0][0]']           \n",
      " dropout_1300 (Dropout)         (None, 512)          0           ['dense_90[0][0]']               \n",
      " dense_91 (Dense)               (None, 512)          262656      ['dropout_1300[0][0]']           \n",
      " dropout_1301 (Dropout)         (None, 512)          0           ['dense_91[0][0]']               \n",
      " dense_92 (Dense)               (None, 1)            513         ['dropout_1301[0][0]']           \n",
      "==================================================================================================\n",
      "Total params: 157,300,481\n",
      "Trainable params: 157,300,481\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "  0%|          | 0/1 [00:52<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_30/roberta/pooler/dense/kernel:0', 'tf_roberta_model_30/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_30/roberta/pooler/dense/kernel:0', 'tf_roberta_model_30/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_30/roberta/pooler/dense/kernel:0', 'tf_roberta_model_30/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_30/roberta/pooler/dense/kernel:0', 'tf_roberta_model_30/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "944/944 - 369s - loss: 1037.5884 - accuracy: 0.7947 - val_loss: 744.0745 - val_accuracy: 0.8562 - 369s/epoch - 391ms/step\n",
      "\n",
      "Model: \"model_30\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_30 (TFRoberta  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " Model)                         thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_61 (S  (None, 53, 768)     0           ['tf_roberta_model_30[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_1297 (Dropout)         (None, 53, 768)      0           ['tf.__operators__.getitem_61[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_60 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_1297[0][0]']           \n",
      " l)                                                                                               \n",
      " dropout_1298 (Dropout)         (None, 53, 1024)     0           ['bidirectional_60[0][0]']       \n",
      " tf.__operators__.getitem_60 (S  (None, 768)         0           ['tf_roberta_model_30[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_61 (Bidirectiona  (None, 1024)        16785408    ['dropout_1298[0][0]']           \n",
      " l)                                                                                               \n",
      " tf.concat_30 (TFOpLambda)      (None, 1792)         0           ['tf.__operators__.getitem_60[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_61[0][0]']       \n",
      " dropout_1299 (Dropout)         (None, 1792)         0           ['tf.concat_30[0][0]']           \n",
      " dense_90 (Dense)               (None, 512)          918016      ['dropout_1299[0][0]']           \n",
      " dropout_1300 (Dropout)         (None, 512)          0           ['dense_90[0][0]']               \n",
      " dense_91 (Dense)               (None, 512)          262656      ['dropout_1300[0][0]']           \n",
      " dropout_1301 (Dropout)         (None, 512)          0           ['dense_91[0][0]']               \n",
      " dense_92 (Dense)               (None, 1)            513         ['dropout_1301[0][0]']           \n",
      "==================================================================================================\n",
      "Total params: 157,300,481\n",
      "Trainable params: 32,654,849\n",
      "Non-trainable params: 124,645,632\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "944/944 - 244s - loss: 528.8034 - accuracy: 0.8453 - val_loss: 348.0806 - val_accuracy: 0.8637 - 244s/epoch - 259ms/step\n",
      "\n",
      "Epoch 2/2\n",
      "944/944 - 299s - loss: 228.1372 - accuracy: 0.8265 - val_loss: 133.1950 - val_accuracy: 0.8622 - 299s/epoch - 317ms/step\n",
      "\n",
      "0.8636521100997925\n",
      "1\n",
      "  0%|          | 0/1 [16:07<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with the following hyperparameters:\n",
      "(1e-05, 0.3, 0.95, 0.9, 0.005, 0.001, 128, 'sigmoid', 512, 512)\n",
      "Model: \"model_31\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_31 (TFRoberta  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " Model)                         thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_63 (S  (None, 53, 768)     0           ['tf_roberta_model_31[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_1339 (Dropout)         (None, 53, 768)      0           ['tf.__operators__.getitem_63[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_62 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_1339[0][0]']           \n",
      " l)                                                                                               \n",
      " dropout_1340 (Dropout)         (None, 53, 1024)     0           ['bidirectional_62[0][0]']       \n",
      " tf.__operators__.getitem_62 (S  (None, 768)         0           ['tf_roberta_model_31[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_63 (Bidirectiona  (None, 1024)        16785408    ['dropout_1340[0][0]']           \n",
      " l)                                                                                               \n",
      " tf.concat_31 (TFOpLambda)      (None, 1792)         0           ['tf.__operators__.getitem_62[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_63[0][0]']       \n",
      " dropout_1341 (Dropout)         (None, 1792)         0           ['tf.concat_31[0][0]']           \n",
      " dense_93 (Dense)               (None, 512)          918016      ['dropout_1341[0][0]']           \n",
      " dropout_1342 (Dropout)         (None, 512)          0           ['dense_93[0][0]']               \n",
      " dense_94 (Dense)               (None, 512)          262656      ['dropout_1342[0][0]']           \n",
      " dropout_1343 (Dropout)         (None, 512)          0           ['dense_94[0][0]']               \n",
      " dense_95 (Dense)               (None, 1)            513         ['dropout_1343[0][0]']           \n",
      "==================================================================================================\n",
      "Total params: 157,300,481\n",
      "Trainable params: 157,300,481\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "  0%|          | 0/1 [16:54<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_31/roberta/pooler/dense/kernel:0', 'tf_roberta_model_31/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_31/roberta/pooler/dense/kernel:0', 'tf_roberta_model_31/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_31/roberta/pooler/dense/kernel:0', 'tf_roberta_model_31/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_31/roberta/pooler/dense/kernel:0', 'tf_roberta_model_31/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "944/944 - 376s - loss: 1037.4694 - accuracy: 0.7927 - val_loss: 743.9675 - val_accuracy: 0.8574 - 376s/epoch - 399ms/step\n",
      "\n",
      "Model: \"model_31\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_31 (TFRoberta  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " Model)                         thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_63 (S  (None, 53, 768)     0           ['tf_roberta_model_31[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_1339 (Dropout)         (None, 53, 768)      0           ['tf.__operators__.getitem_63[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_62 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_1339[0][0]']           \n",
      " l)                                                                                               \n",
      " dropout_1340 (Dropout)         (None, 53, 1024)     0           ['bidirectional_62[0][0]']       \n",
      " tf.__operators__.getitem_62 (S  (None, 768)         0           ['tf_roberta_model_31[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_63 (Bidirectiona  (None, 1024)        16785408    ['dropout_1340[0][0]']           \n",
      " l)                                                                                               \n",
      " tf.concat_31 (TFOpLambda)      (None, 1792)         0           ['tf.__operators__.getitem_62[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_63[0][0]']       \n",
      " dropout_1341 (Dropout)         (None, 1792)         0           ['tf.concat_31[0][0]']           \n",
      " dense_93 (Dense)               (None, 512)          918016      ['dropout_1341[0][0]']           \n",
      " dropout_1342 (Dropout)         (None, 512)          0           ['dense_93[0][0]']               \n",
      " dense_94 (Dense)               (None, 512)          262656      ['dropout_1342[0][0]']           \n",
      " dropout_1343 (Dropout)         (None, 512)          0           ['dense_94[0][0]']               \n",
      " dense_95 (Dense)               (None, 1)            513         ['dropout_1343[0][0]']           \n",
      "==================================================================================================\n",
      "Total params: 157,300,481\n",
      "Trainable params: 32,654,849\n",
      "Non-trainable params: 124,645,632\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "944/944 - 244s - loss: 528.7109 - accuracy: 0.8504 - val_loss: 348.0251 - val_accuracy: 0.8524 - 244s/epoch - 259ms/step\n",
      "\n",
      "Epoch 2/2\n",
      "944/944 - 301s - loss: 228.1125 - accuracy: 0.8296 - val_loss: 133.2189 - val_accuracy: 0.8609 - 301s/epoch - 319ms/step\n",
      "\n",
      "0.8608537912368774\n",
      "2\n",
      "  0%|          | 0/1 [32:19<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with the following hyperparameters:\n",
      "(1e-05, 0.3, 0.95, 0.9, 0.005, 0.001, 128, 'sigmoid', 512, 512)\n",
      "Model: \"model_32\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_32 (TFRoberta  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " Model)                         thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_65 (S  (None, 53, 768)     0           ['tf_roberta_model_32[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_1381 (Dropout)         (None, 53, 768)      0           ['tf.__operators__.getitem_65[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_64 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_1381[0][0]']           \n",
      " l)                                                                                               \n",
      " dropout_1382 (Dropout)         (None, 53, 1024)     0           ['bidirectional_64[0][0]']       \n",
      " tf.__operators__.getitem_64 (S  (None, 768)         0           ['tf_roberta_model_32[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_65 (Bidirectiona  (None, 1024)        16785408    ['dropout_1382[0][0]']           \n",
      " l)                                                                                               \n",
      " tf.concat_32 (TFOpLambda)      (None, 1792)         0           ['tf.__operators__.getitem_64[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_65[0][0]']       \n",
      " dropout_1383 (Dropout)         (None, 1792)         0           ['tf.concat_32[0][0]']           \n",
      " dense_96 (Dense)               (None, 512)          918016      ['dropout_1383[0][0]']           \n",
      " dropout_1384 (Dropout)         (None, 512)          0           ['dense_96[0][0]']               \n",
      " dense_97 (Dense)               (None, 512)          262656      ['dropout_1384[0][0]']           \n",
      " dropout_1385 (Dropout)         (None, 512)          0           ['dense_97[0][0]']               \n",
      " dense_98 (Dense)               (None, 1)            513         ['dropout_1385[0][0]']           \n",
      "==================================================================================================\n",
      "Total params: 157,300,481\n",
      "Trainable params: 157,300,481\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "  0%|          | 0/1 [33:11<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_32/roberta/pooler/dense/kernel:0', 'tf_roberta_model_32/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_32/roberta/pooler/dense/kernel:0', 'tf_roberta_model_32/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_32/roberta/pooler/dense/kernel:0', 'tf_roberta_model_32/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_32/roberta/pooler/dense/kernel:0', 'tf_roberta_model_32/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "944/944 - 373s - loss: 1037.4562 - accuracy: 0.7957 - val_loss: 743.9309 - val_accuracy: 0.8468 - 373s/epoch - 395ms/step\n",
      "\n",
      "Model: \"model_32\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_32 (TFRoberta  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " Model)                         thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_65 (S  (None, 53, 768)     0           ['tf_roberta_model_32[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_1381 (Dropout)         (None, 53, 768)      0           ['tf.__operators__.getitem_65[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_64 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_1381[0][0]']           \n",
      " l)                                                                                               \n",
      " dropout_1382 (Dropout)         (None, 53, 1024)     0           ['bidirectional_64[0][0]']       \n",
      " tf.__operators__.getitem_64 (S  (None, 768)         0           ['tf_roberta_model_32[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_65 (Bidirectiona  (None, 1024)        16785408    ['dropout_1382[0][0]']           \n",
      " l)                                                                                               \n",
      " tf.concat_32 (TFOpLambda)      (None, 1792)         0           ['tf.__operators__.getitem_64[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_65[0][0]']       \n",
      " dropout_1383 (Dropout)         (None, 1792)         0           ['tf.concat_32[0][0]']           \n",
      " dense_96 (Dense)               (None, 512)          918016      ['dropout_1383[0][0]']           \n",
      " dropout_1384 (Dropout)         (None, 512)          0           ['dense_96[0][0]']               \n",
      " dense_97 (Dense)               (None, 512)          262656      ['dropout_1384[0][0]']           \n",
      " dropout_1385 (Dropout)         (None, 512)          0           ['dense_97[0][0]']               \n",
      " dense_98 (Dense)               (None, 1)            513         ['dropout_1385[0][0]']           \n",
      "==================================================================================================\n",
      "Total params: 157,300,481\n",
      "Trainable params: 32,654,849\n",
      "Non-trainable params: 124,645,632\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "944/944 - 244s - loss: 528.6218 - accuracy: 0.8393 - val_loss: 347.8876 - val_accuracy: 0.8573 - 244s/epoch - 258ms/step\n",
      "\n",
      "Epoch 2/2\n",
      "944/944 - 300s - loss: 228.0108 - accuracy: 0.8204 - val_loss: 133.1589 - val_accuracy: 0.8552 - 300s/epoch - 318ms/step\n",
      "\n",
      "0.8573130965232849\n",
      "1\n",
      "FINISHED THE HYPERPARAMTER TUNING\n",
      "0.860606332619985\n",
      "THE RESULTS ARE:\n",
      "{'acc': 0.860606332619985, 'epochs': '[1, 2, 1]', 'status': 'ok', 'space': (1e-05, 0.3, 0.95, 0.9, 0.005, 0.001, 128, 'sigmoid', 512, 512)}\n",
      "  0%|          | 0/1 [48:31<?, ?it/s, best loss: ?]\n",
      "Starting from scratch: nesw trials.\n",
      "FINISHED FMIN\n",
      "best:\n",
      "local variable 'best' referenced before assignment\n",
      "Optimizing New Model\n",
      "Attempt to resume a past training if it exists:\n",
      "Found saved Trials! Loading...\n",
      "Rerunning from 0 trials to add another one.\n",
      "  0%|          | 0/1 [00:00<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with the following hyperparameters:\n",
      "(0.0001, 0.6, 0.99, 0.95, 0, 0, 64, 'sigmoid', 256, 64)\n",
      "Model: \"model_33\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_33 (TFRoberta  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " Model)                         thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_67 (S  (None, 53, 768)     0           ['tf_roberta_model_33[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_1423 (Dropout)         (None, 53, 768)      0           ['tf.__operators__.getitem_67[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_66 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_1423[0][0]']           \n",
      " l)                                                                                               \n",
      " dropout_1424 (Dropout)         (None, 53, 1024)     0           ['bidirectional_66[0][0]']       \n",
      " tf.__operators__.getitem_66 (S  (None, 768)         0           ['tf_roberta_model_33[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_67 (Bidirectiona  (None, 1024)        16785408    ['dropout_1424[0][0]']           \n",
      " l)                                                                                               \n",
      " tf.concat_33 (TFOpLambda)      (None, 1792)         0           ['tf.__operators__.getitem_66[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_67[0][0]']       \n",
      " dropout_1425 (Dropout)         (None, 1792)         0           ['tf.concat_33[0][0]']           \n",
      " dense_99 (Dense)               (None, 256)          459008      ['dropout_1425[0][0]']           \n",
      " dropout_1426 (Dropout)         (None, 256)          0           ['dense_99[0][0]']               \n",
      " dense_100 (Dense)              (None, 64)           16448       ['dropout_1426[0][0]']           \n",
      " dropout_1427 (Dropout)         (None, 64)           0           ['dense_100[0][0]']              \n",
      " dense_101 (Dense)              (None, 1)            65          ['dropout_1427[0][0]']           \n",
      "==================================================================================================\n",
      "Total params: 156,594,817\n",
      "Trainable params: 156,594,817\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "  0%|          | 0/1 [00:49<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_33/roberta/pooler/dense/kernel:0', 'tf_roberta_model_33/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_33/roberta/pooler/dense/kernel:0', 'tf_roberta_model_33/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_33/roberta/pooler/dense/kernel:0', 'tf_roberta_model_33/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_33/roberta/pooler/dense/kernel:0', 'tf_roberta_model_33/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1888/1888 - 536s - loss: 0.6532 - accuracy: 0.5752 - val_loss: 0.6932 - val_accuracy: 0.5007 - 536s/epoch - 284ms/step\n",
      "\n",
      "Model: \"model_33\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_33 (TFRoberta  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " Model)                         thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_67 (S  (None, 53, 768)     0           ['tf_roberta_model_33[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_1423 (Dropout)         (None, 53, 768)      0           ['tf.__operators__.getitem_67[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_66 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_1423[0][0]']           \n",
      " l)                                                                                               \n",
      " dropout_1424 (Dropout)         (None, 53, 1024)     0           ['bidirectional_66[0][0]']       \n",
      " tf.__operators__.getitem_66 (S  (None, 768)         0           ['tf_roberta_model_33[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_67 (Bidirectiona  (None, 1024)        16785408    ['dropout_1424[0][0]']           \n",
      " l)                                                                                               \n",
      " tf.concat_33 (TFOpLambda)      (None, 1792)         0           ['tf.__operators__.getitem_66[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_67[0][0]']       \n",
      " dropout_1425 (Dropout)         (None, 1792)         0           ['tf.concat_33[0][0]']           \n",
      " dense_99 (Dense)               (None, 256)          459008      ['dropout_1425[0][0]']           \n",
      " dropout_1426 (Dropout)         (None, 256)          0           ['dense_99[0][0]']               \n",
      " dense_100 (Dense)              (None, 64)           16448       ['dropout_1426[0][0]']           \n",
      " dropout_1427 (Dropout)         (None, 64)           0           ['dense_100[0][0]']              \n",
      " dense_101 (Dense)              (None, 1)            65          ['dropout_1427[0][0]']           \n",
      "==================================================================================================\n",
      "Total params: 156,594,817\n",
      "Trainable params: 31,949,185\n",
      "Non-trainable params: 124,645,632\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "1888/1888 - 419s - loss: 0.6949 - accuracy: 0.5002 - val_loss: 0.6932 - val_accuracy: 0.5007 - 419s/epoch - 222ms/step\n",
      "\n",
      "Epoch 2/2\n",
      "1888/1888 - 474s - loss: 0.6935 - accuracy: 0.5024 - val_loss: 0.6932 - val_accuracy: 0.5007 - 474s/epoch - 251ms/step\n",
      "\n",
      "0.5006535053253174\n",
      "1\n",
      "  0%|          | 0/1 [24:41<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with the following hyperparameters:\n",
      "(0.0001, 0.6, 0.99, 0.95, 0, 0, 64, 'sigmoid', 256, 64)\n",
      "Model: \"model_34\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_34 (TFRoberta  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " Model)                         thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_69 (S  (None, 53, 768)     0           ['tf_roberta_model_34[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_1465 (Dropout)         (None, 53, 768)      0           ['tf.__operators__.getitem_69[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_68 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_1465[0][0]']           \n",
      " l)                                                                                               \n",
      " dropout_1466 (Dropout)         (None, 53, 1024)     0           ['bidirectional_68[0][0]']       \n",
      " tf.__operators__.getitem_68 (S  (None, 768)         0           ['tf_roberta_model_34[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_69 (Bidirectiona  (None, 1024)        16785408    ['dropout_1466[0][0]']           \n",
      " l)                                                                                               \n",
      " tf.concat_34 (TFOpLambda)      (None, 1792)         0           ['tf.__operators__.getitem_68[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_69[0][0]']       \n",
      " dropout_1467 (Dropout)         (None, 1792)         0           ['tf.concat_34[0][0]']           \n",
      " dense_102 (Dense)              (None, 256)          459008      ['dropout_1467[0][0]']           \n",
      " dropout_1468 (Dropout)         (None, 256)          0           ['dense_102[0][0]']              \n",
      " dense_103 (Dense)              (None, 64)           16448       ['dropout_1468[0][0]']           \n",
      " dropout_1469 (Dropout)         (None, 64)           0           ['dense_103[0][0]']              \n",
      " dense_104 (Dense)              (None, 1)            65          ['dropout_1469[0][0]']           \n",
      "==================================================================================================\n",
      "Total params: 156,594,817\n",
      "Trainable params: 156,594,817\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "  0%|          | 0/1 [25:29<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_34/roberta/pooler/dense/kernel:0', 'tf_roberta_model_34/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_34/roberta/pooler/dense/kernel:0', 'tf_roberta_model_34/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_34/roberta/pooler/dense/kernel:0', 'tf_roberta_model_34/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_34/roberta/pooler/dense/kernel:0', 'tf_roberta_model_34/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1888/1888 - 542s - loss: 0.7249 - accuracy: 0.5085 - val_loss: 0.6931 - val_accuracy: 0.5038 - 542s/epoch - 287ms/step\n",
      "\n",
      "Model: \"model_34\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_34 (TFRoberta  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " Model)                         thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_69 (S  (None, 53, 768)     0           ['tf_roberta_model_34[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_1465 (Dropout)         (None, 53, 768)      0           ['tf.__operators__.getitem_69[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_68 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_1465[0][0]']           \n",
      " l)                                                                                               \n",
      " dropout_1466 (Dropout)         (None, 53, 1024)     0           ['bidirectional_68[0][0]']       \n",
      " tf.__operators__.getitem_68 (S  (None, 768)         0           ['tf_roberta_model_34[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_69 (Bidirectiona  (None, 1024)        16785408    ['dropout_1466[0][0]']           \n",
      " l)                                                                                               \n",
      " tf.concat_34 (TFOpLambda)      (None, 1792)         0           ['tf.__operators__.getitem_68[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_69[0][0]']       \n",
      " dropout_1467 (Dropout)         (None, 1792)         0           ['tf.concat_34[0][0]']           \n",
      " dense_102 (Dense)              (None, 256)          459008      ['dropout_1467[0][0]']           \n",
      " dropout_1468 (Dropout)         (None, 256)          0           ['dense_102[0][0]']              \n",
      " dense_103 (Dense)              (None, 64)           16448       ['dropout_1468[0][0]']           \n",
      " dropout_1469 (Dropout)         (None, 64)           0           ['dense_103[0][0]']              \n",
      " dense_104 (Dense)              (None, 1)            65          ['dropout_1469[0][0]']           \n",
      "==================================================================================================\n",
      "Total params: 156,594,817\n",
      "Trainable params: 31,949,185\n",
      "Non-trainable params: 124,645,632\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "1888/1888 - 422s - loss: 0.6941 - accuracy: 0.5009 - val_loss: 0.6932 - val_accuracy: 0.4962 - 422s/epoch - 223ms/step\n",
      "\n",
      "Epoch 2/2\n",
      "1888/1888 - 477s - loss: 0.6934 - accuracy: 0.5001 - val_loss: 0.6931 - val_accuracy: 0.5038 - 477s/epoch - 253ms/step\n",
      "\n",
      "0.5038385391235352\n",
      "2\n",
      "  0%|          | 0/1 [49:33<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with the following hyperparameters:\n",
      "(0.0001, 0.6, 0.99, 0.95, 0, 0, 64, 'sigmoid', 256, 64)\n",
      "Model: \"model_35\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_35 (TFRoberta  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " Model)                         thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_71 (S  (None, 53, 768)     0           ['tf_roberta_model_35[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_1507 (Dropout)         (None, 53, 768)      0           ['tf.__operators__.getitem_71[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_70 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_1507[0][0]']           \n",
      " l)                                                                                               \n",
      " dropout_1508 (Dropout)         (None, 53, 1024)     0           ['bidirectional_70[0][0]']       \n",
      " tf.__operators__.getitem_70 (S  (None, 768)         0           ['tf_roberta_model_35[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_71 (Bidirectiona  (None, 1024)        16785408    ['dropout_1508[0][0]']           \n",
      " l)                                                                                               \n",
      " tf.concat_35 (TFOpLambda)      (None, 1792)         0           ['tf.__operators__.getitem_70[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_71[0][0]']       \n",
      " dropout_1509 (Dropout)         (None, 1792)         0           ['tf.concat_35[0][0]']           \n",
      " dense_105 (Dense)              (None, 256)          459008      ['dropout_1509[0][0]']           \n",
      " dropout_1510 (Dropout)         (None, 256)          0           ['dense_105[0][0]']              \n",
      " dense_106 (Dense)              (None, 64)           16448       ['dropout_1510[0][0]']           \n",
      " dropout_1511 (Dropout)         (None, 64)           0           ['dense_106[0][0]']              \n",
      " dense_107 (Dense)              (None, 1)            65          ['dropout_1511[0][0]']           \n",
      "==================================================================================================\n",
      "Total params: 156,594,817\n",
      "Trainable params: 156,594,817\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "  0%|          | 0/1 [50:22<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_35/roberta/pooler/dense/kernel:0', 'tf_roberta_model_35/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_35/roberta/pooler/dense/kernel:0', 'tf_roberta_model_35/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_35/roberta/pooler/dense/kernel:0', 'tf_roberta_model_35/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_35/roberta/pooler/dense/kernel:0', 'tf_roberta_model_35/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1888/1888 - 549s - loss: 0.6980 - accuracy: 0.5147 - val_loss: 0.6933 - val_accuracy: 0.5026 - 549s/epoch - 291ms/step\n",
      "\n",
      "Model: \"model_35\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_35 (TFRoberta  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " Model)                         thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_71 (S  (None, 53, 768)     0           ['tf_roberta_model_35[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_1507 (Dropout)         (None, 53, 768)      0           ['tf.__operators__.getitem_71[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_70 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_1507[0][0]']           \n",
      " l)                                                                                               \n",
      " dropout_1508 (Dropout)         (None, 53, 1024)     0           ['bidirectional_70[0][0]']       \n",
      " tf.__operators__.getitem_70 (S  (None, 768)         0           ['tf_roberta_model_35[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_71 (Bidirectiona  (None, 1024)        16785408    ['dropout_1508[0][0]']           \n",
      " l)                                                                                               \n",
      " tf.concat_35 (TFOpLambda)      (None, 1792)         0           ['tf.__operators__.getitem_70[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_71[0][0]']       \n",
      " dropout_1509 (Dropout)         (None, 1792)         0           ['tf.concat_35[0][0]']           \n",
      " dense_105 (Dense)              (None, 256)          459008      ['dropout_1509[0][0]']           \n",
      " dropout_1510 (Dropout)         (None, 256)          0           ['dense_105[0][0]']              \n",
      " dense_106 (Dense)              (None, 64)           16448       ['dropout_1510[0][0]']           \n",
      " dropout_1511 (Dropout)         (None, 64)           0           ['dense_106[0][0]']              \n",
      " dense_107 (Dense)              (None, 1)            65          ['dropout_1511[0][0]']           \n",
      "==================================================================================================\n",
      "Total params: 156,594,817\n",
      "Trainable params: 31,949,185\n",
      "Non-trainable params: 124,645,632\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "1888/1888 - 419s - loss: 0.6946 - accuracy: 0.4993 - val_loss: 0.6931 - val_accuracy: 0.5026 - 419s/epoch - 222ms/step\n",
      "\n",
      "Epoch 2/2\n",
      "1888/1888 - 473s - loss: 0.6935 - accuracy: 0.5008 - val_loss: 0.6931 - val_accuracy: 0.5026 - 473s/epoch - 251ms/step\n",
      "\n",
      "0.5025811195373535\n",
      "1\n",
      "FINISHED THE HYPERPARAMTER TUNING\n",
      "0.5023577213287354\n",
      "THE RESULTS ARE:\n",
      "{'acc': 0.5023577213287354, 'epochs': '[1, 2, 1]', 'status': 'ok', 'space': (0.0001, 0.6, 0.99, 0.95, 0, 0, 64, 'sigmoid', 256, 64)}\n",
      "  0%|          | 0/1 [1:14:27<?, ?it/s, best loss: ?]\n",
      "Starting from scratch: nesw trials.\n",
      "FINISHED FMIN\n",
      "best:\n",
      "local variable 'best' referenced before assignment\n",
      "Optimizing New Model\n",
      "Attempt to resume a past training if it exists:\n",
      "Found saved Trials! Loading...\n",
      "Rerunning from 0 trials to add another one.\n",
      "  0%|          | 0/1 [00:00<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with the following hyperparameters:\n",
      "(0.0001, 0.6, 0.9, 0.95, 0.01, 0.0005, 256, 'tanh', 1024, 512)\n",
      "Model: \"model_36\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_36 (TFRoberta  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " Model)                         thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_73 (S  (None, 53, 768)     0           ['tf_roberta_model_36[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_1549 (Dropout)         (None, 53, 768)      0           ['tf.__operators__.getitem_73[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_72 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_1549[0][0]']           \n",
      " l)                                                                                               \n",
      " dropout_1550 (Dropout)         (None, 53, 1024)     0           ['bidirectional_72[0][0]']       \n",
      " tf.__operators__.getitem_72 (S  (None, 768)         0           ['tf_roberta_model_36[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_73 (Bidirectiona  (None, 1024)        16785408    ['dropout_1550[0][0]']           \n",
      " l)                                                                                               \n",
      " tf.concat_36 (TFOpLambda)      (None, 1792)         0           ['tf.__operators__.getitem_72[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_73[0][0]']       \n",
      " dropout_1551 (Dropout)         (None, 1792)         0           ['tf.concat_36[0][0]']           \n",
      " dense_108 (Dense)              (None, 1024)         1836032     ['dropout_1551[0][0]']           \n",
      " dropout_1552 (Dropout)         (None, 1024)         0           ['dense_108[0][0]']              \n",
      " dense_109 (Dense)              (None, 512)          524800      ['dropout_1552[0][0]']           \n",
      " dropout_1553 (Dropout)         (None, 512)          0           ['dense_109[0][0]']              \n",
      " dense_110 (Dense)              (None, 1)            513         ['dropout_1553[0][0]']           \n",
      "==================================================================================================\n",
      "Total params: 158,480,641\n",
      "Trainable params: 158,480,641\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "  0%|          | 0/1 [00:51<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_36/roberta/pooler/dense/kernel:0', 'tf_roberta_model_36/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_36/roberta/pooler/dense/kernel:0', 'tf_roberta_model_36/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_36/roberta/pooler/dense/kernel:0', 'tf_roberta_model_36/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_36/roberta/pooler/dense/kernel:0', 'tf_roberta_model_36/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472/472 - 279s - loss: 835.1861 - accuracy: 0.8140 - val_loss: 27.0056 - val_accuracy: 0.8506 - 279s/epoch - 591ms/step\n",
      "\n",
      "Model: \"model_36\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_36 (TFRoberta  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " Model)                         thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_73 (S  (None, 53, 768)     0           ['tf_roberta_model_36[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_1549 (Dropout)         (None, 53, 768)      0           ['tf.__operators__.getitem_73[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_72 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_1549[0][0]']           \n",
      " l)                                                                                               \n",
      " dropout_1550 (Dropout)         (None, 53, 1024)     0           ['bidirectional_72[0][0]']       \n",
      " tf.__operators__.getitem_72 (S  (None, 768)         0           ['tf_roberta_model_36[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_73 (Bidirectiona  (None, 1024)        16785408    ['dropout_1550[0][0]']           \n",
      " l)                                                                                               \n",
      " tf.concat_36 (TFOpLambda)      (None, 1792)         0           ['tf.__operators__.getitem_72[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_73[0][0]']       \n",
      " dropout_1551 (Dropout)         (None, 1792)         0           ['tf.concat_36[0][0]']           \n",
      " dense_108 (Dense)              (None, 1024)         1836032     ['dropout_1551[0][0]']           \n",
      " dropout_1552 (Dropout)         (None, 1024)         0           ['dense_108[0][0]']              \n",
      " dense_109 (Dense)              (None, 512)          524800      ['dropout_1552[0][0]']           \n",
      " dropout_1553 (Dropout)         (None, 512)          0           ['dense_109[0][0]']              \n",
      " dense_110 (Dense)              (None, 1)            513         ['dropout_1553[0][0]']           \n",
      "==================================================================================================\n",
      "Total params: 158,480,641\n",
      "Trainable params: 33,835,009\n",
      "Non-trainable params: 124,645,632\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "472/472 - 154s - loss: 6.8890 - accuracy: 0.7901 - val_loss: 2.7577 - val_accuracy: 0.6578 - 154s/epoch - 327ms/step\n",
      "\n",
      "Epoch 2/2\n",
      "472/472 - 210s - loss: 2.7694 - accuracy: 0.6108 - val_loss: 2.7566 - val_accuracy: 0.6535 - 210s/epoch - 445ms/step\n",
      "\n",
      "0.6578150391578674\n",
      "1\n",
      "  0%|          | 0/1 [11:38<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with the following hyperparameters:\n",
      "(0.0001, 0.6, 0.9, 0.95, 0.01, 0.0005, 256, 'tanh', 1024, 512)\n",
      "Model: \"model_37\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_37 (TFRoberta  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " Model)                         thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_75 (S  (None, 53, 768)     0           ['tf_roberta_model_37[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_1591 (Dropout)         (None, 53, 768)      0           ['tf.__operators__.getitem_75[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_74 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_1591[0][0]']           \n",
      " l)                                                                                               \n",
      " dropout_1592 (Dropout)         (None, 53, 1024)     0           ['bidirectional_74[0][0]']       \n",
      " tf.__operators__.getitem_74 (S  (None, 768)         0           ['tf_roberta_model_37[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_75 (Bidirectiona  (None, 1024)        16785408    ['dropout_1592[0][0]']           \n",
      " l)                                                                                               \n",
      " tf.concat_37 (TFOpLambda)      (None, 1792)         0           ['tf.__operators__.getitem_74[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_75[0][0]']       \n",
      " dropout_1593 (Dropout)         (None, 1792)         0           ['tf.concat_37[0][0]']           \n",
      " dense_111 (Dense)              (None, 1024)         1836032     ['dropout_1593[0][0]']           \n",
      " dropout_1594 (Dropout)         (None, 1024)         0           ['dense_111[0][0]']              \n",
      " dense_112 (Dense)              (None, 512)          524800      ['dropout_1594[0][0]']           \n",
      " dropout_1595 (Dropout)         (None, 512)          0           ['dense_112[0][0]']              \n",
      " dense_113 (Dense)              (None, 1)            513         ['dropout_1595[0][0]']           \n",
      "==================================================================================================\n",
      "Total params: 158,480,641\n",
      "Trainable params: 158,480,641\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "  0%|          | 0/1 [12:24<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_37/roberta/pooler/dense/kernel:0', 'tf_roberta_model_37/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_37/roberta/pooler/dense/kernel:0', 'tf_roberta_model_37/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_37/roberta/pooler/dense/kernel:0', 'tf_roberta_model_37/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_37/roberta/pooler/dense/kernel:0', 'tf_roberta_model_37/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472/472 - 281s - loss: 835.1381 - accuracy: 0.8236 - val_loss: 27.0346 - val_accuracy: 0.8315 - 281s/epoch - 596ms/step\n",
      "\n",
      "Model: \"model_37\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_37 (TFRoberta  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " Model)                         thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_75 (S  (None, 53, 768)     0           ['tf_roberta_model_37[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_1591 (Dropout)         (None, 53, 768)      0           ['tf.__operators__.getitem_75[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_74 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_1591[0][0]']           \n",
      " l)                                                                                               \n",
      " dropout_1592 (Dropout)         (None, 53, 1024)     0           ['bidirectional_74[0][0]']       \n",
      " tf.__operators__.getitem_74 (S  (None, 768)         0           ['tf_roberta_model_37[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_75 (Bidirectiona  (None, 1024)        16785408    ['dropout_1592[0][0]']           \n",
      " l)                                                                                               \n",
      " tf.concat_37 (TFOpLambda)      (None, 1792)         0           ['tf.__operators__.getitem_74[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_75[0][0]']       \n",
      " dropout_1593 (Dropout)         (None, 1792)         0           ['tf.concat_37[0][0]']           \n",
      " dense_111 (Dense)              (None, 1024)         1836032     ['dropout_1593[0][0]']           \n",
      " dropout_1594 (Dropout)         (None, 1024)         0           ['dense_111[0][0]']              \n",
      " dense_112 (Dense)              (None, 512)          524800      ['dropout_1594[0][0]']           \n",
      " dropout_1595 (Dropout)         (None, 512)          0           ['dense_112[0][0]']              \n",
      " dense_113 (Dense)              (None, 1)            513         ['dropout_1595[0][0]']           \n",
      "==================================================================================================\n",
      "Total params: 158,480,641\n",
      "Trainable params: 33,835,009\n",
      "Non-trainable params: 124,645,632\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "472/472 - 154s - loss: 6.8214 - accuracy: 0.8516 - val_loss: 2.5801 - val_accuracy: 0.8472 - 154s/epoch - 327ms/step\n",
      "\n",
      "Epoch 2/2\n",
      "472/472 - 209s - loss: 2.5709 - accuracy: 0.8582 - val_loss: 2.5599 - val_accuracy: 0.8485 - 209s/epoch - 443ms/step\n",
      "\n",
      "0.8484613299369812\n",
      "2\n",
      "  0%|          | 0/1 [23:12<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with the following hyperparameters:\n",
      "(0.0001, 0.6, 0.9, 0.95, 0.01, 0.0005, 256, 'tanh', 1024, 512)\n",
      "Model: \"model_38\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_38 (TFRoberta  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " Model)                         thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_77 (S  (None, 53, 768)     0           ['tf_roberta_model_38[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_1633 (Dropout)         (None, 53, 768)      0           ['tf.__operators__.getitem_77[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_76 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_1633[0][0]']           \n",
      " l)                                                                                               \n",
      " dropout_1634 (Dropout)         (None, 53, 1024)     0           ['bidirectional_76[0][0]']       \n",
      " tf.__operators__.getitem_76 (S  (None, 768)         0           ['tf_roberta_model_38[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_77 (Bidirectiona  (None, 1024)        16785408    ['dropout_1634[0][0]']           \n",
      " l)                                                                                               \n",
      " tf.concat_38 (TFOpLambda)      (None, 1792)         0           ['tf.__operators__.getitem_76[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_77[0][0]']       \n",
      " dropout_1635 (Dropout)         (None, 1792)         0           ['tf.concat_38[0][0]']           \n",
      " dense_114 (Dense)              (None, 1024)         1836032     ['dropout_1635[0][0]']           \n",
      " dropout_1636 (Dropout)         (None, 1024)         0           ['dense_114[0][0]']              \n",
      " dense_115 (Dense)              (None, 512)          524800      ['dropout_1636[0][0]']           \n",
      " dropout_1637 (Dropout)         (None, 512)          0           ['dense_115[0][0]']              \n",
      " dense_116 (Dense)              (None, 1)            513         ['dropout_1637[0][0]']           \n",
      "==================================================================================================\n",
      "Total params: 158,480,641\n",
      "Trainable params: 158,480,641\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "  0%|          | 0/1 [23:59<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_38/roberta/pooler/dense/kernel:0', 'tf_roberta_model_38/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_38/roberta/pooler/dense/kernel:0', 'tf_roberta_model_38/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_38/roberta/pooler/dense/kernel:0', 'tf_roberta_model_38/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_38/roberta/pooler/dense/kernel:0', 'tf_roberta_model_38/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472/472 - 281s - loss: 835.3997 - accuracy: 0.8204 - val_loss: 28.2806 - val_accuracy: 0.8253 - 281s/epoch - 595ms/step\n",
      "\n",
      "Model: \"model_38\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_38 (TFRoberta  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " Model)                         thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_77 (S  (None, 53, 768)     0           ['tf_roberta_model_38[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_1633 (Dropout)         (None, 53, 768)      0           ['tf.__operators__.getitem_77[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_76 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_1633[0][0]']           \n",
      " l)                                                                                               \n",
      " dropout_1634 (Dropout)         (None, 53, 1024)     0           ['bidirectional_76[0][0]']       \n",
      " tf.__operators__.getitem_76 (S  (None, 768)         0           ['tf_roberta_model_38[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_77 (Bidirectiona  (None, 1024)        16785408    ['dropout_1634[0][0]']           \n",
      " l)                                                                                               \n",
      " tf.concat_38 (TFOpLambda)      (None, 1792)         0           ['tf.__operators__.getitem_76[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_77[0][0]']       \n",
      " dropout_1635 (Dropout)         (None, 1792)         0           ['tf.concat_38[0][0]']           \n",
      " dense_114 (Dense)              (None, 1024)         1836032     ['dropout_1635[0][0]']           \n",
      " dropout_1636 (Dropout)         (None, 1024)         0           ['dense_114[0][0]']              \n",
      " dense_115 (Dense)              (None, 512)          524800      ['dropout_1636[0][0]']           \n",
      " dropout_1637 (Dropout)         (None, 512)          0           ['dense_115[0][0]']              \n",
      " dense_116 (Dense)              (None, 1)            513         ['dropout_1637[0][0]']           \n",
      "==================================================================================================\n",
      "Total params: 158,480,641\n",
      "Trainable params: 33,835,009\n",
      "Non-trainable params: 124,645,632\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "472/472 - 154s - loss: 6.9385 - accuracy: 0.8473 - val_loss: 2.5877 - val_accuracy: 0.8452 - 154s/epoch - 327ms/step\n",
      "\n",
      "Epoch 2/2\n",
      "472/472 - 210s - loss: 2.5739 - accuracy: 0.8587 - val_loss: 2.5499 - val_accuracy: 0.8541 - 210s/epoch - 444ms/step\n",
      "\n",
      "0.8540867567062378\n",
      "2\n",
      "FINISHED THE HYPERPARAMTER TUNING\n",
      "0.7867877086003622\n",
      "THE RESULTS ARE:\n",
      "{'acc': 0.7867877086003622, 'epochs': '[1, 2, 2]', 'status': 'ok', 'space': (0.0001, 0.6, 0.9, 0.95, 0.01, 0.0005, 256, 'tanh', 1024, 512)}\n",
      "  0%|          | 0/1 [34:47<?, ?it/s, best loss: ?]\n",
      "Starting from scratch: nesw trials.\n",
      "FINISHED FMIN\n",
      "best:\n",
      "local variable 'best' referenced before assignment\n",
      "Optimizing New Model\n",
      "Attempt to resume a past training if it exists:\n",
      "Found saved Trials! Loading...\n",
      "Rerunning from 0 trials to add another one.\n",
      "  0%|          | 0/1 [00:00<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with the following hyperparameters:\n",
      "(1e-06, 0.5, 0.97, 0.97, 0.0001, 0.0005, 128, 'tanh', 512, 0)\n",
      "Model: \"model_39\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_39 (TFRoberta  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " Model)                         thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_79 (S  (None, 53, 768)     0           ['tf_roberta_model_39[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_1675 (Dropout)         (None, 53, 768)      0           ['tf.__operators__.getitem_79[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_78 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_1675[0][0]']           \n",
      " l)                                                                                               \n",
      " dropout_1676 (Dropout)         (None, 53, 1024)     0           ['bidirectional_78[0][0]']       \n",
      " tf.__operators__.getitem_78 (S  (None, 768)         0           ['tf_roberta_model_39[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_79 (Bidirectiona  (None, 1024)        16785408    ['dropout_1676[0][0]']           \n",
      " l)                                                                                               \n",
      " tf.concat_39 (TFOpLambda)      (None, 1792)         0           ['tf.__operators__.getitem_78[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_79[0][0]']       \n",
      " dropout_1677 (Dropout)         (None, 1792)         0           ['tf.concat_39[0][0]']           \n",
      " dense_117 (Dense)              (None, 512)          918016      ['dropout_1677[0][0]']           \n",
      " dropout_1678 (Dropout)         (None, 512)          0           ['dense_117[0][0]']              \n",
      " dense_118 (Dense)              (None, 1)            513         ['dropout_1678[0][0]']           \n",
      "==================================================================================================\n",
      "Total params: 157,037,825\n",
      "Trainable params: 157,037,825\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "  0%|          | 0/1 [00:52<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_39/roberta/pooler/dense/kernel:0', 'tf_roberta_model_39/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_39/roberta/pooler/dense/kernel:0', 'tf_roberta_model_39/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_39/roberta/pooler/dense/kernel:0', 'tf_roberta_model_39/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_39/roberta/pooler/dense/kernel:0', 'tf_roberta_model_39/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "944/944 - 366s - loss: 29.4535 - accuracy: 0.6932 - val_loss: 28.5152 - val_accuracy: 0.8402 - 366s/epoch - 388ms/step\n",
      "\n",
      "Model: \"model_39\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_39 (TFRoberta  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " Model)                         thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_79 (S  (None, 53, 768)     0           ['tf_roberta_model_39[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_1675 (Dropout)         (None, 53, 768)      0           ['tf.__operators__.getitem_79[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_78 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_1675[0][0]']           \n",
      " l)                                                                                               \n",
      " dropout_1676 (Dropout)         (None, 53, 1024)     0           ['bidirectional_78[0][0]']       \n",
      " tf.__operators__.getitem_78 (S  (None, 768)         0           ['tf_roberta_model_39[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_79 (Bidirectiona  (None, 1024)        16785408    ['dropout_1676[0][0]']           \n",
      " l)                                                                                               \n",
      " tf.concat_39 (TFOpLambda)      (None, 1792)         0           ['tf.__operators__.getitem_78[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_79[0][0]']       \n",
      " dropout_1677 (Dropout)         (None, 1792)         0           ['tf.concat_39[0][0]']           \n",
      " dense_117 (Dense)              (None, 512)          918016      ['dropout_1677[0][0]']           \n",
      " dropout_1678 (Dropout)         (None, 512)          0           ['dense_117[0][0]']              \n",
      " dense_118 (Dense)              (None, 1)            513         ['dropout_1678[0][0]']           \n",
      "==================================================================================================\n",
      "Total params: 157,037,825\n",
      "Trainable params: 32,392,193\n",
      "Non-trainable params: 124,645,632\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "944/944 - 243s - loss: 27.7959 - accuracy: 0.8312 - val_loss: 27.0127 - val_accuracy: 0.8550 - 243s/epoch - 257ms/step\n",
      "\n",
      "Epoch 2/2\n",
      "944/944 - 298s - loss: 26.3174 - accuracy: 0.8438 - val_loss: 25.5752 - val_accuracy: 0.8595 - 298s/epoch - 316ms/step\n",
      "\n",
      "0.8594993352890015\n",
      "2\n",
      "  0%|          | 0/1 [16:01<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with the following hyperparameters:\n",
      "(1e-06, 0.5, 0.97, 0.97, 0.0001, 0.0005, 128, 'tanh', 512, 0)\n",
      "Model: \"model_40\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_40 (TFRoberta  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " Model)                         thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_81 (S  (None, 53, 768)     0           ['tf_roberta_model_40[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_1716 (Dropout)         (None, 53, 768)      0           ['tf.__operators__.getitem_81[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_80 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_1716[0][0]']           \n",
      " l)                                                                                               \n",
      " dropout_1717 (Dropout)         (None, 53, 1024)     0           ['bidirectional_80[0][0]']       \n",
      " tf.__operators__.getitem_80 (S  (None, 768)         0           ['tf_roberta_model_40[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_81 (Bidirectiona  (None, 1024)        16785408    ['dropout_1717[0][0]']           \n",
      " l)                                                                                               \n",
      " tf.concat_40 (TFOpLambda)      (None, 1792)         0           ['tf.__operators__.getitem_80[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_81[0][0]']       \n",
      " dropout_1718 (Dropout)         (None, 1792)         0           ['tf.concat_40[0][0]']           \n",
      " dense_119 (Dense)              (None, 512)          918016      ['dropout_1718[0][0]']           \n",
      " dropout_1719 (Dropout)         (None, 512)          0           ['dense_119[0][0]']              \n",
      " dense_120 (Dense)              (None, 1)            513         ['dropout_1719[0][0]']           \n",
      "==================================================================================================\n",
      "Total params: 157,037,825\n",
      "Trainable params: 157,037,825\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "  0%|          | 0/1 [16:48<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_40/roberta/pooler/dense/kernel:0', 'tf_roberta_model_40/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_40/roberta/pooler/dense/kernel:0', 'tf_roberta_model_40/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_40/roberta/pooler/dense/kernel:0', 'tf_roberta_model_40/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_40/roberta/pooler/dense/kernel:0', 'tf_roberta_model_40/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "944/944 - 371s - loss: 29.4458 - accuracy: 0.7024 - val_loss: 28.5331 - val_accuracy: 0.8362 - 371s/epoch - 393ms/step\n",
      "\n",
      "Model: \"model_40\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_40 (TFRoberta  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " Model)                         thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_81 (S  (None, 53, 768)     0           ['tf_roberta_model_40[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_1716 (Dropout)         (None, 53, 768)      0           ['tf.__operators__.getitem_81[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_80 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_1716[0][0]']           \n",
      " l)                                                                                               \n",
      " dropout_1717 (Dropout)         (None, 53, 1024)     0           ['bidirectional_80[0][0]']       \n",
      " tf.__operators__.getitem_80 (S  (None, 768)         0           ['tf_roberta_model_40[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_81 (Bidirectiona  (None, 1024)        16785408    ['dropout_1717[0][0]']           \n",
      " l)                                                                                               \n",
      " tf.concat_40 (TFOpLambda)      (None, 1792)         0           ['tf.__operators__.getitem_80[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_81[0][0]']       \n",
      " dropout_1718 (Dropout)         (None, 1792)         0           ['tf.concat_40[0][0]']           \n",
      " dense_119 (Dense)              (None, 512)          918016      ['dropout_1718[0][0]']           \n",
      " dropout_1719 (Dropout)         (None, 512)          0           ['dense_119[0][0]']              \n",
      " dense_120 (Dense)              (None, 1)            513         ['dropout_1719[0][0]']           \n",
      "==================================================================================================\n",
      "Total params: 157,037,825\n",
      "Trainable params: 32,392,193\n",
      "Non-trainable params: 124,645,632\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "944/944 - 243s - loss: 27.8099 - accuracy: 0.8310 - val_loss: 27.0432 - val_accuracy: 0.8492 - 243s/epoch - 257ms/step\n",
      "\n",
      "Epoch 2/2\n",
      "944/944 - 300s - loss: 26.3465 - accuracy: 0.8433 - val_loss: 25.6173 - val_accuracy: 0.8541 - 300s/epoch - 317ms/step\n",
      "\n",
      "0.8540867567062378\n",
      "2\n",
      "  0%|          | 0/1 [32:05<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with the following hyperparameters:\n",
      "(1e-06, 0.5, 0.97, 0.97, 0.0001, 0.0005, 128, 'tanh', 512, 0)\n",
      "Model: \"model_41\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_41 (TFRoberta  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " Model)                         thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_83 (S  (None, 53, 768)     0           ['tf_roberta_model_41[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_1757 (Dropout)         (None, 53, 768)      0           ['tf.__operators__.getitem_83[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_82 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_1757[0][0]']           \n",
      " l)                                                                                               \n",
      " dropout_1758 (Dropout)         (None, 53, 1024)     0           ['bidirectional_82[0][0]']       \n",
      " tf.__operators__.getitem_82 (S  (None, 768)         0           ['tf_roberta_model_41[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_83 (Bidirectiona  (None, 1024)        16785408    ['dropout_1758[0][0]']           \n",
      " l)                                                                                               \n",
      " tf.concat_41 (TFOpLambda)      (None, 1792)         0           ['tf.__operators__.getitem_82[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_83[0][0]']       \n",
      " dropout_1759 (Dropout)         (None, 1792)         0           ['tf.concat_41[0][0]']           \n",
      " dense_121 (Dense)              (None, 512)          918016      ['dropout_1759[0][0]']           \n",
      " dropout_1760 (Dropout)         (None, 512)          0           ['dense_121[0][0]']              \n",
      " dense_122 (Dense)              (None, 1)            513         ['dropout_1760[0][0]']           \n",
      "==================================================================================================\n",
      "Total params: 157,037,825\n",
      "Trainable params: 157,037,825\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "  0%|          | 0/1 [32:56<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_41/roberta/pooler/dense/kernel:0', 'tf_roberta_model_41/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_41/roberta/pooler/dense/kernel:0', 'tf_roberta_model_41/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_41/roberta/pooler/dense/kernel:0', 'tf_roberta_model_41/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_41/roberta/pooler/dense/kernel:0', 'tf_roberta_model_41/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "944/944 - 373s - loss: 29.4829 - accuracy: 0.6818 - val_loss: 28.5353 - val_accuracy: 0.8362 - 373s/epoch - 395ms/step\n",
      "\n",
      "Model: \"model_41\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_41 (TFRoberta  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " Model)                         thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_83 (S  (None, 53, 768)     0           ['tf_roberta_model_41[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_1757 (Dropout)         (None, 53, 768)      0           ['tf.__operators__.getitem_83[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_82 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_1757[0][0]']           \n",
      " l)                                                                                               \n",
      " dropout_1758 (Dropout)         (None, 53, 1024)     0           ['bidirectional_82[0][0]']       \n",
      " tf.__operators__.getitem_82 (S  (None, 768)         0           ['tf_roberta_model_41[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_83 (Bidirectiona  (None, 1024)        16785408    ['dropout_1758[0][0]']           \n",
      " l)                                                                                               \n",
      " tf.concat_41 (TFOpLambda)      (None, 1792)         0           ['tf.__operators__.getitem_82[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_83[0][0]']       \n",
      " dropout_1759 (Dropout)         (None, 1792)         0           ['tf.concat_41[0][0]']           \n",
      " dense_121 (Dense)              (None, 512)          918016      ['dropout_1759[0][0]']           \n",
      " dropout_1760 (Dropout)         (None, 512)          0           ['dense_121[0][0]']              \n",
      " dense_122 (Dense)              (None, 1)            513         ['dropout_1760[0][0]']           \n",
      "==================================================================================================\n",
      "Total params: 157,037,825\n",
      "Trainable params: 32,392,193\n",
      "Non-trainable params: 124,645,632\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "944/944 - 244s - loss: 27.8176 - accuracy: 0.8281 - val_loss: 27.0489 - val_accuracy: 0.8451 - 244s/epoch - 258ms/step\n",
      "\n",
      "Epoch 2/2\n",
      "944/944 - 299s - loss: 26.3467 - accuracy: 0.8408 - val_loss: 25.6044 - val_accuracy: 0.8553 - 299s/epoch - 316ms/step\n",
      "\n",
      "0.8553441762924194\n",
      "2\n",
      "FINISHED THE HYPERPARAMTER TUNING\n",
      "0.8563100894292196\n",
      "THE RESULTS ARE:\n",
      "{'acc': 0.8563100894292196, 'epochs': '[2, 2, 2]', 'status': 'ok', 'space': (1e-06, 0.5, 0.97, 0.97, 0.0001, 0.0005, 128, 'tanh', 512, 0)}\n",
      "  0%|          | 0/1 [48:14<?, ?it/s, best loss: ?]\n",
      "Starting from scratch: nesw trials.\n",
      "FINISHED FMIN\n",
      "best:\n",
      "local variable 'best' referenced before assignment\n",
      "Optimizing New Model\n",
      "Attempt to resume a past training if it exists:\n",
      "Found saved Trials! Loading...\n",
      "Rerunning from 0 trials to add another one.\n",
      "  0%|          | 0/1 [00:00<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with the following hyperparameters:\n",
      "(1e-05, 0.6, 0.9, 0.9, 0, 0.005, 64, 'tanh', 512, 256)\n",
      "Model: \"model_42\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_42 (TFRoberta  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " Model)                         thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_85 (S  (None, 53, 768)     0           ['tf_roberta_model_42[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_1798 (Dropout)         (None, 53, 768)      0           ['tf.__operators__.getitem_85[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_84 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_1798[0][0]']           \n",
      " l)                                                                                               \n",
      " dropout_1799 (Dropout)         (None, 53, 1024)     0           ['bidirectional_84[0][0]']       \n",
      " tf.__operators__.getitem_84 (S  (None, 768)         0           ['tf_roberta_model_42[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_85 (Bidirectiona  (None, 1024)        16785408    ['dropout_1799[0][0]']           \n",
      " l)                                                                                               \n",
      " tf.concat_42 (TFOpLambda)      (None, 1792)         0           ['tf.__operators__.getitem_84[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_85[0][0]']       \n",
      " dropout_1800 (Dropout)         (None, 1792)         0           ['tf.concat_42[0][0]']           \n",
      " dense_123 (Dense)              (None, 512)          918016      ['dropout_1800[0][0]']           \n",
      " dropout_1801 (Dropout)         (None, 512)          0           ['dense_123[0][0]']              \n",
      " dense_124 (Dense)              (None, 256)          131328      ['dropout_1801[0][0]']           \n",
      " dropout_1802 (Dropout)         (None, 256)          0           ['dense_124[0][0]']              \n",
      " dense_125 (Dense)              (None, 1)            257         ['dropout_1802[0][0]']           \n",
      "==================================================================================================\n",
      "Total params: 157,168,897\n",
      "Trainable params: 157,168,897\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "  0%|          | 0/1 [00:46<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_42/roberta/pooler/dense/kernel:0', 'tf_roberta_model_42/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_42/roberta/pooler/dense/kernel:0', 'tf_roberta_model_42/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_42/roberta/pooler/dense/kernel:0', 'tf_roberta_model_42/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_42/roberta/pooler/dense/kernel:0', 'tf_roberta_model_42/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1888/1888 - 556s - loss: 19.9864 - accuracy: 0.8164 - val_loss: 9.1086 - val_accuracy: 0.8640 - 556s/epoch - 294ms/step\n",
      "\n",
      "Model: \"model_42\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_42 (TFRoberta  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " Model)                         thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_85 (S  (None, 53, 768)     0           ['tf_roberta_model_42[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_1798 (Dropout)         (None, 53, 768)      0           ['tf.__operators__.getitem_85[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_84 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_1798[0][0]']           \n",
      " l)                                                                                               \n",
      " dropout_1799 (Dropout)         (None, 53, 1024)     0           ['bidirectional_84[0][0]']       \n",
      " tf.__operators__.getitem_84 (S  (None, 768)         0           ['tf_roberta_model_42[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_85 (Bidirectiona  (None, 1024)        16785408    ['dropout_1799[0][0]']           \n",
      " l)                                                                                               \n",
      " tf.concat_42 (TFOpLambda)      (None, 1792)         0           ['tf.__operators__.getitem_84[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_85[0][0]']       \n",
      " dropout_1800 (Dropout)         (None, 1792)         0           ['tf.concat_42[0][0]']           \n",
      " dense_123 (Dense)              (None, 512)          918016      ['dropout_1800[0][0]']           \n",
      " dropout_1801 (Dropout)         (None, 512)          0           ['dense_123[0][0]']              \n",
      " dense_124 (Dense)              (None, 256)          131328      ['dropout_1801[0][0]']           \n",
      " dropout_1802 (Dropout)         (None, 256)          0           ['dense_124[0][0]']              \n",
      " dense_125 (Dense)              (None, 1)            257         ['dropout_1802[0][0]']           \n",
      "==================================================================================================\n",
      "Total params: 157,168,897\n",
      "Trainable params: 32,523,265\n",
      "Non-trainable params: 124,645,632\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "1888/1888 - 432s - loss: 5.0131 - accuracy: 0.8676 - val_loss: 2.7731 - val_accuracy: 0.8686 - 432s/epoch - 229ms/step\n",
      "\n",
      "Epoch 2/2\n",
      "1888/1888 - 487s - loss: 2.1279 - accuracy: 0.8860 - val_loss: 1.7547 - val_accuracy: 0.8744 - 487s/epoch - 258ms/step\n",
      "\n",
      "0.874439537525177\n",
      "2\n",
      "  0%|          | 0/1 [25:27<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with the following hyperparameters:\n",
      "(1e-05, 0.6, 0.9, 0.9, 0, 0.005, 64, 'tanh', 512, 256)\n",
      "Model: \"model_43\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_43 (TFRoberta  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " Model)                         thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_87 (S  (None, 53, 768)     0           ['tf_roberta_model_43[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_1840 (Dropout)         (None, 53, 768)      0           ['tf.__operators__.getitem_87[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_86 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_1840[0][0]']           \n",
      " l)                                                                                               \n",
      " dropout_1841 (Dropout)         (None, 53, 1024)     0           ['bidirectional_86[0][0]']       \n",
      " tf.__operators__.getitem_86 (S  (None, 768)         0           ['tf_roberta_model_43[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_87 (Bidirectiona  (None, 1024)        16785408    ['dropout_1841[0][0]']           \n",
      " l)                                                                                               \n",
      " tf.concat_43 (TFOpLambda)      (None, 1792)         0           ['tf.__operators__.getitem_86[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_87[0][0]']       \n",
      " dropout_1842 (Dropout)         (None, 1792)         0           ['tf.concat_43[0][0]']           \n",
      " dense_126 (Dense)              (None, 512)          918016      ['dropout_1842[0][0]']           \n",
      " dropout_1843 (Dropout)         (None, 512)          0           ['dense_126[0][0]']              \n",
      " dense_127 (Dense)              (None, 256)          131328      ['dropout_1843[0][0]']           \n",
      " dropout_1844 (Dropout)         (None, 256)          0           ['dense_127[0][0]']              \n",
      " dense_128 (Dense)              (None, 1)            257         ['dropout_1844[0][0]']           \n",
      "==================================================================================================\n",
      "Total params: 157,168,897\n",
      "Trainable params: 157,168,897\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "  0%|          | 0/1 [26:14<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_43/roberta/pooler/dense/kernel:0', 'tf_roberta_model_43/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_43/roberta/pooler/dense/kernel:0', 'tf_roberta_model_43/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_43/roberta/pooler/dense/kernel:0', 'tf_roberta_model_43/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_43/roberta/pooler/dense/kernel:0', 'tf_roberta_model_43/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 55) dtype=int64>, <tf.Tensor 'cond/Identity_16:0' shape=(None, 1) dtype=int64>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1888/1888 - 559s - loss: 20.2771 - accuracy: 0.8145 - val_loss: 9.5727 - val_accuracy: 0.8611 - 559s/epoch - 296ms/step\n",
      "\n",
      "Model: \"model_43\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 55)]         0           []                               \n",
      " input_attention (InputLayer)   [(None, 55)]         0           []                               \n",
      " tf_roberta_model_43 (TFRoberta  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " Model)                         thPoolingAndCrossAt               'input_attention[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 55,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      " tf.__operators__.getitem_87 (S  (None, 53, 768)     0           ['tf_roberta_model_43[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " dropout_1840 (Dropout)         (None, 53, 768)      0           ['tf.__operators__.getitem_87[0][\n",
      "                                                                 0]']                             \n",
      " bidirectional_86 (Bidirectiona  (None, 53, 1024)    14688256    ['dropout_1840[0][0]']           \n",
      " l)                                                                                               \n",
      " dropout_1841 (Dropout)         (None, 53, 1024)     0           ['bidirectional_86[0][0]']       \n",
      " tf.__operators__.getitem_86 (S  (None, 768)         0           ['tf_roberta_model_43[0][0]']    \n",
      " licingOpLambda)                                                                                  \n",
      " bidirectional_87 (Bidirectiona  (None, 1024)        16785408    ['dropout_1841[0][0]']           \n",
      " l)                                                                                               \n",
      " tf.concat_43 (TFOpLambda)      (None, 1792)         0           ['tf.__operators__.getitem_86[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'bidirectional_87[0][0]']       \n",
      " dropout_1842 (Dropout)         (None, 1792)         0           ['tf.concat_43[0][0]']           \n",
      " dense_126 (Dense)              (None, 512)          918016      ['dropout_1842[0][0]']           \n",
      " dropout_1843 (Dropout)         (None, 512)          0           ['dense_126[0][0]']              \n",
      " dense_127 (Dense)              (None, 256)          131328      ['dropout_1843[0][0]']           \n",
      " dropout_1844 (Dropout)         (None, 256)          0           ['dense_127[0][0]']              \n",
      " dense_128 (Dense)              (None, 1)            257         ['dropout_1844[0][0]']           \n",
      "==================================================================================================\n",
      "Total params: 157,168,897\n",
      "Trainable params: 32,523,265\n",
      "Non-trainable params: 124,645,632\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "1888/1888 - 431s - loss: 5.2344 - accuracy: 0.8701 - val_loss: 2.8254 - val_accuracy: 0.8733 - 431s/epoch - 228ms/step\n",
      "\n",
      "Epoch 2/2\n",
      "  0%|          | 0/1 [42:47<?, ?it/s, best loss: ?]"
     ]
    }
   ],
   "source": [
    "eval_num = 0\n",
    "best_acc = None\n",
    "best_hyperparams = None\n",
    "\n",
    "# space to be searched \n",
    "parameter_space = [\n",
    "                    hp.choice('learning_rate',[5e-5, 5e-4, 5e-3, 5e-6, 1e-5, 1e-4, 1e-3, 1e-6]),\n",
    "                    hp.choice('drop_rate', [0.3, 0.4, 0.5, 0.6]),\n",
    "                    hp.choice('beta1',    [0.9, 0.95, 0.97, 0.99]),\n",
    "                    hp.choice('beta2',    [ 0.9, 0.95, 0.97, 0.99]),\n",
    "                    hp.choice('l2',    [0.0001, 0.0005, 0.001, 0.005, 0.01, 0]),\n",
    "                    hp.choice('l1',    [0.0001, 0.0005, 0.001, 0.005, 0.01, 0]),\n",
    "                    hp.choice('batchsize',    [ 128, 256, 64]),\n",
    "                    hp.choice('activation',   ['relu', 'tanh', 'sigmoid']),\n",
    "                    hp.choice('layer1_nodes',  [1024, 512, 256]),\n",
    "                    hp.choice('layer2_nodes',  [512, 256, 128, 64, 0]),\n",
    "                  ]\n",
    "\n",
    "def f(hyperparams):\n",
    "    \"\"\"\"\"\"\"\"\"\n",
    "    Function the TPE algorithm aims to minimize\n",
    "\n",
    "    Input:\n",
    "        - hyperparams:  set of hyperparameters to be used\n",
    "    Output:\n",
    "        - result:       dictionary of results \n",
    "    \"\"\"\"\"\"\"\"\"\n",
    "    \n",
    "    global eval_num\n",
    "    global best_acc\n",
    "    global best_hyperparams\n",
    "    \n",
    "    # run cross-validation\n",
    "    acc, epochs = hyperopt_cross_val(hyperparams)\n",
    "    print('FINISHED THE HYPERPARAMTER TUNING')\n",
    "    print(acc)\n",
    "    \n",
    "    # save globally optimal accuracy\n",
    "    if best_acc is None or acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_hyperparams = hyperparams\n",
    "    \n",
    "    # save results \n",
    "    result = {'acc': -acc, 'epochs': str(epochs),  'status': STATUS_OK, 'space': hyperparams}\n",
    "    print(\"THE RESULTS ARE:\")\n",
    "    print(result)\n",
    "    save_json_result(str(acc), result)\n",
    "    return result\n",
    "\n",
    "def run_a_trial():\n",
    "    \"\"\"\"\"\"\"\"\"\n",
    "    Function that runs one iteration of the TPE algorithm\n",
    "    \"\"\"\"\"\"\"\"\"\n",
    "    \n",
    "    max_evals = nb_evals = 1\n",
    "\n",
    "    print(\"Attempt to resume a past training if it exists:\")\n",
    "    try:\n",
    "        # https://github.com/hyperopt/hyperopt/issues/267\n",
    "        with open('results.pkl', 'rb') as in_file:\n",
    "              trials = pickle.load(in_file)\n",
    "        # trials = pickle.load(open(\"results.pkl\", \"rb\"))\n",
    "        print(\"Found saved Trials! Loading...\")\n",
    "        max_evals = len(trials.trials) + nb_evals\n",
    "        print(\"Rerunning from {} trials to add another one.\".format(len(trials.trials)))\n",
    "        best = fmin(f, parameter_space, algo=tpe.suggest, max_evals=max_evals, trials=trials)\n",
    "    except:\n",
    "        trials = Trials()\n",
    "        print(\"Starting from scratch: nesw trials.\")\n",
    "\n",
    "    print(\"FINISHED FMIN\")\n",
    "    with open('results.pkl', 'wb') as out_file:\n",
    "        pickle.dump(trials, out_file)\n",
    "    print('best:')\n",
    "    print(best)\n",
    "      \n",
    "while True:\n",
    "    print(\"Optimizing New Model\")\n",
    "    try:\n",
    "        run_a_trial()\n",
    "    except Exception as err:\n",
    "        err_str = str(err)\n",
    "        print(err_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a1DDtzsjqPUS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of cross-validation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
