{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification, InputExample, InputFeatures, pipeline\n",
    "from transformers import TFDistilBertModel, DistilBertTokenizer, TFAutoModel, AutoTokenizer, RobertaTokenizer, TFRobertaModel, DistilBertTokenizerFast\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from helpers import get_sent_embe\n",
    "from helpers import loadData"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load datasets\n",
    "First, we use the loadData function to load the training observations into pandas DataFrames"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "POS_TRAINING_PATH = 'twitter-datasets/train_pos.txt'\n",
    "NEG_TRAINING_PATH = 'twitter-datasets/train_neg.txt'\n",
    "POS_TRAINING_PATH_FULL = 'twitter-datasets/train_pos_full.txt'\n",
    "NEG_TRAINING_PATH_FULL = 'twitter-datasets/train_neg_full.txt'\n",
    "\n",
    "TEST_PATH = 'twitter-datasets/test_data.txt'\n",
    "load_features_distilBERT = False\n",
    "use_distilBERT = True\n",
    "load_features_BERTweet = False\n",
    "use_BERTweet = False\n",
    "load_features_RoBERTa = False\n",
    "use_RoBERTa = False\n",
    "\n",
    "# Load both small and large training sets\n",
    "# df_train_small = loadData(POS_TRAINING_PATH, NEG_TRAINING_PATH)\n",
    "df_full = loadData(POS_TRAINING_PATH_FULL, NEG_TRAINING_PATH_FULL)\n",
    "# df_full.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since for the test data we need to save the indexing separately and we shoud not dropping possible duplicates, we do not use the loadData function."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "   index                                              tweet\n0      1  sea doo pro sea scooter ( sports with the port...\n1      2  <user> shucks well i work all week so now i ca...\n2      3          i cant stay away from bug thats my baby\\n\n3      4  <user> no ma'am ! ! ! lol im perfectly fine an...\n4      5  whenever i fall asleep watching the tv , i alw...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>tweet</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>sea doo pro sea scooter ( sports with the port...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>&lt;user&gt; shucks well i work all week so now i ca...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>i cant stay away from bug thats my baby\\n</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>&lt;user&gt; no ma'am ! ! ! lol im perfectly fine an...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>whenever i fall asleep watching the tv , i alw...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the test set.\n",
    "test_sent = []\n",
    "idx = []\n",
    "with open(TEST_PATH) as test:\n",
    "    for line in test:\n",
    "        split = line.split(\",\", 1)\n",
    "        idx.append(int(split[0]))\n",
    "        test_sent.append(split[1])\n",
    "\n",
    "data = {'index':idx,'tweet':test_sent}\n",
    "df_test = pd.DataFrame(data)\n",
    "df_test.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check if GPU is available, in which case we will use this to improve the runnning time"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/device:GPU:0']\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "print(get_available_gpus())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_built_with_cuda()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### DistilBERT"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "if use_distilBERT==True:\n",
    "    if load_features_distilBERT==True:\n",
    "        # initialize DistilBERT tokenizer and model\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "        model = TFDistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "        # obtain sentence representations\n",
    "        features_np_full = get_sent_embe(df_full, tokenizer, model)\n",
    "        features_test_np = get_sent_embe(df_test, tokenizer, model)\n",
    "\n",
    "        # transform features into Pandas DataFrame\n",
    "        distilBERT_pd_full = pd.DataFrame(data=features_np_full)\n",
    "        distilBERT_pd_test = pd.DataFrame(data=features_test_np)\n",
    "\n",
    "        # Save the features as a pickle file to prevent running the feature extraction every time\n",
    "        distilBERT_pd_test.to_pickle(\"./DistilBERT_features_test.pkl\", compression='zip')\n",
    "        distilBERT_pd_full.to_pickle(\"./DistilBERT_features_full.pkl\", compression='zip')\n",
    "    else:\n",
    "        # Load the pickle file with the sentence representations\n",
    "        features_np_full = pd.read_pickle(\"./DistilBERT_features_full.pkl\", compression='zip').to_numpy()\n",
    "        # features_np_small = pd.read_pickle(\"./DistilBERT_features.pkl\", compression='zip').to_numpy()\n",
    "        # features_test_np = pd.read_pickle(\"./DistilBERT_features_test.pkl\", compression='zip').to_numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### RoBERTa pretrained features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2270482, 137)\n",
      "iteration:  0\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Exception encountered when calling layer \"intermediate\" (type TFRobertaIntermediate).\n\nfailed to allocate memory [Op:RealDiv]\n\nCall arguments received:\n  â€¢ hidden_states=tf.Tensor(shape=(200, 137, 768), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mResourceExhaustedError\u001B[0m                    Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_98532/840701984.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m         \u001B[1;31m# obtain sentence representations\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 8\u001B[1;33m         \u001B[0mfeatures_np_full\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mget_sent_embe\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdf_full\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtokenizer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      9\u001B[0m         \u001B[0mfeatures_test_np\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mget_sent_embe\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdf_test\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtokenizer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\pythonProject1\\helpers.py\u001B[0m in \u001B[0;36mget_sent_embe\u001B[1;34m(df, tokenizer, model)\u001B[0m\n\u001B[0;32m     89\u001B[0m             \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"iteration: \"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcount\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     90\u001B[0m             \u001B[0mcount\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 91\u001B[1;33m             \u001B[0mlast_hidden_layer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0melement\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mattention_mask\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0miterator\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_next\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m:\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnumpy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     92\u001B[0m             \u001B[0mfeatures\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlast_hidden_layer\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     93\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\lucaz\\pycharmprojects\\pythonproject1\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001B[0m in \u001B[0;36merror_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     65\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m  \u001B[1;31m# pylint: disable=broad-except\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     66\u001B[0m       \u001B[0mfiltered_tb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_process_traceback_frames\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 67\u001B[1;33m       \u001B[1;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwith_traceback\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfiltered_tb\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     68\u001B[0m     \u001B[1;32mfinally\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     69\u001B[0m       \u001B[1;32mdel\u001B[0m \u001B[0mfiltered_tb\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\lucaz\\pycharmprojects\\pythonproject1\\venv\\lib\\site-packages\\transformers\\models\\roberta\\modeling_tf_roberta.py\u001B[0m in \u001B[0;36mcall\u001B[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, training, **kwargs)\u001B[0m\n\u001B[0;32m    744\u001B[0m             \u001B[0mkwargs_call\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    745\u001B[0m         )\n\u001B[1;32m--> 746\u001B[1;33m         outputs = self.roberta(\n\u001B[0m\u001B[0;32m    747\u001B[0m             \u001B[0minput_ids\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"input_ids\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    748\u001B[0m             \u001B[0mattention_mask\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"attention_mask\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\lucaz\\pycharmprojects\\pythonproject1\\venv\\lib\\site-packages\\transformers\\models\\roberta\\modeling_tf_roberta.py\u001B[0m in \u001B[0;36mcall\u001B[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, training, **kwargs)\u001B[0m\n\u001B[0;32m    556\u001B[0m             \u001B[0minputs\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"head_mask\"\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconfig\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnum_hidden_layers\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    557\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 558\u001B[1;33m         encoder_outputs = self.encoder(\n\u001B[0m\u001B[0;32m    559\u001B[0m             \u001B[0mhidden_states\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0membedding_output\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    560\u001B[0m             \u001B[0mattention_mask\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mextended_attention_mask\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\lucaz\\pycharmprojects\\pythonproject1\\venv\\lib\\site-packages\\transformers\\models\\roberta\\modeling_tf_roberta.py\u001B[0m in \u001B[0;36mcall\u001B[1;34m(self, hidden_states, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict, training)\u001B[0m\n\u001B[0;32m    416\u001B[0m                 \u001B[0mall_hidden_states\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mall_hidden_states\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mhidden_states\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    417\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 418\u001B[1;33m             layer_outputs = layer_module(\n\u001B[0m\u001B[0;32m    419\u001B[0m                 \u001B[0mhidden_states\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mhidden_states\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    420\u001B[0m                 \u001B[0mattention_mask\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mattention_mask\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\lucaz\\pycharmprojects\\pythonproject1\\venv\\lib\\site-packages\\transformers\\models\\roberta\\modeling_tf_roberta.py\u001B[0m in \u001B[0;36mcall\u001B[1;34m(self, hidden_states, attention_mask, head_mask, output_attentions, training)\u001B[0m\n\u001B[0;32m    383\u001B[0m         )\n\u001B[0;32m    384\u001B[0m         \u001B[0mattention_output\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mattention_outputs\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 385\u001B[1;33m         \u001B[0mintermediate_output\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mintermediate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mhidden_states\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mattention_output\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    386\u001B[0m         layer_output = self.bert_output(\n\u001B[0;32m    387\u001B[0m             \u001B[0mhidden_states\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mintermediate_output\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput_tensor\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mattention_output\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtraining\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtraining\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\lucaz\\pycharmprojects\\pythonproject1\\venv\\lib\\site-packages\\transformers\\models\\roberta\\modeling_tf_roberta.py\u001B[0m in \u001B[0;36mcall\u001B[1;34m(self, hidden_states)\u001B[0m\n\u001B[0;32m    334\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mcall\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mhidden_states\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTensor\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTensor\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    335\u001B[0m         \u001B[0mhidden_states\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdense\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mhidden_states\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 336\u001B[1;33m         \u001B[0mhidden_states\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mintermediate_act_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mhidden_states\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    337\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    338\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mhidden_states\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mResourceExhaustedError\u001B[0m: Exception encountered when calling layer \"intermediate\" (type TFRobertaIntermediate).\n\nfailed to allocate memory [Op:RealDiv]\n\nCall arguments received:\n  â€¢ hidden_states=tf.Tensor(shape=(200, 137, 768), dtype=float32)"
     ]
    }
   ],
   "source": [
    "if use_RoBERTa==True:\n",
    "    if load_features_RoBERTa==True:\n",
    "        # initialize DistilBERT tokenizer and model\n",
    "        tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "        model = TFRobertaModel.from_pretrained('roberta-base')\n",
    "\n",
    "        # obtain sentence representations\n",
    "        features_np_full = get_sent_embe(df_full, tokenizer, model)\n",
    "        features_test_np = get_sent_embe(df_test, tokenizer, model)\n",
    "\n",
    "        # transform features into Pandas DataFrame\n",
    "        RoBERTa_pd_full = pd.DataFrame(data=features_np_full)\n",
    "        RoBERTa_pd_test = pd.DataFrame(data=features_test_np)\n",
    "\n",
    "        # Save the features as a pickle file to prevent running the feature extraction every time\n",
    "        RoBERTa_pd_test.to_pickle(\"./RoBERTa_features_test.pkl\", compression='zip')\n",
    "        RoBERTa_pd_full.to_pickle(\"./RoBERTa_features_full.pkl\", compression='zip')\n",
    "    else:\n",
    "        # Load the pickle file with the sentence representations\n",
    "        features_np_full = pd.read_pickle(\"./RoBERTa_features_full.pkl\", compression='zip').to_numpy()\n",
    "        # features_np_small = pd.read_pickle(\"./DistilBERT_features.pkl\", compression='zip').to_numpy()\n",
    "        features_test_np = pd.read_pickle(\"./RoBERTa_features_test.pkl\", compression='zip').to_numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_projector', 'vocab_layer_norm', 'vocab_transform', 'activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 65)\n",
      "iteration:  0\n",
      "iteration:  1\n",
      "iteration:  2\n",
      "iteration:  3\n",
      "iteration:  4\n",
      "iteration:  5\n",
      "iteration:  6\n",
      "iteration:  7\n",
      "iteration:  8\n",
      "iteration:  9\n",
      "iteration:  10\n",
      "iteration:  11\n",
      "iteration:  12\n",
      "iteration:  13\n",
      "iteration:  14\n",
      "iteration:  15\n",
      "iteration:  16\n",
      "iteration:  17\n",
      "iteration:  18\n",
      "iteration:  19\n",
      "iteration:  20\n",
      "iteration:  21\n",
      "iteration:  22\n",
      "iteration:  23\n",
      "iteration:  24\n",
      "iteration:  25\n",
      "iteration:  26\n",
      "iteration:  27\n",
      "iteration:  28\n",
      "iteration:  29\n",
      "iteration:  30\n",
      "iteration:  31\n",
      "iteration:  32\n",
      "iteration:  33\n",
      "iteration:  34\n",
      "iteration:  35\n",
      "iteration:  36\n",
      "iteration:  37\n",
      "iteration:  38\n",
      "iteration:  39\n",
      "iteration:  40\n",
      "iteration:  41\n",
      "iteration:  42\n",
      "iteration:  43\n",
      "iteration:  44\n",
      "iteration:  45\n",
      "iteration:  46\n",
      "iteration:  47\n",
      "iteration:  48\n",
      "iteration:  49\n",
      "iteration:  50\n",
      "iteration:  51\n",
      "iteration:  52\n",
      "iteration:  53\n",
      "iteration:  54\n",
      "iteration:  55\n",
      "iteration:  56\n",
      "iteration:  57\n",
      "iteration:  58\n",
      "iteration:  59\n",
      "iteration:  60\n",
      "iteration:  61\n",
      "iteration:  62\n",
      "iteration:  63\n",
      "iteration:  64\n",
      "iteration:  65\n",
      "iteration:  66\n",
      "iteration:  67\n",
      "iteration:  68\n",
      "iteration:  69\n",
      "iteration:  70\n",
      "iteration:  71\n",
      "iteration:  72\n",
      "iteration:  73\n",
      "iteration:  74\n",
      "iteration:  75\n",
      "iteration:  76\n",
      "iteration:  77\n",
      "iteration:  78\n",
      "iteration:  79\n",
      "iteration:  80\n",
      "iteration:  81\n",
      "iteration:  82\n",
      "iteration:  83\n",
      "iteration:  84\n",
      "iteration:  85\n",
      "iteration:  86\n",
      "iteration:  87\n",
      "iteration:  88\n",
      "iteration:  89\n",
      "iteration:  90\n",
      "iteration:  91\n",
      "iteration:  92\n",
      "iteration:  93\n",
      "iteration:  94\n",
      "iteration:  95\n",
      "iteration:  96\n",
      "iteration:  97\n",
      "iteration:  98\n",
      "iteration:  99\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = TFDistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "features_test_np = get_sent_embe(df_test, tokenizer, model)\n",
    "\n",
    "# Save the features as a pickle file to prevent running the feature extraction every time\n",
    "distillBERT_test_pd = pd.DataFrame(data=features_test_np)\n",
    "distillBERT_test_pd.to_pickle(\"./DistilBERT_test_features.pkl\", compression='zip')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "labels = df_full['sentiment']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "data": {
      "text/plain": "(2270482, 768)"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_np_full.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features_np_small, df_train_small['sentiment'], train_size=0.8, random_state=123)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\lucaz\\pycharmprojects\\pythonproject1\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.8097063284158279"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression().fit(train_features, train_labels)\n",
    "clf.score(test_features, test_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "y_pred = clf.predict(features_test_np)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "data": {
      "text/plain": "array([    1,     2,     3, ...,  9998,  9999, 10000])"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.asarray(df_test.index.values.tolist()) +1\n",
    "idx"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "import csv\n",
    "def create_csv_submission(ids, y_pred, name):\n",
    "    \"\"\"\n",
    "    Creates an output file in .csv format for submission to Kaggle or AIcrowd\n",
    "    Arguments: ids (event ids associated with each prediction)\n",
    "               y_pred (predicted class labels)\n",
    "               name (string name of .csv output file to be created)\n",
    "    \"\"\"\n",
    "    with open(name, 'w') as csvfile:\n",
    "        fieldnames = ['Id', 'Prediction']\n",
    "        writer = csv.DictWriter(csvfile, delimiter=\",\", fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for r1, r2 in zip(ids, y_pred):\n",
    "            writer.writerow({'Id':int(r1),'Prediction':int(r2)})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [],
   "source": [
    "y_pred[y_pred==0] = -1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [
    {
     "data": {
      "text/plain": "array([-1, -1, -1, ..., -1,  1, -1], dtype=int64)"
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'LogisitcRegression_DistilBERTemb_full_MLP_3'\n",
    "create_csv_submission(idx, y_pred, OUTPUT_PATH)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "data": {
      "text/plain": "0.8023990468996712"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "gb_clf = HistGradientBoostingClassifier(scoring='accuracy', max_depth=50, loss='binary_crossentropy', learning_rate=1, max_iter=100, l2_regularization=0.1).fit(train_features, train_labels)\n",
    "gb_clf.score(test_features, test_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\lucaz\\pycharmprojects\\pythonproject1\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:33:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7975458430994071\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "xgb_model = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42, learning_rate=0.05, n_estimators=200)\n",
    "xgb_model.fit(train_features, train_labels)\n",
    "\n",
    "y_pred = xgb_model.predict(test_features)\n",
    "\n",
    "print(accuracy_score(test_labels, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "0.8236867503102164"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf_mlp = MLPClassifier(random_state=123, max_iter=300, solver='adam', alpha=0.0001, learning_rate='adaptive', early_stopping=True).fit(train_features, train_labels)\n",
    "clf_mlp.score(test_features, test_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\lucaz\\pycharmprojects\\pythonproject1\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "30 fits failed out of a total of 50.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "30 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\lucaz\\pycharmprojects\\pythonproject1\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 681, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\users\\lucaz\\pycharmprojects\\pythonproject1\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 752, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"c:\\users\\lucaz\\pycharmprojects\\pythonproject1\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 384, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"c:\\users\\lucaz\\pycharmprojects\\pythonproject1\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 493, in _validate_hyperparameters\n",
      "    raise ValueError(\n",
      "ValueError: The activation 'sigmoid' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\users\\lucaz\\pycharmprojects\\pythonproject1\\venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.7846223         nan 0.80312019        nan\n",
      " 0.82442841        nan        nan 0.82485639]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "RandomizedSearchCV(cv=5, estimator=MLPClassifier(max_iter=500), n_jobs=-1,\n                   param_distributions={'activation': ['sigmoid', 'relu'],\n                                        'alpha': [0.0001, 0.001, 0.05, 0.01],\n                                        'early_stopping': [True, False],\n                                        'hidden_layer_sizes': [(256, 128, 32),\n                                                               (256, 32),\n                                                               (128, 32),\n                                                               (32,)],\n                                        'learning_rate': ['constant',\n                                                          'adaptive']},\n                   scoring='f1')"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp_gs = MLPClassifier(max_iter=500)\n",
    "parameter_space = {\n",
    "    'hidden_layer_sizes': [(256,128,32), (256,32), (128, 32), (32,)],\n",
    "    'activation': ['logistic', 'relu'],\n",
    "    #'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.001, 0.05, 0.01],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "    'early_stopping': [True, False],\n",
    "}\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "clf = RandomizedSearchCV(mlp_gs, parameter_space, n_jobs=-1, cv=5, scoring='f1')\n",
    "clf.fit(features_np_small, df_train_small['sentiment'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [],
   "source": [
    "y_pred = clf_mlp.predict(features_test_np)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "clf_mlp = MLPClassifier(activation='relu', hidden_layer_sizes= (64), random_state=123, max_iter=300, solver='adam', alpha=0.001, learning_rate_init= 0.005, beta_1= 0.9, beta_2= 0.999, early_stopping=True).fit(features_np_full, df_full['sentiment'])\n",
    "\n",
    "y_pred = clf_mlp.predict(features_test_np)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "array([-1, -1, -1, ..., -1,  1, -1], dtype=int64)"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[y_pred==0] = -1\n",
    "y_pred"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "idx = df_test['index']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'MLP_DistilBERTemb_full'\n",
    "create_csv_submission(idx, y_pred, OUTPUT_PATH)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7916722638555198\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgbm\n",
    "from sklearn.metrics import accuracy_score\n",
    "lgbm_model = lgbm.LGBMClassifier(objective=\"binary\", random_state=42, learning_rate=0.05, n_estimators=200)\n",
    "lgbm_model.fit(train_features, train_labels)\n",
    "\n",
    "y_pred = lgbm_model.predict(test_features)\n",
    "\n",
    "print(accuracy_score(test_labels, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}