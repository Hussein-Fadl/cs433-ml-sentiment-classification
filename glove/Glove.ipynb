{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f6b9cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.sparse import *\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import pandas as pd\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "868ce1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loades the tweet to embed and the vacabulary from which the embeding matrix was constructed\n",
    "#data_neg = pd.read_csv('twitter-datasets/train_neg.txt',sep='[\\t]', header = None,dtype='str',error_bad_lines = False)\n",
    "#data_pos = pd.read_csv('twitter-datasets/train_pos.txt',sep='[\\t]', header = None,dtype='str',error_bad_lines = False)\n",
    "sentences_pos=[]\n",
    "with open('twitter-datasets/train_pos.txt') as pos:\n",
    "    for line in pos:\n",
    "        sentences_pos.append(line)\n",
    "data_pos = pd.DataFrame(sentences_pos)\n",
    "\n",
    "sentences_neg=[]\n",
    "with open('twitter-datasets/train_neg.txt') as neg:\n",
    "    for line in neg:\n",
    "        sentences_neg.append(line)\n",
    "data_neg = pd.DataFrame(sentences_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2051df84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   0\n",
      "0  vinco tresorpack 6 ( difficulty 10 of 10 objec...\n",
      "1  glad i dot have taks tomorrow ! ! #thankful #s...\n",
      "2  1-3 vs celtics in the regular season = were fu...\n",
      "3  <user> i could actually kill that girl i'm so ...\n",
      "4  <user> <user> <user> i find that very hard to ...\n",
      "5   wish i could be out all night tonight ! <user>\\n\n",
      "6                  <user> i got kicked out the wgm\\n\n",
      "7  rt <user> <user> <user> yes she is ! u tell it...\n",
      "8                      why is she so perfect <url>\\n\n",
      "9  <user> hi harry ! did u havea good time in aus...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data_neg[:10])\n",
    "len(data_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fe7ce4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n",
      "                                                   0\n",
      "0  sony dcr-sx 41 flash camcorder w / 60x optical...\n",
      "1  <user> i wish i could just be happy in one of ...\n",
      "2  whys everyone talking about chickfila on here ...\n",
      "3         studying for physics is agonising #exams\\n\n",
      "4  10x24 custom picture frame / poster frame . 75...\n",
      "5  monkeys ( animals that live in the rain forest...\n",
      "6  <user> it was the guys who were in after you ....\n",
      "7                                   <user> it is .\\n\n",
      "8  <user> <user> don't let me start talking about...\n",
      "9  03x29 custom picture frame / poster frame 2.37...\n",
      "                                                     0\n",
      "990  \" knock knock \" \" who's there ? \" \" rugby \" \" ...\n",
      "991  rt <user> <user> for saying that ' i smell ' y...\n",
      "992  i want a giant aquarium with sea turtles , and...\n",
      "993                       <user> oh it was amazing x\\n\n",
      "994  <user> can you sign my twitition <url> ? ? i t...\n",
      "995  \" <user> wow i have never had this many birthd...\n",
      "996  <user> that's all that's getting me through to...\n",
      "997  <user> chicken shed is made and ready , i'm go...\n",
      "998                 bout to burn it down with <user>\\n\n",
      "999  this is <user> your shit .. this cleaning shit...\n"
     ]
    }
   ],
   "source": [
    "def take_part(data,num_data,seed):\n",
    "    #input is a pd data frmae\n",
    "    np.random.seed(seed)\n",
    "    indexs=np.random.permutation(len(data))\n",
    "    \n",
    "    idx=indexs[: num_data]# smaller size\n",
    "    data_small=data.loc[idx]# takes randomly datas with size 1000 out of the data frame\n",
    "    data_small.index=[np.array(range(1,num_data+1))-1]# changes the label of the dtatframe to 0,1,2,3,4,5 etc\n",
    "    return data_small\n",
    "\n",
    "small_neg=take_part(data_neg,1000,20)\n",
    "small_pos=take_part(data_pos,1000,20)\n",
    "print(len(small_neg))\n",
    "print(len(small_pos))\n",
    "#small_neg.index=[np.array(range(1,1001))-1]\n",
    "print(small_neg[:10])\n",
    "print(small_pos[990:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c8fb1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('vocab_cut.txt') as pos:\n",
    "    for line in pos:\n",
    "        sentences_pos.append(line)\n",
    "voca = pd.DataFrame(sentences_pos)\n",
    "\n",
    "data=pd.concat([small_neg,small_pos],ignore_index=True)\n",
    "em = np.load('embeddings.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b88dd87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101298, 20)\n",
      "(201298, 1)\n",
      "2000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sony dcr-sx 41 flash camcorder w / 60x optical...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;user&gt; i wish i could just be happy in one of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>whys everyone talking about chickfila on here ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>studying for physics is agonising #exams\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10x24 custom picture frame / poster frame . 75...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>\" &lt;user&gt; wow i have never had this many birthd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>&lt;user&gt; that's all that's getting me through to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>&lt;user&gt; chicken shed is made and ready , i'm go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>bout to burn it down with &lt;user&gt;\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>this is &lt;user&gt; your shit .. this cleaning shit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      0\n",
       "0     sony dcr-sx 41 flash camcorder w / 60x optical...\n",
       "1     <user> i wish i could just be happy in one of ...\n",
       "2     whys everyone talking about chickfila on here ...\n",
       "3            studying for physics is agonising #exams\\n\n",
       "4     10x24 custom picture frame / poster frame . 75...\n",
       "...                                                 ...\n",
       "1995  \" <user> wow i have never had this many birthd...\n",
       "1996  <user> that's all that's getting me through to...\n",
       "1997  <user> chicken shed is made and ready , i'm go...\n",
       "1998                 bout to burn it down with <user>\\n\n",
       "1999  this is <user> your shit .. this cleaning shit...\n",
       "\n",
       "[2000 rows x 1 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(em.shape)\n",
    "print(voca.shape)\n",
    "print(len(data))\n",
    "type(data)\n",
    "data\n",
    "#if(data[:1000]==small_neg):\n",
    "#    print('the same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2ad207f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_token_list=data[0].apply(lambda x: nltk.word_tokenize(x)) # appplies to the 0 vector a word tokenisation and puts it into a new list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22ffe65f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tweet_token_list) # is a panda serie not animore a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b587b25f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [sony, dcr-sx, 41, flash, camcorder, w, /, 60x...\n",
      "1       [<, user, >, i, wish, i, could, just, be, happ...\n",
      "2       [whys, everyone, talking, about, chickfila, on...\n",
      "3       [studying, for, physics, is, agonising, #, exams]\n",
      "4       [10x24, custom, picture, frame, /, poster, fra...\n",
      "                              ...                        \n",
      "1995    [``, <, user, >, wow, i, have, never, had, thi...\n",
      "1996    [<, user, >, that, 's, all, that, 's, getting,...\n",
      "1997    [<, user, >, chicken, shed, is, made, and, rea...\n",
      "1998         [bout, to, burn, it, down, with, <, user, >]\n",
      "1999    [this, is, <, user, >, your, shit, .., this, c...\n",
      "Name: 0, Length: 2000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(tweet_token_list) #problem tokenisation of it's -> [it,'s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "21f0cf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "em_dim=em.shape[1]\n",
    "tweet_matrix=np.zeros(em_dim).reshape(em_dim,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "59014534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 2001)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_save_dispatcher() missing 1 required positional argument: 'arr'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7144/2409095577.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mtweet_matrix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet_matrix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtweet_vec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#apends the mean of the tweet value to get a (20,-1) matrix so summes all the vetors together to get a meaning ful representation of the tweet in the glove vector space\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet_matrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;31m#important! at the beginning are the negative tweets and at the end are the positive tweets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m#print(tweet_matrix.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msave\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: _save_dispatcher() missing 1 required positional argument: 'arr'"
     ]
    }
   ],
   "source": [
    "#Atention!! this shit takes time if the data set is to big\n",
    "for twe in tweet_token_list:\n",
    "    tweet_vec=np.zeros(em_dim) \n",
    "    for token in twe:                #np.where(voca==token)[0] the [0] says that the 1st indice where equality is met was taken used to avoid a list of indices\n",
    "        ind=np.where(voca==token)[0] #indice of token in the vocablurary and so the position of vector represents this word in the words embedings space \n",
    "        if len(ind)==0:              #if token is not in dictionarry word vector is zero\n",
    "            token_vec=np.zeros(em_dim)#every word is a vector of the embedings vector space\n",
    "        else:\n",
    "            token_vec=em[ind,:]#writes finally the corresponding vector of the token \n",
    "        tweet_vec=np.vstack((tweet_vec,token_vec))#puts the token vetor in one big tweet vector wich is one feature of the feature matrix\n",
    "                                                  #tweet_vec.shpae(-1,20)\n",
    "    if len(tweet_vec)==0:                         #fills the final feature matrix\n",
    "        tweet_vec=tweet_vec.T.reshape(em_dim,-1)\n",
    "        tweet_matrix=np.append(tweet_matrix,tweet_vec,axis=1)#tweet_vec.shpae(20,-1)\n",
    "    else:\n",
    "        tweet_vec=tweet_vec.T.mean(axis=1).reshape(em_dim,-1)\n",
    "\n",
    "        tweet_matrix=np.append(tweet_matrix,tweet_vec,axis=1)#apends the mean of the tweet value to get a (20,-1) matrix so summes all the vetors together to get a meaning ful representation of the tweet in the glove vector space\n",
    "tweet_matrix=np.delete(tweet_matrix,[0,0],axis=1)#delets the first row becaus it is just zero from the initialisation\n",
    "print(tweet_matrix.shape)\n",
    "#important! at the beginning are the negative tweets and at the end are the positive tweets\n",
    "#print(tweet_matrix.shape)\n",
    "#tweet_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "afc4033c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('feature_matrix_1000',tweet_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "be73a67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 2001)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "tX=np.load('feature_matrix_1000.npy')\n",
    "print(tX.shape)\n",
    "print(type(tX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9b15baf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[-0.08409718 -0.15980797  0.28695514 -0.28721007  0.15514346 -0.07946157\n",
      "  0.33652397  0.07334604  0.32803222  0.04050116  0.17896864  0.3141617\n",
      "  0.03127516 -0.30388865  0.20162648 -0.01727124  0.18387582 -0.07168783\n",
      "  0.00897475 -0.26566464]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(20, 2000)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tX[:,0])\n",
    "tX=np.delete(tX,[0,0],axis=1)\n",
    "print(tX[:,0])\n",
    "tX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5bf8d86e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_neg=np.zeros(len(small_neg))\n",
    "y_pos=np.ones(len(small_pos))\n",
    "print(y_neg.shape)\n",
    "Y=np.append(y_neg,y_pos)\n",
    "type(Y)\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "de796f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(x, y, ratio, seed):\n",
    "    # set seed\n",
    "    np.random.seed(seed)\n",
    "    # split the data based on the given ratio\n",
    "    index=np.random.permutation(len(y))\n",
    "    train_size=int(np.floor(ratio*len(y)))\n",
    "    \n",
    "    idx_train=index[: train_size]#trainings data indices\n",
    "    idx_val=index[train_size :]#valdiation data indices\n",
    "    \n",
    "    y_train=y[idx_train]#trainings results\n",
    "    y_val=y[idx_val]#validation results\n",
    "    \n",
    "    x_train=x[idx_train,:]#trainings data\n",
    "    x_val=x[idx_val,:]#validation data\n",
    "    return y_train, y_val, x_train, x_val\n",
    "[y_tr,y_te,x_tr,x_te]=split_data(tX.T,Y,0.9,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a80c1147",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1429b0",
   "metadata": {},
   "source": [
    "# Stochastic gradinet decent with hige loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "468b48ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 25.96, NNZs: 20, Bias: -1.353863, T: 1800, Avg. loss: 3.519662\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 11.72, NNZs: 20, Bias: 0.677977, T: 3600, Avg. loss: 2.118254\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 12.13, NNZs: 20, Bias: -3.036571, T: 5400, Avg. loss: 1.661402\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 8.98, NNZs: 20, Bias: -0.245252, T: 7200, Avg. loss: 1.450184\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 8.27, NNZs: 20, Bias: -1.389435, T: 9000, Avg. loss: 1.272821\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 8.02, NNZs: 20, Bias: -0.473332, T: 10800, Avg. loss: 1.220993\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 7.02, NNZs: 20, Bias: -1.251260, T: 12600, Avg. loss: 1.140706\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 7.51, NNZs: 20, Bias: 0.732481, T: 14400, Avg. loss: 1.114689\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 7.20, NNZs: 20, Bias: -0.593369, T: 16200, Avg. loss: 1.063303\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 6.82, NNZs: 20, Bias: -1.673652, T: 18000, Avg. loss: 1.061635\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 6.71, NNZs: 20, Bias: -2.085311, T: 19800, Avg. loss: 1.028042\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 6.29, NNZs: 20, Bias: -0.215720, T: 21600, Avg. loss: 1.001996\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 5.86, NNZs: 20, Bias: -1.075235, T: 23400, Avg. loss: 0.988045\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 5.90, NNZs: 20, Bias: 0.093141, T: 25200, Avg. loss: 0.994474\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 6.00, NNZs: 20, Bias: -0.320677, T: 27000, Avg. loss: 0.958514\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 6.04, NNZs: 20, Bias: -1.012633, T: 28800, Avg. loss: 0.953133\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 6.58, NNZs: 20, Bias: -0.685935, T: 30600, Avg. loss: 0.942738\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 6.23, NNZs: 20, Bias: -0.089372, T: 32400, Avg. loss: 0.944861\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 5.80, NNZs: 20, Bias: -0.123210, T: 34200, Avg. loss: 0.949195\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 6.05, NNZs: 20, Bias: -0.695328, T: 36000, Avg. loss: 0.933304\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 6.00, NNZs: 20, Bias: -0.182737, T: 37800, Avg. loss: 0.924778\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 6.29, NNZs: 20, Bias: -0.453138, T: 39600, Avg. loss: 0.917939\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 5.91, NNZs: 20, Bias: -0.933102, T: 41400, Avg. loss: 0.925661\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 5.95, NNZs: 20, Bias: -0.472576, T: 43200, Avg. loss: 0.917573\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 5.82, NNZs: 20, Bias: 0.386906, T: 45000, Avg. loss: 0.906831\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 5.81, NNZs: 20, Bias: -0.908144, T: 46800, Avg. loss: 0.881053\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 5.91, NNZs: 20, Bias: -0.903394, T: 48600, Avg. loss: 0.885946\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 5.76, NNZs: 20, Bias: -1.283596, T: 50400, Avg. loss: 0.904093\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 5.93, NNZs: 20, Bias: -0.700668, T: 52200, Avg. loss: 0.896854\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 5.98, NNZs: 20, Bias: -0.701225, T: 54000, Avg. loss: 0.883247\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 5.83, NNZs: 20, Bias: -1.052856, T: 55800, Avg. loss: 0.894777\n",
      "Total training time: 0.04 seconds.\n",
      "Convergence after 31 epochs took 0.04 seconds\n"
     ]
    }
   ],
   "source": [
    "clf = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=19000,verbose=True)\n",
    "clf.fit(x_tr, y_tr)\n",
    "y_pred=clf.predict(x_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d0bfc656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuarcy = 0.52\n"
     ]
    }
   ],
   "source": [
    "acc=np.sum(y_pred==y_te)/len(y_te)\n",
    "print('accuarcy = '+str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0396b2",
   "metadata": {},
   "source": [
    "# KNN clasification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e4ddd1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "bfcc2e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh = KNeighborsClassifier(n_neighbors=20)\n",
    "neigh.fit(x_tr, y_tr)\n",
    "y_pred=neigh.predict(x_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "0d3704b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuarcy = 0.56\n"
     ]
    }
   ],
   "source": [
    "acc=np.sum(y_pred==y_te)/len(y_te)\n",
    "print('accuarcy = '+str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfd0666",
   "metadata": {},
   "source": [
    "# SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "b4b8a2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "f9f5548b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVmod=SVC(degree=1)\n",
    "SVmod.fit(x_tr, y_tr)\n",
    "y_pred=SVmod.predict(x_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "5d80de62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuarcy = 0.575\n"
     ]
    }
   ],
   "source": [
    "acc=np.sum(y_pred==y_te)/len(y_te)\n",
    "print('accuarcy = '+str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fe165f",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "aee114aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "f775a11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                     hidden_layer_sizes=(1000), random_state=1, activation='relu', max_iter=50000)\n",
    "NN.fit(x_tr, y_tr)\n",
    "y_pred=NN.predict(x_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "fe0dfe08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuarcy = 0.575\n"
     ]
    }
   ],
   "source": [
    "acc=np.sum(y_pred==y_te)/len(y_te)\n",
    "print('accuarcy = '+str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9ebccccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 2,  3,  4],\n",
       "       [ 6,  7,  8],\n",
       "       [10, 11, 12]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\n",
    "arr=np.delete(arr, [0, 0],axis=1)\n",
    "print(arr.shape)\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d0fe45a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -0.08409718, -0.0628698 , ..., -0.01176455,\n",
       "         0.33380607,  0.17915743],\n",
       "       [ 0.        , -0.15980797, -0.04677629, ..., -0.02081695,\n",
       "        -0.55361333, -0.15582666],\n",
       "       [ 0.        ,  0.28695514, -0.44161309, ..., -0.32798236,\n",
       "         0.54019872, -0.04530718],\n",
       "       ...,\n",
       "       [ 0.        , -0.07168783,  0.0118153 , ...,  0.2654626 ,\n",
       "        -0.38930304,  0.03570304],\n",
       "       [ 0.        ,  0.00897475, -0.28477148, ..., -0.19180417,\n",
       "         0.16186536, -0.25459723],\n",
       "       [ 0.        , -0.26566464,  0.2394526 , ...,  0.22564004,\n",
       "        -0.12968422,  0.08770686]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.array([1,2,3]).reshape(1,-1)\n",
    "b=np.array([1,2,3]).reshape(1,-1)\n",
    "np.stack((a,b),axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "82a9c21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3, 2)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.array([1,2,3])\n",
    "b=np.array([1,2,3])\n",
    "a=a.reshape(-1,1)\n",
    "b=b.reshape(-1,1)\n",
    "print(a.shape)\n",
    "np.append(a,b,axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1460fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
